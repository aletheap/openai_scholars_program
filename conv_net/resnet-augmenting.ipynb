{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alethea's Attempt at ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            'batch_size': 400,\n",
    "            #'dataset': 'imagenette2-320',\n",
    "            'dataset': 'oxford-iiit-pet',\n",
    "            'dropout': 0.1,\n",
    "            'init_gain': 5,\n",
    "            'initializer': None,\n",
    "            'learning_rate': 0.1,\n",
    "            'load_workers': os.cpu_count(), \n",
    "            'max_epochs': 1000,\n",
    "            'optimizer': 'SGD',\n",
    "            'random_seed': 1,\n",
    "            'training_loops': 4,\n",
    "            'cuda_device_ids': [0, 1, 2],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['random_seed']:\n",
    "    torch.manual_seed(config['random_seed'])\n",
    "    torch.cuda.manual_seed(config['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"my-resnet34-augmenting\", config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our data. \n",
    "\n",
    "I'm using advice from https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/ about regularizing image data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note to self: Try this: https://www.basicml.com/performance/2019/04/16/pytorch-data-augmentation-with-nvidia-dali.html\n",
    "# cause I think cpu image transforms are a bottleneck for me\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomAffine(30, translate=(.2, .2), scale=(.8, 1.2), shear=None, resample=False, fillcolor=0),\n",
    "                                transforms.Resize(256),\n",
    "                                transforms.RandomCrop(224),\n",
    "                                transforms.ColorJitter(brightness=.5, contrast=.5, saturation=.5, hue=.5),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation(180),\n",
    "                                transforms.ToTensor(),         \n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],    \n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "                               ])\n",
    "\n",
    "dev_test_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),         \n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],    \n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "                               ])\n",
    "\n",
    "\n",
    "datadir = os.path.join(\"/home/apower/data\", config['dataset']) \n",
    "print('datadir:', datadir)\n",
    "traindir = os.path.join(datadir, 'train')\n",
    "print('traindir:', traindir)\n",
    "devdir = os.path.join(datadir, 'dev')\n",
    "print('devdir:', devdir)\n",
    "#testdir = os.path.join(datadir, 'test')\n",
    "#print('testdir:', testdir)\n",
    "\n",
    "X_train = torchvision.datasets.ImageFolder(traindir, transform)\n",
    "X_dev = torchvision.datasets.ImageFolder(devdir, dev_test_transform)\n",
    "#X_test = torchvision.datasets.ImageFolder(testdir, dev_test_transform)\n",
    "\n",
    "num_labels = len(X_train.classes)\n",
    "\n",
    "#print('training_set:', len(X_train), '\\ndev_set:', len(X_dev), '\\ntest_set:', len(X_test), '\\nlabels:', num_labels)\n",
    "print('training_set:', len(X_train), '\\ndev_set:', len(X_dev), '\\nlabels:', num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_pic = torchvision.transforms.ToPILImage()\n",
    "#to_pic(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_pic = torchvision.transforms.ToPILImage()\n",
    "#to_pic(X_dev[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(X_train, batch_size=config['batch_size'], shuffle=True, num_workers=config['load_workers'])\n",
    "dev_loader = DataLoader(X_dev, batch_size=1, shuffle=False)\n",
    "#test_loader = DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm basing this on the resnet diagram from: https://cv-tricks.com/keras/understand-implement-resnets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "                \n",
    "        if in_channels == out_channels:\n",
    "            self.proj = None \n",
    "        else:\n",
    "            self.proj = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2)\n",
    "            if config['initializer']:\n",
    "                getattr(nn.init, config['initializer'])(self.proj.weight, gain=config['init_gain'])\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.proj:\n",
    "            return self.proj(X)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            stride = 1\n",
    "        else:\n",
    "            stride = 2\n",
    "\n",
    "        conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        if config['initializer']:\n",
    "            getattr(nn.init, config['initializer'])(conv1.weight, gain=config['init_gain'])\n",
    "        \n",
    "        conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if config['initializer']:\n",
    "            getattr(nn.init, config['initializer'])(conv2.weight, gain=config['init_gain'])\n",
    "        \n",
    "        self.layer1 = nn.Sequential(conv1,\n",
    "                                    nn.BatchNorm2d(out_channels), \n",
    "                                    nn.Dropout2d(p=config['dropout']),\n",
    "                                    nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(conv2,\n",
    "                                    nn.BatchNorm2d(out_channels), \n",
    "                                    nn.Dropout2d(p=config['dropout']),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.proj = Projection(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        a = X\n",
    "        a = self.layer1(a)\n",
    "        a = self.layer2(a)\n",
    "        return a + self.proj(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self. num_labels = num_labels\n",
    "        \n",
    "        # 7x7 Conv\n",
    "        conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2)\n",
    "        if config['initializer']:\n",
    "            getattr(nn.init, config['initializer'])(conv1.weight, gain=config['init_gain'])\n",
    "        self.layer1 = nn.Sequential(conv1,\n",
    "                                    nn.BatchNorm2d(num_features=64),\n",
    "                                    nn.Dropout2d(p=config['dropout']),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        # 3x3 MaxPool\n",
    "        self.layer2 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                    nn.BatchNorm2d(num_features=64),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        # Stage 1\n",
    "        self.stage1 = nn.Sequential(ResidualBlock(in_channels=64, out_channels=64),\n",
    "                                    ResidualBlock(in_channels=64, out_channels=64),\n",
    "                                    ResidualBlock(in_channels=64, out_channels=64))\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = nn.Sequential(ResidualBlock(in_channels=64, out_channels=128),\n",
    "                                    ResidualBlock(in_channels=128, out_channels=128),\n",
    "                                    ResidualBlock(in_channels=128, out_channels=128),\n",
    "                                    ResidualBlock(in_channels=128, out_channels=128))\n",
    "\n",
    "        # Stage 3\n",
    "        self.stage3 = nn.Sequential(ResidualBlock(in_channels=128, out_channels=256),\n",
    "                                    ResidualBlock(in_channels=256, out_channels=256),\n",
    "                                    ResidualBlock(in_channels=256, out_channels=256),\n",
    "                                    ResidualBlock(in_channels=256, out_channels=256),\n",
    "                                    ResidualBlock(in_channels=256, out_channels=256),\n",
    "                                    ResidualBlock(in_channels=256, out_channels=256))\n",
    "        \n",
    "        # Stage 4\n",
    "        self.stage4 = nn.Sequential(ResidualBlock(in_channels=256, out_channels=512),\n",
    "                                    ResidualBlock(in_channels=512, out_channels=512),\n",
    "                                    ResidualBlock(in_channels=512, out_channels=512))\n",
    "        \n",
    "\n",
    "        # AveragePool\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully Connected\n",
    "        lin = nn.Linear(in_features=4608, out_features=num_labels)\n",
    "        if config['initializer']:\n",
    "            getattr(nn.init, config['initializer'])(lin.weight, gain=config['init_gain'])        \n",
    "        self.fc = nn.Sequential(lin, nn.Softmax(dim=1)) \n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        prefix = ' ' * 0\n",
    "        a = X       \n",
    "        a = self.layer1(X)\n",
    "        a = self.layer2(a)\n",
    "        a = self.stage1(a)\n",
    "        a = self.stage2(a)\n",
    "        a = self.stage3(a)\n",
    "        a = self.stage4(a)\n",
    "        a = self.avgpool(a)\n",
    "        a = a.reshape(a.size(0), -1)\n",
    "        a = self.fc(a)\n",
    "        return a\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = ResNet34(num_labels=num_labels)\n",
    "print(\"Let's use\", len(config['cuda_device_ids']), \"GPUs:\")\n",
    "\n",
    "for i in config['cuda_device_ids']:\n",
    "    print(i, \":\", torch.cuda.get_device_name(i))\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=config['cuda_device_ids'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader, name, cpu=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            if not cpu:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, learning_rate=0.1, max_epochs=20):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        for local_batch, local_labels in loader:\n",
    "            # Transfer to GPU\n",
    "            X, y = local_batch.to(device), local_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.item()  # <-- If you delete this it won't learn\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        t1 = time.time()\n",
    "        duration = t1-t0\n",
    "        loss_num = loss.item()\n",
    "        train_accuracy = accuracy(model, train_loader, 'train')\n",
    "        dev_accuracy = accuracy(model, dev_loader, 'dev')\n",
    "        relative_accuracy = dev_accuracy / train_accuracy\n",
    "        torch.save(model.state_dict(), './resnet-augmenting-' + config['dataset'] + '.pt')\n",
    "        print(' ' * 4, \n",
    "              '%.1f seconds -' % (duration), \n",
    "              'epoch:', epoch, \n",
    "              'lr: %.3f  ' % learning_rate,\n",
    "              'loss: %.3f  ' % loss_num, \n",
    "              'train: %.3f  ' % train_accuracy, \n",
    "              'dev: %.3f  ' % dev_accuracy, \n",
    "              'relative_accuracy: %.3f  ' % relative_accuracy)\n",
    "        try:\n",
    "            wandb.log({'loss': loss.item(), \n",
    "                   'learning_rate': learning_rate,\n",
    "                   'secs_per_epoch': duration, \n",
    "                   'train_accuracy': train_accuracy, \n",
    "                   'dev_accuracy': dev_accuracy, \n",
    "                   'relative_accuracy': relative_accuracy})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = config['learning_rate']\n",
    "\n",
    "for i in range(config['training_loops']):\n",
    "    # train model\n",
    "    print('learning_rate:', learning_rate, 'max_epochs:', config['max_epochs'])\n",
    "    model = train_model(model, \n",
    "                        train_loader, \n",
    "                        learning_rate=learning_rate, \n",
    "                        max_epochs=config['max_epochs'])\n",
    "\n",
    "    # slow down learning\n",
    "    learning_rate = learning_rate / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
