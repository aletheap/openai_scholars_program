{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer\n",
    "\n",
    "I wrote this to help me understand _Attention Is All You Need_: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "I cut out the Encoder and I'm using it to generate English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import multiprocessing.pool\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPU(s):\n",
      "    cuda:1: GeForce RTX 2080 Ti\n",
      "    cuda:0: GeForce RTX 2080 Ti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic Config\n",
    "conf = {\n",
    "        'attn_heads': 8,\n",
    "        'bptt_len': 40,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],  # I need better GPU coolng first\n",
    "        'cuda_device_ids': [1,0],\n",
    "        'd_model': 512,\n",
    "        #'datafile': './city_names.txt', # from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "        #'datafile': './corncob_lowercase.txt',  # from: http://www.mieliestronk.com/corncob_lowercase.txt\n",
    "        #'datafile': './alphabet_short.txt',  \n",
    "        #'datafile': './dummy_data.txt', \n",
    "        #'dataset': 'WikiText2',\n",
    "        'dataset': 'WikiText103',\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.0001,\n",
    "        'epochs_per_loop': 1,\n",
    "        'total_training_loops': 20,\n",
    "        'num_blocks_encoder': 0,\n",
    "        'num_blocks_decoder': 6,\n",
    "        #'minibatch_size': 32 * 16,\n",
    "        'minibatch_size': 20,\n",
    "        'optimizer': 'Adam',  \n",
    "        #'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "        #'warmup_steps': 50,\n",
    "        }\n",
    "\n",
    "\n",
    "# debugging\n",
    "#conf['attn_heads'] = 1\n",
    "#conf['d_model'] = 1\n",
    "#conf['bptt_len'] = 2\n",
    "#conf['datafile'] = './dummy_data.txt'  \n",
    "#conf['num_blocks_decoder'] = 1\n",
    "#conf['minibatch_size'] = 1\n",
    "#conf['epochs_per_loop'] = 1\n",
    "\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "assert conf['d_model'] % conf['attn_heads'] == 0, \\\n",
    "    f'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], \n",
    "                                                         conf['d_model'])\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "\n",
    "# Set up the RNGs for repeatability\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "    \n",
    "    \n",
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "print()\n",
    "\n",
    "# I use this bare FIXME:\n",
    "bptt_len = conf['bptt_len']\n",
    "\n",
    "# Logging\n",
    "wandb = None\n",
    "#wandb.init(project=\"official-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab, \n",
    "                 d_model=conf['d_model'], \n",
    "                 nhead=conf['attn_heads'], \n",
    "                 num_encoder_layers=conf['num_blocks_encoder'],\n",
    "                 num_decoder_layers=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab), \n",
    "                                      embedding_dim=d_model, \n",
    "                                      padding_idx=vocab.stoi['<pad>'])\n",
    "\n",
    "        pe = self._position_encoding(bptt_len=conf['bptt_len'], d_model=d_model)\n",
    "        self.register_buffer('position_encoding', pe)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model=d_model, \n",
    "                                          nhead=nhead, \n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers, \n",
    "                                          dropout=dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model,len(vocab))\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(sz=conf['bptt_len'])\n",
    "        self.register_buffer('tgt_mask', tgt_mask)\n",
    "\n",
    "    def _position_encoding(self, bptt_len, d_model):\n",
    "        cols = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(bptt_len)]\n",
    "        stack = torch.stack(cols, dim=0).unsqueeze(1)\n",
    "        return stack\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.position_encoding[:src.shape[0],:,:]\n",
    "        tgt = self.embedding(tgt) + self.position_encoding[:tgt.shape[0],:,:]\n",
    "        #print('src.size(2)=', src.size(2))\n",
    "        tgt_mask = self.tgt_mask[:tgt.shape[0],:tgt.shape[0]]\n",
    "        #print('tgt=', tgt)\n",
    "        #print('tgt_mask=', tgt_mask)\n",
    "        transformed = self.transformer(src=src, tgt=tgt, tgt_mask=tgt_mask)\n",
    "        out = self.linear(transformed)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# dataloader and vocab\n",
    "#train_ds = load_dataset()\n",
    "dataloader = getattr(torchtext.datasets, conf['dataset'])\n",
    "train_ds, val_ds, test_ds = dataloader.iters(batch_size=conf['minibatch_size'], \n",
    "                                             bptt_len=2 * conf['bptt_len'])\n",
    "vocab = train_ds.dataset.fields['text'].vocab\n",
    "\n",
    "pad_token = '_'\n",
    "pad_index = vocab.stoi[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = EmbeddingTransformer(vocab=vocab,\n",
    "                                d_model=conf['d_model'], \n",
    "                                nhead=conf['attn_heads'], \n",
    "                                num_encoder_layers=conf['num_blocks_encoder'],\n",
    "                                num_decoder_layers=conf['num_blocks_decoder'], \n",
    "                                dropout=conf['dropout'])\n",
    "#model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the Loss\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(dataset=train_ds):\n",
    "    bptt_len = conf['bptt_len']\n",
    "    for batch in dataset:\n",
    "        #print('batch:', batch)\n",
    "        if batch.text.shape[0] < bptt_len + 1:\n",
    "            continue\n",
    "        eo = batch.text[:bptt_len, :]\n",
    "        di = batch.text[bptt_len:, :]\n",
    "        y = batch.target[bptt_len:, :]\n",
    "        yield eo, di, y\n",
    "        \n",
    "def accuracy(output, expected_indices):\n",
    "    indices = torch.max(output, dim=-1)[1]\n",
    "    indices = indices.squeeze()\n",
    "    acc = (indices == expected_indices) / float(indices.numel())\n",
    "    acc = float(acc.sum())\n",
    "    return acc\n",
    "\n",
    "def run_minibatch(eo, di, y, optimizer):\n",
    "    \"\"\"Runs one minibatch training and returns the loss and accuracy for that minibatch\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    eo, di, y = eo.to(device), di.to(device), y.to(device)\n",
    "    y_pred = model(src=eo, tgt=di)\n",
    "    acc = accuracy(y_pred, y)\n",
    "    y_pred = y_pred.transpose(-2, -1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()  # Not sure why, but this step logs a UserWarning\n",
    "    optimizer.step()\n",
    "    return loss.item(), acc\n",
    "\n",
    "def test_set_accuracy(model):\n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "        accuracies = []\n",
    "        minibatches = 0\n",
    "        for eo, di, y in get_minibatches(test_ds):\n",
    "            eo, di, y = eo.to(device), di.to(device), y.to(device)\n",
    "            y_pred = model(src=eo, tgt=di)\n",
    "            accuracies.append(accuracy(y_pred, y))\n",
    "            minibatches += 1\n",
    "    acc = 100 * tensor(accuracies, device=device).float().mean().item()\n",
    "    return acc\n",
    "            \n",
    "def do_epoch(epoch, optimizer, model, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Runs one full training batch and returns the average loss,\n",
    "    accuracy, and duration time in seconds\"\"\"\n",
    "    model = model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    train_accuracies = []\n",
    "    for eo, di, y in get_minibatches():\n",
    "        #print('eo.shape:', encoder_out.shape, 'di.shape', decoder_in.shape, 'y.shape:', y.shape)\n",
    "        loss, train_acc = run_minibatch(eo, di, y, optimizer) \n",
    "        losses.append(loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "    #losses = [run_minibatch(*args) for args in get_minibatches(train_ds)]\n",
    "    tf = time.time()\n",
    "    if losses:\n",
    "        avg_loss = tensor(losses, device=device).float().mean().item()\n",
    "        avg_train_accuracy = 100 * tensor(train_accuracies, device=device).float().mean().item()\n",
    "    else:\n",
    "        avg_loss = 0\n",
    "        avg_train_accuracy = 0\n",
    "    avg_test_accuracy = test_set_accuracy(model)\n",
    "    return (avg_loss, avg_train_accuracy, avg_test_accuracy, tf-t0)\n",
    "\n",
    "def train(optimizer, num_epochs=conf['epochs_per_loop'], start_epoch=0, model=model, \n",
    "          vocab=vocab, criterion=criterion):\n",
    "    \"\"\"Runs num_epochs training batches and prints out results\"\"\"\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        loss, train_accuracy, test_accuracy, seconds = do_epoch(epoch, optimizer, model)\n",
    "        if wandb:\n",
    "            wandb.log({'epoch': epoch,\n",
    "                       'loss': loss,\n",
    "                       'train_accuracy': train_accuracy,\n",
    "                       'test_accuracy': test_accuracy,\n",
    "                       'seconds': seconds})\n",
    "        print('epoch:', epoch, '(%.1fs)' % seconds, 'loss=%f' % loss, 'train_accuracy=%.1f%%' % (train_accuracy), 'test_accuracy=%.1f%%' % (test_accuracy))   \n",
    "    return epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Optimizer\n",
    "#optimizer_class = getattr(torch.optim, conf['optimizer']) \n",
    "#lr = conf['learning_rate']\n",
    "#optimizer = optimizer_class(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb935988a6c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr = %.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs_per_loop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msave_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./official-transformer_%s_%s-layer_%s-epochs.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_blocks_decoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f4d6a472528e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, num_epochs, start_epoch, model, vocab, criterion)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m\"\"\"Runs num_epochs training batches and prints out results\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             wandb.log({'epoch': epoch,\n",
      "\u001b[0;32m<ipython-input-7-f4d6a472528e>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(epoch, optimizer, model, bptt_len)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mavg_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mavg_test_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_train_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f4d6a472528e>\u001b[0m in \u001b[0;36mtest_set_accuracy\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mminibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mminibatches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f7015a54edea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print('src.size(2)=', src.size(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "if 'learning_rate' in conf:\n",
    "    lr = conf['learning_rate']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=conf['learning_rate'])\n",
    "\n",
    "for _ in range(conf['total_training_loops']):\n",
    "    if 'warmup_steps' in conf:\n",
    "        warmup_steps = conf['warmup_steps']\n",
    "        lr = (conf['d_model']**-.5) * min(epoch**-.5, epoch * (warmup_steps**-1.5))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=lr)\n",
    "    print('lr = %.6f' % lr)\n",
    "    epoch = train(optimizer=optimizer, num_epochs=conf['epochs_per_loop'], start_epoch=epoch)\n",
    "    save_file_name = './official-transformer_%s_%s-layer_%s-epochs.pt' % (conf['dataset'], conf['num_blocks_decoder'], epoch-1)\n",
    "    if hasattr(model, 'module'):\n",
    "        torch.save(model.module.state_dict(), save_file_name)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), save_file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.8202623128891"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = './official-transformer_%s_%s-layer_%s-epochs.pt' % (conf['dataset'], conf['num_blocks_decoder'], epoch-1)\n",
    "torch.save(model.state_dict(), save_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.000100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 818.00 MiB (GPU 1; 10.76 GiB total capacity; 8.64 GiB already allocated; 18.25 MiB free; 1.57 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2bd1059457e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr = %.6f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs_per_loop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msave_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./official-transformer_%s_%s-layer_%s-epochs.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_blocks_decoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ae39e3a635b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, num_epochs, start_epoch, model, vocab, criterion)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m\"\"\"Runs num_epochs training batches and prints out results\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             wandb.log({'epoch': epoch,\n",
      "\u001b[0;32m<ipython-input-13-ae39e3a635b1>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(epoch, optimizer, model, bptt_len)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#print('eo.shape:', encoder_out.shape, 'di.shape', decoder_in.shape, 'y.shape:', y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ae39e3a635b1>\u001b[0m in \u001b[0;36mrun_minibatch\u001b[0;34m(eo, di, y, optimizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#acc=-1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Not sure why, but this step logs a UserWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sandbox1/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 818.00 MiB (GPU 1; 10.76 GiB total capacity; 8.64 GiB already allocated; 18.25 MiB free; 1.57 GiB cached)"
     ]
    }
   ],
   "source": [
    "if 'learning_rate' in conf:\n",
    "    lr = conf['learning_rate']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=conf['learning_rate'])\n",
    "\n",
    "for _ in range(conf['total_training_loops']):\n",
    "    if 'warmup_steps' in conf:\n",
    "        warmup_steps = conf['warmup_steps']\n",
    "        lr = (conf['d_model']**-.5) * min(epoch**-.5, epoch * (warmup_steps**-1.5))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=lr)\n",
    "    print('lr = %.6f' % lr)\n",
    "    epoch = train(optimizer=optimizer, num_epochs=conf['epochs_per_loop'], start_epoch=epoch)\n",
    "    save_file_name = './official-transformer_%s_%s-layer_%s-epochs.pt' % (conf['dataset'], conf['num_blocks_decoder'], epoch-1)\n",
    "    if hasattr(model, 'module'):\n",
    "        torch.save(model.module.state_dict(), save_file_name)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), save_file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.module.state_dict(), './my-transformer-wikitext-2-test-21.4_pct.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(tokens):\n",
    "    \"\"\"Takse a string and returns a tensor of vocab indices for the tokens\"\"\"\n",
    "    indices = list([vocab.stoi[t] for t in tokens])\n",
    "    return torch.tensor(indices).to(device).unsqueeze(1)\n",
    "\n",
    "def tokenize(indices):\n",
    "    \"Takes a tensor of token indices and returns a string\"\n",
    "    tokens = [vocab.itos[i] for i in indices.squeeze()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_next_token(encoder_out, decoder_in, pos, model=model, deterministic=False):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`.\"\"\"\n",
    "    \n",
    "    decoder_out = model(src=encoder_out, tgt=decoder_in)\n",
    "    \n",
    "    if deterministic:\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "    else:\n",
    "        probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "        m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "        _, indices = torch.max(m.sample(), dim=-1)\n",
    "\n",
    "    next_index = int(indices[pos,0])\n",
    "    return next_index, vocab.itos[next_index]\n",
    "\n",
    "def sample(prompt, deterministic=False, vocab=vocab, prnt=True):\n",
    "    \"\"\"Auto-regresses using prompt to create the encoder_out tensor\"\"\"\n",
    "    bptt_len = conf['bptt_len']\n",
    "    prompt_tokens = prompt.split()\n",
    "    assert len(prompt_tokens) == bptt_len + 1, 'Prompt strings must be %s tokens long' % bptt_len + 1   \n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "\n",
    "        eo = numericalize(prompt_tokens[:bptt_len])\n",
    "        di = numericalize(prompt_tokens[bptt_len:])\n",
    "        out = []\n",
    "\n",
    "        next_token = None\n",
    "        next_index = None\n",
    "        for pos in range(bptt_len):\n",
    "            next_index, next_token = get_next_token(eo, di, pos=pos, model=eval_model, deterministic=deterministic)\n",
    "            if next_token in (eos_token, pad_token):\n",
    "                break\n",
    "            if next_token is not None:\n",
    "                out.append(next_token)\n",
    "                if pos+1 < bptt_len:\n",
    "                    di = torch.cat((di, numericalize([next_token])), dim=0)\n",
    "        \n",
    "    out = ' '.join(out)\n",
    "    if prnt:\n",
    "        print(prompt + '\\n --> \\n' + out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forced_sample(prompt, expected_output, deterministic=False, vocab=vocab, prnt=True):\n",
    "    \"\"\"Auto-regresses using prompt to create the encoder_out tensor\"\"\"\n",
    "    bptt_len = conf['bptt_len']\n",
    "    prompt_tokens = prompt.split()\n",
    "    expected_out_tokens = expected_out.split()\n",
    "    assert len(prompt_tokens) == bptt_len, 'Prompt strings must be %s tokens long' % bptt_len    \n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "\n",
    "        eo = numericalize(prompt_tokens)\n",
    "        di = numericalize(expected_out_tokens)\n",
    "        out = []\n",
    "        #print('eo = ', eo)\n",
    "        #print('eo.shape = ', eo.shape)\n",
    "        #print('di = ', di)\n",
    "        #print('di.shape = ', di.shape)\n",
    "\n",
    "        next_token = None\n",
    "        next_index = None\n",
    "        for pos in range(bptt_len):\n",
    "            print('di=', di)\n",
    "            next_index, next_token = get_next_token(eo, di, pos=pos, model=eval_model, deterministic=deterministic)\n",
    "            print('pos=', pos, 'next_index=', next_index, 'next_token=next_token')\n",
    "            if next_token in (eos_token, pad_token):\n",
    "                break\n",
    "        \n",
    "    out = ' '.join(out)\n",
    "    if prnt:\n",
    "        print(prompt + '\\n --> \\n' + out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_eval = model.eval()\n",
    "for eo, di, y in get_minibatches():\n",
    "    for i in range(eo.shape[0]):\n",
    "        eo_s = eo[:,i].unsqueeze(1)\n",
    "        di_s = di[:,i].unsqueeze(1)\n",
    "        y_s = y[:,i].unsqueeze(1)\n",
    "        #print(eo_s)\n",
    "        with torch.no_grad():\n",
    "            decoder_out = model(src=eo_s, tgt=di_s)\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "        eo_t, di_t, y_t = tokenize(eo_s), tokenize(di_s), tokenize(y_s)\n",
    "        out = tokenize(indices)\n",
    "        acc = out == y_t\n",
    "        print('================= \\n eo=%r \\n di=%r \\n -> \\n out=%r \\n y=%r \\n (%r)' % (eo_t, di_t, out, y_t, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eo, di, y in get_minibatches(dataset=test_ds):\n",
    "    if eo.shape[0] != 40:\n",
    "        continue\n",
    "    prompt_indices = torch.cat((eo[:,0], di[:1,0]), dim=0).unsqueeze(1)\n",
    "    #print(prompt_indices)\n",
    "    prompt = tokenize(prompt_indices)\n",
    "    #print(prompt)\n",
    "\n",
    "    print(\"========================\")\n",
    "    sample(prompt, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\n",
      " --> \n",
      "to prison in 1909 .\n",
      "========================\n",
      "Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\n",
      " --> \n",
      "to time with her time to death . Moore continued to live in Atlanta with smallpox but her final appearance in the first week of eating . At age 18 , Hindley , a family of sons . At prison\n",
      "========================\n",
      "Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\n",
      " --> \n",
      "to money , was easier to breakdown .\n",
      "========================\n",
      "Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\n",
      " --> \n",
      "to Presbyterian intercourse to ninety years of prison with his parents .\n",
      "========================\n",
      "Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\n",
      " --> \n",
      "to nine years of age and educated in prison in the school school 's three years in the town . In 1945 , he raised £ 7 @,@ 000 in securing his court and claimed that the Jewish community had\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\"\"\"\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"========================\")\n",
    "    sample(prompt, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being\"\"\"\n",
    "expected_out=\"\"\"sentenced to 10 years in prison in 1946 for larceny and breaking and entering . In prison , he joined the Nation of Islam , adopted the name Malcolm X , and quickly became one of the organization 's most\"\"\"\n",
    "#forced_sample(prompt, expected_out, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\n",
      " --> \n",
      "balance others . In the last year firsthand was the main visitor of the nice and owe him directly with the investments . The cross was handed , and in the Stafford burial to the Conor elixirs the strong level\n",
      "========================\n",
      "There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\n",
      " --> \n",
      "be portrayed as well as Upshaw to get Abby Professor to prepare the photographers . Affleck 's role was preparing until the first Russian director was the authors inflammation of spectators and participated in the history of a commitment @-@\n",
      "========================\n",
      "There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\n",
      " --> \n",
      "compelling murder taught the differences between disease and disorder . In November 1942 , the seven members of the Vandal People and dissidents were seemingly granted post @-@ communist service during shooting conditions to 30 % of the population .\n",
      "========================\n",
      "There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\n",
      " --> \n",
      "avoid booty . Although this article suggested that this point either a specific analysis of catchphrases 's own Cinematographer of the principles had to simply own that an individual has Beauforts . The Marxist court has also interpreted divided and\n",
      "========================\n",
      "There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\n",
      " --> \n",
      "girls they behaved to German – Russian Catholic foreigners , Christian attitude of religious interactions at zeal \"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed to\"\"\"\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"========================\")\n",
    "    sample(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(get_minibatches(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eo=minibatches[0][0][:,0]\n",
    "di=minibatches[0][1][:,0]\n",
    "y=minibatches[0][2][:,0]\n",
    "print('eo=', ''.join(tokenize(eo)), 'eo.shape=', eo.shape)\n",
    "print('di=', ''.join(tokenize(di)), 'di.shape=', di.shape)\n",
    "print('y=', ''.join(tokenize(y)), 'y.shape=', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = model.eval()\n",
    "with torch.no_grad():\n",
    "    out = eval_model(src=eo.unsqueeze(1), tgt=di.unsqueeze(1))\n",
    "    print(out.shape)\n",
    "    indices = torch.max(out, dim=-1)[1]\n",
    "    print(tokenize(indices))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatches=batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = list(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = bb[0]\n",
    "print(batch.text)\n",
    "print(batch.target)\n",
    "bptt_len = conf['bptt_len']\n",
    "eo = batch.text[:bptt_len, :]\n",
    "di = batch.text[bptt_len:, :]\n",
    "y = batch.target[bptt_len:, :]\n",
    "print('eo=', eo)\n",
    "print('di=', di)\n",
    "print('y=', y)\n",
    "print('eo_t=', ''.join(tokenize(eo[:,0])))\n",
    "print('di_t=', ''.join(tokenize(di[:,0])))\n",
    "print('y_t=', ''.join(tokenize(y[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "\n",
    "def compute_jacobian(inputs, output):\n",
    "    \"\"\"\n",
    "    :param inputs: Batch X Size (e.g. Depth X Width X Height)\n",
    "    :param output: Batch X Classes\n",
    "    :return: jacobian: Batch X Classes X Size\n",
    "    \"\"\"\n",
    "    assert inputs.requires_grad\n",
    "\n",
    "    num_classes = output.size()[0]\n",
    "\n",
    "    jacobian = torch.zeros(num_classes, *inputs.size())\n",
    "    grad_output = torch.zeros(*output.size())\n",
    "    if inputs.is_cuda:\n",
    "        grad_output = grad_output.to(inputs.device)\n",
    "        jacobian = jacobian.to(inputs.device)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        zero_gradients(inputs)\n",
    "        grad_output.zero_()\n",
    "        grad_output[i, :, :] = 1\n",
    "        output.backward(grad_output, retain_graph=True)\n",
    "        jacobian[i] = inputs.grad.data\n",
    "\n",
    "    jacobian = jacobian.squeeze()\n",
    "    jacobian = torch.transpose(jacobian, dim0=0, dim1=1)\n",
    "\n",
    "    return jacobian\n",
    "\n",
    "bptt_len = conf['bptt_len']\n",
    "#model = model.train()\n",
    "#model = model.train()\n",
    "\n",
    "\n",
    "size = (2, 1)\n",
    "#print('size=', size)\n",
    "eo = torch.ones(size).long()\n",
    "di = torch.tensor([[1],[2]])\n",
    "assert di.shape == size\n",
    "\n",
    "class Nonsense(EmbeddingTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.module = nn.Module()\n",
    "        \n",
    "\n",
    "cpu_model = EmbeddingTransformer(vocab=vocab,\n",
    "                                d_model=conf['d_model'], \n",
    "                                nhead=conf['attn_heads'], \n",
    "                                num_encoder_layers=conf['num_blocks_encoder'],\n",
    "                                num_decoder_layers=conf['num_blocks_decoder'], \n",
    "                                dropout=conf['dropout'])\n",
    "\n",
    "#path=~/src/openai_scholars_program/sequence_models\n",
    "d = torch.load('./91.9_Pct/official-transformer_WikiText2_6-layer_9-epochs.pt', map_location=torch.device('cpu'))\n",
    "cpu_model.load_state_dict(d)\n",
    "\n",
    "src=eo\n",
    "tgt=di\n",
    "self = cpu_model\n",
    "\n",
    "print('tgt.shape=', tgt.shape)\n",
    "\n",
    "src = self.embedding(src) # + self.position_encoding[:src.shape[0],:,:]\n",
    "tgt = self.embedding(tgt) # + self.position_encoding[:tgt.shape[0],:,:]\n",
    "#print('src.size(2)=', src.size(2))\n",
    "tgt_mask = self.tgt_mask[:tgt.shape[0],:tgt.shape[0]]\n",
    "print('tgt=', tgt)\n",
    "print('tgt_mask=', tgt_mask)\n",
    "transformed = self.transformer(src=src, tgt=tgt, tgt_mask=tgt_mask)\n",
    "out = self.linear(transformed)\n",
    "print('out.shape=', out.shape)\n",
    "\n",
    "print('out:', out)\n",
    "\n",
    "\n",
    "j = compute_jacobian(tgt, out)\n",
    "print()\n",
    "print('squeezed jacobian:')\n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.position_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
