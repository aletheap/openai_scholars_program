{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer\n",
    "\n",
    "I wrote this to help me understand _Attention Is All You Need_: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "I cut out the Encoder and I'm using it to generate English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "        'attn_heads': 4,\n",
    "        'bptt_len': 25,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],  # I need better GPU coolng first\n",
    "        'cuda_device_ids': [3],\n",
    "        'd_model': 20,\n",
    "        #'datafile': './city_names.txt', # from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "        'datafile': './corncob_lowercase.txt',  # from: http://www.mieliestronk.com/corncob_lowercase.txt\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.1,\n",
    "        'epochs_per_loop': 50,\n",
    "        'total_training_loops': 30,\n",
    "        'num_blocks_encoder': 1,\n",
    "        'num_blocks_decoder': 2,\n",
    "        'minibatch_size': 45000,\n",
    "        #'optimizer': 'Adam'  # Adam gives me nans. Not sure why yet.\n",
    "        'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "        }\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "assert conf['d_key'] == int(conf['d_key']), \\\n",
    "    f'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], \n",
    "                                                         conf['d_model'])\n",
    "\n",
    "# Set up the RNGs for repeatability\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "\n",
    "# Logging\n",
    "wandb.init(project=\"my-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.1: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Implements section 3.2.1\"\"\"\n",
    "    def __init__(self, mask=True, d_model=conf['d_model'], d_key=conf['d_key'], \n",
    "                 bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_key = d_key\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        if mask:\n",
    "            ninf = np.NINF * torch.ones([bptt_len, bptt_len])\n",
    "            self.mask = nn.Parameter(ninf.triu(1), requires_grad=False)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "        scores = scores / sqrt(self.d_key)\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if self.mask is not None:\n",
    "            scores = scores.tril() + self.mask\n",
    "\n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Section 3.2.2\"\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        attn_heads = [AttentionHead(mask, d_model, d_key, bptt_len) \n",
    "                      for _ in range(heads)]\n",
    "        self.attn_heads = nn.ModuleList(attn_heads)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values) \n",
    "                      for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        return self.Wo(head_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"Section 3.3\"\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"Section 3.1, Encoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(False, d_model, heads, bptt_len)\n",
    "        self.self_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.self_attn_drop(self.self_attn(x, x, x))\n",
    "        a2 = self.self_attn_norm(x + a1)\n",
    "        a3 = self.ffn_norm(a2 + self.ffn_drop(self.ffn(a2)))\n",
    "\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"Section 3.1, Decoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.self_attn = MultiHeadAttention(True, d_model, heads, bptt_len)\n",
    "        \n",
    "        self.self_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attn = MultiHeadAttention(mask_all, d_model, heads, bptt_len)\n",
    "        self.enc_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        a1_attn = self.self_attn(x, x, x)\n",
    "        a1_drop = self.self_attn_drop(a1_attn)\n",
    "        a1 = self.self_attn_norm(x + a1_drop)\n",
    "        a2_attn = self.enc_attn(a1, encoder_out, encoder_out)\n",
    "        a2_drop = self.enc_attn_drop(a2_attn)\n",
    "        a2 = self.enc_attn_norm(a1 + a2_drop)\n",
    "        a3 = self.ffn_norm(a2 + self.ffn_drop(self.ffn(a2)))\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, bptt_len, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, bptt_len, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "        \n",
    "        pad_idx = vocab.stoi['<pad>']\n",
    "        self.embedding = nn.Embedding(len(vocab), d_model, padding_idx=pad_idx)\n",
    "        self.position_encoding = nn.Parameter(self._position_encoding(), \n",
    "                                              requires_grad=False)\n",
    "        self.embed_dropout = nn.Dropout(p=dropout)\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, bptt_len, num_blocks_encoder, \n",
    "        #                       dropout)\n",
    "        self.decoder = Decoder(d_model, heads, bptt_len, num_blocks_decoder, \n",
    "                               dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, len(self.vocab))\n",
    "        self.linear_softmax = nn.Softmax(dim=-1)\n",
    "           \n",
    "    def _position_encoding(self):\n",
    "        d_model = self.d_model\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(self.bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        \n",
    "        return stack.T\n",
    "    \n",
    "    def embed(self, indices):\n",
    "        \"\"\"Implements the embedding from Section 3.4 Embeddings and Softmax\"\"\"\n",
    "        embedded = self.embedding(tensor(indices))\n",
    "        return embedded + self.position_encoding\n",
    "        \n",
    "    #def forward(self, encoder_in=None, encoder_out=None, decoder_in=[]):\n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        encoder_out: (optional rank-1 tensor) passing this skips \n",
    "                     the encoder execution, and acts and if this were \n",
    "                     the indices the encoder produced.\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "        \n",
    "        # Encode\n",
    "        #encoder_out = self.embed_dropout(self.embed(embedded))\n",
    "        encoder_out = self.embed(encoder_out)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_in = self.embed(decoder_in)\n",
    "        decoder_out = self.decoder(encoder_out, decoder_in)\n",
    "\n",
    "        # Return predictions for next token\n",
    "        y_pred = self.linear_softmax(self.linear(decoder_out))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Vocab and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "print()\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Make the vocabulary\n",
    "with open(conf['datafile'], 'r') as f:\n",
    "    strings = [line.strip().lower() for line in f.readlines()]\n",
    "    token_iterator = ''.join(strings)\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(token_iterator)\n",
    "    vocab.stoi['<pad>'] = 0\n",
    "    vocab.itos[0] = '<pad>'\n",
    "    vocab.stoi['<eos>'] = 1\n",
    "    vocab.stoi[1] = '<eos>'\n",
    "    vocab.freqs['<eos>'] = len(strings)\n",
    "\n",
    "# Create the model\n",
    "model = Transformer(vocab)\n",
    "#model = model.half()\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "\n",
    "# Define the Optimizer\n",
    "optimizer_class = getattr(torch.optim, conf['optimizer']) \n",
    "optimizer = optimizer_class(model.parameters(), lr=conf['learning_rate'])\n",
    "\n",
    "# Define the Loss\n",
    "#CE_freqs = [float(vocab.freqs[t]) for t in vocab.itos]\n",
    "#CE_weight = [(0. if f == 0 else 1/f) for f in CE_freqs]\n",
    "#CE_weight = torch.tensor(CE_weight, dtype=torch.half, device=device)\n",
    "#pad_idx = vocab.stoi['<pad>']\n",
    "#criterion = nn.CrossEntropyLoss(weight=CE_weight, ignore_index=pad_idx)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_indices(indices, right_shift=False, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Takes a list of token `indices`, appends the index for the\n",
    "    <eos> token and pads the list on the right to the bptt_len length.\n",
    "    If you pass `right_shift=True`, then it also inserts the index for \n",
    "    the pad token to the beginning of the list and shifts everything \n",
    "    else to the right (still maintaining bptt_len.\"\"\"\n",
    "    indices = list(map(int, indices))\n",
    "    eos_index = vocab.stoi['<eos>']\n",
    "    pad_index = vocab.stoi['<pad>']\n",
    "    if (not indices) or (indices[-1] != eos_index):\n",
    "        indices.append(eos_index)\n",
    "    if right_shift:\n",
    "        indices.insert(0, pad_index)\n",
    "    indices = indices[:bptt_len]\n",
    "    pad_len = bptt_len - len(indices)\n",
    "    indices += [pad_index] * pad_len\n",
    "    return indices\n",
    "    \n",
    "def get_indices(string, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"takes a string, tokenizes it, and returns the (unpadded) \n",
    "    list of token incides for the tokens in the string. The output\n",
    "    of this method is suitable input for `pad_indices()`\"\"\"\n",
    "    tokens = list(string.strip().lower())\n",
    "    tokens = tokens[:bptt_len]\n",
    "    indices = list(map(lambda x: vocab.stoi[x], tokens))\n",
    "    return indices\n",
    "\n",
    "def _get_tensors(string, model=model, vocab=vocab, criterion=criterion, \n",
    "                 optimizer=optimizer, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Takes a string, splits it into two parts at each position,\n",
    "    and returns lists of the appropriately padded and shifted \n",
    "    `encoder_out`, `decoder_in`, and `y` tensors for each split.\"\"\"\n",
    "\n",
    "    indices = get_indices(string, vocab)\n",
    "    encoder_out = []\n",
    "    decoder_in = []\n",
    "    y = []\n",
    "    for i in range(len(indices)):\n",
    "        this_enc_out = tensor(pad_indices(indices[:i])).unsqueeze(0)\n",
    "        this_dec_in = tensor(pad_indices(indices[i:], \n",
    "                                         right_shift=True)).unsqueeze(0)\n",
    "        this_y = tensor(pad_indices(indices[i:])).unsqueeze(0)\n",
    "        encoder_out.append(this_enc_out)\n",
    "        decoder_in.append(this_dec_in)\n",
    "        y.append(this_y)\n",
    "    return encoder_out, decoder_in, y\n",
    "\n",
    "def get_data(minibatch_size=conf['minibatch_size'], vocab=vocab, \n",
    "             data_file=conf['datafile']):\n",
    "    \"\"\"Reads the \\n separated training strings from the data file,\n",
    "    and returns a generator of `encoder_out`, `encoder_in`, and `y` tensors. \n",
    "    Each tensor contains vocab indices for tokens and has a shape:\n",
    "    (minibatch_size, bptt_len)\"\"\"\n",
    "    \n",
    "    global data\n",
    "\n",
    "    if not data:\n",
    "        # Cache this in memory so we don't have to re-read it. My datasets\n",
    "        # are small enough for now to fit in ram easily\n",
    "        with open(data_file,'r') as f:\n",
    "            strings = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "        for string in strings:\n",
    "            encoder_out, decoder_in, y = _get_tensors(string)\n",
    "            data['encoder_out'].extend(encoder_out)\n",
    "            data['decoder_in'].extend(decoder_in)\n",
    "            data['y'].extend(y)\n",
    "        data['encoder_out'] = torch.cat(data['encoder_out']).to(device)\n",
    "        data['decoder_in'] = torch.cat(data['decoder_in']).to(device)\n",
    "        data['y'] = torch.cat(data['y']).to(device)\n",
    "\n",
    "    eo_split = torch.split(data['encoder_out'], minibatch_size, dim=0)\n",
    "    di_split = torch.split(data['decoder_in'], minibatch_size, dim=0)\n",
    "    y_split = torch.split(data['y'], minibatch_size, dim=0)\n",
    "    for encoder_out, decoder_in, y_split in zip(eo_split, di_split, y_split):\n",
    "        yield encoder_out, decoder_in, y_split \n",
    "        \n",
    "def train_data(encoder_out, decoder_in, y):\n",
    "    \"\"\"Runs one minibatch training and returns the loss for that minibatch\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    y_pred = torch.transpose(y_pred, -2, -1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #sys.stdout.write('.')\n",
    "    return loss.item()\n",
    "\n",
    "def do_epoch(epoch, model=model, vocab=vocab, criterion=criterion, \n",
    "             optimizer=optimizer):\n",
    "    \"\"\"Runs one full training batch and returns the average loss and \n",
    "    duration time in seconds\"\"\"\n",
    "    t0 = time.time()\n",
    "    losses = [train_data(*args) for args in get_data()]\n",
    "    tf = time.time()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return (avg_loss, tf-t0)\n",
    "\n",
    "def train(num_epochs=conf['epochs_per_loop'], start_epoch=0, model=model, \n",
    "          vocab=vocab, criterion=criterion, optimizer=optimizer):\n",
    "    \"\"\"Runs num_epochs training batchs and prints out results\"\"\"\n",
    "    model = model.train()\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        loss, seconds = do_epoch(epoch, model, vocab, criterion, optimizer)\n",
    "        #print()\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'loss': loss,\n",
    "                   'seconds': seconds})\n",
    "        print('epoch:', epoch, '(%.1fs)' % seconds, 'loss=%.4f' % loss)    \n",
    "    return epoch + 1\n",
    "\n",
    "\n",
    "#induction_weight = CE_weight.unsqueeze(dim=0)\n",
    "\n",
    "def get_next_token(encoder_out, decoder_in, pos, model=model, vocab=vocab):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`. Uses a Multinomial distribution for sampling.\"\"\"\n",
    "    eval_model = model.eval()\n",
    "    \n",
    "    encoder_out = pad_indices(get_indices(encoder_out))\n",
    "    encoder_out = tensor(encoder_out).to(device)\n",
    "    \n",
    "    decoder_in = pad_indices(get_indices(decoder_in), right_shift=True)\n",
    "    decoder_in = tensor(decoder_in).to(device)\n",
    "    \n",
    "    decoder_out = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    #decoder_out = decoder_out * induction_weight\n",
    "    \n",
    "    probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "    m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "    _, indices = torch.max(m.sample(), dim=-1)\n",
    "    \n",
    "    next_index = int(indices[pos])\n",
    "    return vocab.itos[next_index]\n",
    "\n",
    "def sample_with_prompt(prompt, model=model, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"samples a string the network beginning with prompt\"\"\"\n",
    "    encoder_out = prompt\n",
    "    decoder_out = ''\n",
    "    next_token = ''\n",
    "    \n",
    "    while next_token not in ('<pad>', '<eos>') and len(decoder_out) < (bptt_len - 1):\n",
    "        decoder_out += next_token\n",
    "        next_token = get_next_token(encoder_out, decoder_out, \n",
    "                                    pos=len(decoder_out))\n",
    "        \n",
    "    return prompt + decoder_out\n",
    "\n",
    "def alphabet_sample():\n",
    "    \"\"\"samples strings beginning with each letter of the alphabet\"\"\"\n",
    "    for p in [chr(i) for i in range(ord('a'), ord('z')+1)]: \n",
    "        print('%r' % sample_with_prompt(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_sample()\n",
    "\n",
    "for _ in range(conf['total_training_loops']):\n",
    "    epoch = train(conf['epochs_per_loop'], epoch)\n",
    "    alphabet_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
