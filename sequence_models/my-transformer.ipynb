{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " \n",
    "Pulling list of cities from: \n",
    "https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "        'attn_heads': 3,\n",
    "        'bptt_len': 20,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],\n",
    "        'cuda_device_ids': [3],\n",
    "        'd_model': 9,\n",
    "        'device': 'cuda',\n",
    "        'datafile': './city_names.txt',\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_epochs': 100,\n",
    "        #'num_blocks_encoder': 6,\n",
    "        'num_blocks_decoder': 3,\n",
    "        #'optimizer': 'Adam',\n",
    "        'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "           \n",
    "        #'batch_size': 400,\n",
    "        #'dataset': 'imagenette2-320',\n",
    "        #'init_gain': 5,\n",
    "        #'initializer': None,\n",
    "        #'load_workers': os.cpu_count(), \n",
    "        #'training_loops': 4,\n",
    "        #'cuda_device_ids': [0, 1, 2],\n",
    "        #'num_hidden_nodes': 300,\n",
    "        }\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "assert conf['d_key'] == int(conf['d_key']), 'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], conf['d_model'])\n",
    "\n",
    "# Set up the RNGs\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "\n",
    "# Logging\n",
    "wandb.init(project=\"my-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, mask=True, d_model=conf['d_model'], d_key=conf['d_key'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_key = d_key\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        if mask:\n",
    "            self.mask = nn.Parameter((np.NINF * torch.ones([bptt_len, bptt_len])).triu(1), requires_grad=False)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) / sqrt(self.d_key) #shape = (heads, bptt_len, bptt_len)\n",
    "        #assert scores.shape == torch.Size([self.bptt_len, self.bptt_len])\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if self.mask is not None:\n",
    "            scores = scores.tril() + self.mask\n",
    "\n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "        #assert attn.shape == torch.Size([self.bptt_len, self.d_key])\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], heads=conf['attn_heads'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(mask, d_model, d_key, bptt_len) for _ in range(heads)])\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values) for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        return self.Wo(head_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(False, d_model, heads, bptt_len)\n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.ffn_norm(a1 + self.ffn_dropout(self.ffn(a1)))\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.self_attn = MultiHeadAttention(True, d_model, heads, bptt_len)\n",
    "        \n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attn = MultiHeadAttention(mask_all, d_model, heads, bptt_len)\n",
    "        self.enc_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.enc_attn_norm(a1 + self.enc_attn_dropout(self.enc_attn(a1, encoder_out, encoder_out)))\n",
    "        a3 = self.ffn_norm(a2 + self.ffn_dropout(self.ffn(a2)))\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(vocab), d_model, padding_idx=vocab.stoi['<pad>'])\n",
    "        self.position_encoding = nn.Parameter(self._position_encoding(), requires_grad=False)\n",
    "        self.embed_dropout = nn.Dropout(p=dropout)\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, bptt_len, num_blocks_encoder, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, bptt_len, num_blocks_decoder, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, len(self.vocab))\n",
    "        #self.linear.weight = self.embedding.weight  # Section 3.4\n",
    "        self.linear_dropout = nn.Dropout(p=dropout)\n",
    "        self.linear_norm = nn.LayerNorm(len(vocab))\n",
    "           \n",
    "    def _position_encoding(self):\n",
    "        d_model = self.d_model\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(self.bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        assert stack.shape == torch.Size([self.d_model, self.bptt_len])\n",
    "        \n",
    "        return stack.T\n",
    "    \n",
    "    def embed(self, indices):\n",
    "        embedded = self.embedding(tensor(indices))\n",
    "        #assert embedded.shape == torch.Size([self.bptt_len, self.d_model])\n",
    "        return embedded + self.position_encoding\n",
    "        \n",
    "    #def forward(self, encoder_in=None, encoder_out=None, decoder_in=[]):\n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        encoder_out: (optional rank-1 tensor) passing this skips \n",
    "                     the encoder execution, and acts and if this were \n",
    "                     the indices the encoder produced.\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "        \n",
    "        # Embed\n",
    "        #embedded = self.embed_dropout(self.embed(encoder_in))\n",
    "            \n",
    "        # Encode\n",
    "        #encoder_out = self.embed_dropout(self.embed(embedded))\n",
    "        \n",
    "        # Decode\n",
    "        encoder_out = self.embed(encoder_out)\n",
    "        #print('encoder_out = ', encoder_out)\n",
    "        decoder_in = self.embed(decoder_in)\n",
    "        #print('decoder_in = ', decoder_in)\n",
    "        decoder_out = self.decoder(encoder_out, decoder_in)\n",
    "        #print('decoder_out = ', decoder_out)\n",
    "        #print('decoded:', decoded)\n",
    "\n",
    "        # Return predictions for next token\n",
    "        y_pred = self.linear(decoder_out)\n",
    "        return self.linear_norm(self.linear_dropout(y_pred))\n",
    "        #y_pred = torch.matmul(decoded, self.embedding.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Vocab and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "# Make the vocabulary\n",
    "with open(conf['datafile'], 'r') as f:\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(f.read().replace('\\n','').lower())\n",
    "    vocab.stoi['<pad>'] = 0\n",
    "    vocab.stoi['<eos>'] = 1\n",
    "    vocab.itos[0] = '<pad>'\n",
    "    vocab.stoi[1] = '<eos>'\n",
    "with open(conf['datafile'], 'r') as f:\n",
    "    vocab.freqs['<eos>'] = len(f.readlines())\n",
    "\n",
    "# define the model\n",
    "model = Transformer(vocab)\n",
    "model = model.half().to(device)\n",
    "model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "optimizer = getattr(torch.optim, conf['optimizer'])(model.parameters(), lr=conf['learning_rate'])\n",
    "\n",
    "CE_freqs = [float(vocab.freqs[t]) for t in vocab.itos]\n",
    "CE_weight = [(0. if f == 0 else 1/f) for f in CE_freqs]\n",
    "CE_weight = torch.tensor(CE_weight, dtype=torch.half, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=CE_weight, ignore_index=vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "\n",
    "def pad_indices(indices, right_shift=False, bptt_len=conf['bptt_len']):\n",
    "    indices = list(map(int, indices))\n",
    "    eos_index = vocab.stoi['<eos>']\n",
    "    pad_index = vocab.stoi['<pad>']\n",
    "    if (not indices) or (indices[-1] != eos_index):\n",
    "        indices.append(eos_index)\n",
    "    if right_shift:\n",
    "        indices.insert(0, pad_index)\n",
    "    indices = indices[:bptt_len]\n",
    "    pad_len = bptt_len - len(indices)\n",
    "    indices += [pad_index] * pad_len\n",
    "    return indices\n",
    "    \n",
    "#def get_indices(string, vocab, max_tokens=conf['bptt_len'], include_shifted=False):\n",
    "def get_indices(string, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    tokens = list(string.strip().lower())\n",
    "    tokens = tokens[:bptt_len]\n",
    "    indices = list(map(lambda x: vocab.stoi[x], tokens))\n",
    "    return indices\n",
    "\n",
    "def _make_filter(shape):\n",
    "    return \n",
    "\n",
    "def _get_tensors(string, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer, bptt_len=conf['bptt_len']):\n",
    "    indices = get_indices(string, vocab)\n",
    "    encoder_out = []\n",
    "    decoder_in = []\n",
    "    y = []\n",
    "    for i in range(len(indices)):\n",
    "        encoder_out.append(tensor(pad_indices(indices[:i])).unsqueeze(0))\n",
    "        decoder_in.append(tensor(pad_indices(indices[i:], right_shift=True)).unsqueeze(0))\n",
    "        y.append(tensor(pad_indices(indices[i:])).unsqueeze(0))\n",
    "    return encoder_out, decoder_in, y\n",
    "\n",
    "def get_data(minibatch_size=500, vocab=vocab, data_file=conf['datafile']):\n",
    "    global data\n",
    "    if not data:\n",
    "        with open(data_file,'r') as f:\n",
    "            strings = [line.strip().lower() for line in f.readlines()]\n",
    "        for string in strings:\n",
    "            encoder_out, decoder_in, y = _get_tensors(string)\n",
    "            data['encoder_out'].extend(encoder_out)\n",
    "            data['decoder_in'].extend(decoder_in)\n",
    "            data['y'].extend(y)\n",
    "        data['encoder_out'] = torch.cat(data['encoder_out']).to(device)\n",
    "        data['decoder_in'] = torch.cat(data['decoder_in']).to(device)\n",
    "        data['y'] = torch.cat(data['y']).to(device)\n",
    "\n",
    "    batch_size = data['y'].shape[0]\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        j = i + minibatch_size\n",
    "        encoder_out = data['encoder_out'][i:j,:]\n",
    "        decoder_in = data['decoder_in'][i:j,:]\n",
    "        y = data['y'][i:j,:]\n",
    "        yield encoder_out, decoder_in, y\n",
    "        i = j\n",
    "        \n",
    "def train_data(encoder_out, decoder_in, y):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    y_pred = torch.transpose(y_pred, -2, -1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #sys.stdout.write('.')\n",
    "    return loss.item()\n",
    "\n",
    "def do_epoch(epoch, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer):\n",
    "    t0 = time.time()\n",
    "    losses = [train_data(*args) for args in get_data()]\n",
    "    tf = time.time()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return (avg_loss, tf-t0)\n",
    "\n",
    "def train(num_epochs=conf['max_epochs'], start_epoch=0, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer):\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        loss, seconds = do_epoch(epoch, model, vocab, criterion, optimizer)\n",
    "        #print()\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'loss': loss,\n",
    "                   'seconds': seconds})\n",
    "        print(epoch, '%.2f secs:' % seconds, 'loss: %.4f' % loss)    \n",
    "    return epoch + 1\n",
    "\n",
    "\n",
    "induction_weight = CE_weight.unsqueeze(dim=0)\n",
    "\n",
    "def get_next_token(encoder_out, decoder_in, pos, model=model, vocab=vocab):\n",
    "    eval_model = model.eval()\n",
    "    encoder_out = tensor(pad_indices(get_indices(encoder_out))).to(device)\n",
    "    decoder_in = tensor(pad_indices(get_indices(decoder_in), right_shift=True)).to(device)\n",
    "    decoder_out = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    decoder_out = decoder_out * induction_weight\n",
    "    _, indices = torch.max(nn.functional.softmax(decoder_out, dim=1), dim=1)\n",
    "    index = int(indices[pos])\n",
    "    #print('index:', index, 'pos:', pos)\n",
    "    return vocab.itos[index]\n",
    "\n",
    "def sample_with_prompt(prompt, model=model, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    encoder_out = prompt\n",
    "    decoder_out = ''\n",
    "    next_token = ''\n",
    "    \n",
    "    while next_token not in ('<pad>', '<eos>') and len(decoder_out) < (bptt_len - 1):\n",
    "        #print('prompt:', prompt, 'decoder_out:', decoder_out, 'next_index:', next_index, 'next_token:', next_token)\n",
    "        decoder_out += next_token\n",
    "        next_token = get_next_token(encoder_out, decoder_out, pos=len(decoder_out))\n",
    "        \n",
    "    return prompt + decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_per_run = 100\n",
    "for _ in range(15):\n",
    "    epoch = train(epochs_per_run, epoch)\n",
    "    for p in [chr(i) for i in range(ord('a'), ord('z')+1)]: \n",
    "        print('%r' % sample_with_prompt(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = model.eval()\n",
    "encoder_out = tensor(pad_indices(get_indices('N'))).to(device)\n",
    "decoder_in = tensor(pad_indices(get_indices(''), right_shift=True)).to(device)\n",
    "decoder_out = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "_, indices = torch.max(nn.functional.softmax(decoder_out, dim=1), dim=1)\n",
    "indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
