{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Config, Logging, RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "        'attn_heads': 2,\n",
    "        'bptt_len': 50,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],\n",
    "        'cuda_device_ids': [3],\n",
    "        'd_model': 4,\n",
    "        'device': 'cuda',\n",
    "        'datafile': './city_names.txt',\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_epochs': 1000,\n",
    "        'num_blocks_encoder': 6,\n",
    "        'num_blocks_decoder': 2,\n",
    "        'optimizer': 'Adam',\n",
    "        'random_seed': 0,\n",
    "           \n",
    "        #'batch_size': 400,\n",
    "        #'dataset': 'imagenette2-320',\n",
    "        #'init_gain': 5,\n",
    "        #'initializer': None,\n",
    "        #'load_workers': os.cpu_count(), \n",
    "        #'training_loops': 4,\n",
    "        #'cuda_device_ids': [0, 1, 2],\n",
    "        #'num_hidden_nodes': 300,\n",
    "        }\n",
    "\n",
    "# Pulling list of cities from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "assert conf['d_key'] == int(conf['d_key']), 'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], conf['d_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"my-transformer\", config=conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, mask=True, d_model=conf['d_model'], d_key=conf['d_key'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_key = d_key\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        if mask:\n",
    "            self.mask = nn.Parameter((np.NINF * torch.ones([bptt_len, bptt_len])).triu(1), requires_grad=False)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        #print('(AttenHead) queries=', queries)\n",
    "        keys = self.Wk(keys)\n",
    "        #print('(AttenHead) keys=', keys)\n",
    "        values = self.Wv(values)\n",
    "        #print('(AttenHead) values=', values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, keys.T) / sqrt(self.d_key) #shape = (heads, bptt_len, bptt_len)\n",
    "        #print('(AttenHead) scores=', scores)\n",
    "        assert scores.shape == torch.Size([self.bptt_len, self.bptt_len])\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if self.mask is not None:\n",
    "            t = scores.tril()\n",
    "            #print('(AttenHead) zeroed scores=', t)\n",
    "            #print('(AttenHead) mask=', self.mask)\n",
    "\n",
    "            scores = scores.tril() + self.mask\n",
    "            #print('(AttenHead) masked scores=', scores)\n",
    "\n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        #print('(AttenHead) softmax scores=', scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "        #print('(AttenHead) attn=', attn)\n",
    "        assert attn.shape == torch.Size([self.bptt_len, self.d_key])\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], heads=conf['attn_heads'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(mask, d_model, d_key, bptt_len) for _ in range(heads)])\n",
    "                    \n",
    "    def forward(self, queries, keys, values):\n",
    "        head_attns = [h(queries, keys, values) for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=1)\n",
    "        return self.Wo(head_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(False, d_model, heads, bptt_len)\n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.ffn_norm(a1 + self.ffn_dropout(self.ffn(a1)))\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.self_attn = MultiHeadAttention(True, d_model, heads, bptt_len)\n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attn = MultiHeadAttention(mask_all, d_model, heads, bptt_len)\n",
    "        self.enc_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoded):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.enc_attn_norm(a1 + self.enc_attn_dropout(self.enc_attn(a1, encoded, encoded)))\n",
    "        a3 = self.ffn_norm(a2 + self.ffn_dropout(self.ffn(a2)))\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a, x)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(len(vocab), d_model, padding_idx=1)\n",
    "        self.position_encoding = nn.Parameter(self._position_encoding(), requires_grad=False)\n",
    "        self.initial_dropout = nn.Dropout(p=dropout)\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, bptt_len, num_blocks_encoder, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, bptt_len, num_blocks_decoder, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, len(self.vocab), bias=False)\n",
    "        self.linear.weight = self.embedding.weight  # Section 3.4\n",
    "        self.final_dropout = nn.Dropout(p=dropout)\n",
    "           \n",
    "    def _position_encoding(self):\n",
    "        d_model = self.d_model\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(self.bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        assert stack.shape == torch.Size([self.d_model, self.bptt_len])\n",
    "        \n",
    "        return stack.T\n",
    "    \n",
    "    def embed(self, indices):\n",
    "        embedded = self.embedding(tensor(indices))\n",
    "        assert embedded.shape == torch.Size([self.bptt_len, self.d_model])\n",
    "        return embedded + self.position_encoding\n",
    "        \n",
    "    def forward(self, indices):\n",
    "        # Embed\n",
    "        embedded = self.initial_dropout(self.embed(indices))\n",
    "        #print('embedded:', embedded)\n",
    "\n",
    "        # Encode\n",
    "        #encoded= self.encoder(embedded)\n",
    "        \n",
    "        # Decoder\n",
    "        #decoded = self.decoder(encoded)\n",
    "        decoded = self.decoder(embedded)\n",
    "        #print('decoded:', decoded)\n",
    "\n",
    "        # Dis-embed\n",
    "        predictions = self.final_dropout(self.linear(decoded))\n",
    "        #print('predictions:', predictions)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(vocab, data_file=conf['datafile']):\n",
    "    with open(data_file,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            yield line.strip().lower()\n",
    "            #yield get_indices(string, vocab, include_shifted=True)\n",
    "\n",
    "def make_vocab(data_file=conf['datafile']):\n",
    "    with open(data_file,'r') as f:\n",
    "        vocab = torchtext.vocab.build_vocab_from_iterator(f.read().replace('\\n','').lower())\n",
    "    return vocab\n",
    "\n",
    "def get_indices(string, vocab, max_tokens=conf['bptt_len'], include_shifted=False):\n",
    "    tokens = list(string.strip().lower())\n",
    "    tokens = tokens[:max_tokens]\n",
    "    pad_len = max_tokens - len(tokens)\n",
    "    tokens.extend(['<pad>'] * pad_len)\n",
    "    indices = list(map(lambda x: vocab.stoi[x], tokens))\n",
    "    t = tensor(indices)\n",
    "    if include_shifted:\n",
    "        t_shifted = tensor([0] + indices[:-1])\n",
    "        return t, t_shifted\n",
    "    else:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_string(model, vocab, string, criterion, optimizer):\n",
    "    x, y = get_indices(string, vocab, include_shifted=True)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    print('y_pred:', y_pred)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()            \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, vocab, criterion, optimizer, epoch='    '):\n",
    "    losses = []\n",
    "    for string in get_data(vocab=vocab):\n",
    "        loss = train_string(model, vocab, string, criterion, optimizer)\n",
    "        losses.append(loss.item())\n",
    "        #sys.stdout.write('.')\n",
    "        print(epoch, 'string:', string, 'loss:', loss) \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vocab, criterion, optimizer, max_epochs=conf['max_epochs']):\n",
    "    for epoch in range(max_epochs):\n",
    "        loss = do_epoch(model, vocab, criterion, optimizer)\n",
    "        print('epoch:', epoch, 'loss:', loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "# Make the vocabulary\n",
    "vocab = make_vocab()\n",
    "\n",
    "# define the model\n",
    "model = Transformer(vocab)\n",
    "model = model.half().to(device)\n",
    "model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = getattr(torch.optim, conf['optimizer'])(model.parameters(), lr=conf['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_string(model, vocab, 'Atlanta', criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_epoch(model, vocab, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, vocab, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, out_indices = torch.max(self.softmax(decoded), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def next_predicted_token(model, vocab, string, beam_width=1):\n",
    "    model = model.eval()\n",
    "    y_pred=model(string)\n",
    "    y_pred_list = list(y_pred.squeeze())\n",
    "    max_val = max(y_pred_list)\n",
    "    index = y_pred_list.index(max_val)\n",
    "    predicted_token = vocab.itos[index]\n",
    "    return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_prompt(model, vocab, prompt, beam_width=1):\n",
    "    output = vocab.tokenize(prompt)\n",
    "    \n",
    "    next_token = None\n",
    "    while next_token != '<EOS>' and len(output) <= conf['bptt_len']:\n",
    "        next_token = next_predicted_token(model, vocab, vocab.detokenize(output))\n",
    "        output.append(next_token)\n",
    "\n",
    "    return vocab.detokenize(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(nn.Module):\n",
    "    def __init__(self, \n",
    "                 data_file=conf['datafile'], \n",
    "                 d_model=conf['d_model'], \n",
    "                 split_field='', \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 device=device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_file = data_file\n",
    "        self.split_field = split_field\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "        self.stoe = {}\n",
    "        self.freq = defaultdict(int)\n",
    "        self._register_token('<EOS>')\n",
    "\n",
    "        for line in self.load_strings():\n",
    "            for token in self.tokenize(line):\n",
    "                self._register_token(token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def _register_token(self, token):\n",
    "        if not token in self.stoi:\n",
    "            self.itos.append(token)\n",
    "            self.stoi[token] = len(self) - 1\n",
    "            self.stoe[token] = torch.randn(self.d_model, device=device, dtype=torch.half, requires_grad=True)\n",
    "        self.freq[token] += 1\n",
    "\n",
    "    def tokenize(self, string):\n",
    "        if self.split_field == '':\n",
    "            ret = list(string)\n",
    "        else:\n",
    "            ret = string.split(self.split_field)\n",
    "        tokens = list(map(str.lower, ret))\n",
    "        tokens = tokens[:self.bptt_len - 1] + ['<EOS>']\n",
    "        return tokens\n",
    "    \n",
    "    def detokenize(self, tokens):\n",
    "        if tokens[-1] == '<EOS>':\n",
    "            tokens.pop()\n",
    "        return ''.join(tokens)            \n",
    "\n",
    "    def load_strings(self, shuffle=True):\n",
    "        with open(self.data_file, 'r') as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        if shuffle:\n",
    "            random.shuffle(lines)\n",
    "        return lines\n",
    "\n",
    "    def embed(self, string):\n",
    "        tokens = self.tokenize(string)\n",
    "        if len(tokens) < self.bptt_len:\n",
    "            tokens.extend(['<EOS>'] * (self.bptt_len - len(tokens)))\n",
    "        vectors = [self.stoe[token] for token in tokens]\n",
    "        tensors = list(map(lambda t: t.unsqueeze(0), vectors))\n",
    "        return torch.cat(tensors, 0)\n",
    "                    \n",
    "    def forward(self, string):\n",
    "        return self.embed(string)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_with_prompt(model, vocab, 'N')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
