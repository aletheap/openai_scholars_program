{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer\n",
    "\n",
    "I wrote this to help me understand _Attention Is All You Need_: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "I cut out the Encoder and I'm using it to generate English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import multiprocessing.pool\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Config\n",
    "conf = {\n",
    "        'attn_heads': 8,\n",
    "        'bptt_len': 40,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],  # I need better GPU coolng first\n",
    "        'cuda_device_ids': [1],\n",
    "        'd_model': 512,\n",
    "        #'datafile': './city_names.txt', # from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "        #'datafile': './corncob_lowercase.txt',  # from: http://www.mieliestronk.com/corncob_lowercase.txt\n",
    "        #'datafile': './alphabet_short.txt',  \n",
    "        #'datafile': './dummy_data.txt', \n",
    "        'dataset': 'WikiText2',\n",
    "        #'dataset': 'WikiText103',\n",
    "        'dropout': 0.1,\n",
    "        'final_softmax': False,\n",
    "        #'learning_rate': 0.01,\n",
    "        'epochs_per_loop': 1,\n",
    "        'total_training_loops': 6,\n",
    "        'num_blocks_encoder': 0,\n",
    "        'num_blocks_decoder': 24,\n",
    "        #'minibatch_size': 32 * 16,\n",
    "        'minibatch_size': 16,\n",
    "        'optimizer': 'Adam',  \n",
    "        #'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "        'warmup_steps': 100,\n",
    "        }\n",
    "\n",
    "\n",
    "# debugging\n",
    "#conf['attn_heads'] = 1\n",
    "#conf['d_model'] = 1\n",
    "#conf['bptt_len'] = 2\n",
    "#conf['datafile'] = './dummy_data.txt'  \n",
    "#conf['num_blocks_decoder'] = 1\n",
    "#conf['minibatch_size'] = 1\n",
    "#conf['epochs_per_loop'] = 1\n",
    "\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "assert conf['d_model'] % conf['attn_heads'] == 0, \\\n",
    "    f'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], \n",
    "                                                         conf['d_model'])\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "\n",
    "# Set up the RNGs for repeatability\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "    \n",
    "    \n",
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "print()\n",
    "\n",
    "# I use this bare FIXME:\n",
    "bptt_len = conf['bptt_len']\n",
    "\n",
    "# Logging\n",
    "#wandb = None\n",
    "wandb.init(project=\"my-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.1: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Implements section 3.2.1\"\"\"\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], d_key=conf['d_key'], \n",
    "                 bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_key = d_key\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        if mask:\n",
    "            t_mask = torch.ones([bptt_len, bptt_len]).tril()\n",
    "            self.register_buffer('mask', t_mask)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "        scores = scores / sqrt(self.d_key)\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if self.mask is not None:\n",
    "            this_mask = self.mask[:scores.shape[1], :scores.shape[2]]\n",
    "            scores.masked_fill_(this_mask == 0, float('-inf'))\n",
    "            \n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Section 3.2.2\"\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        attn_heads = [AttentionHead(mask=mask, d_model=d_model, d_key=d_key, bptt_len=bptt_len) \n",
    "                      for _ in range(heads)]\n",
    "        self.attn_heads = nn.ModuleList(attn_heads)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values) \n",
    "                      for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        ret = self.Wo(head_attn)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"Section 3.3\"\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff, bias=False), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"Section 3.1, Encoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(mask=False, d_model=d_model, heads=heads, bptt_len=bptt_len)\n",
    "        self.attn_drop = nn.Dropout(p=dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.attn(x, x, x)\n",
    "        a1 = self.attn_drop(a1)\n",
    "        a1 = self.attn_norm(x + a1) \n",
    "\n",
    "        a2 = self.ffn(a1)\n",
    "        a2 = self.ffn_drop()\n",
    "        a2 = self.ffn_norm(a1 + a2)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"Section 3.1, Decoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.self_attn = MultiHeadAttention(mask=True, d_model=d_model, heads=heads, bptt_len=bptt_len)\n",
    "        self.self_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attn = MultiHeadAttention(mask=False, d_model=d_model, heads=heads, bptt_len=bptt_len)\n",
    "        self.enc_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        a1 = self.self_attn(x, x, x)\n",
    "        a1 = self.self_attn_drop(a1)\n",
    "        a1 = x + a1  # residual\n",
    "        a1 = self.self_attn_norm(a1) \n",
    "        \n",
    "        a2 = self.enc_attn(a1, encoder_out, encoder_out)\n",
    "        a2 = self.enc_attn_drop(a2)\n",
    "        a2 = a1 + a2  # residual\n",
    "        a2 = self.enc_attn_norm(a2)\n",
    "\n",
    "        a3 = self.ffn(a2)\n",
    "        a3 = self.ffn_drop(a3)\n",
    "        a3 = a2 + a3  # residual\n",
    "        a3 = self.ffn_norm(a3)\n",
    "        \n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, bptt_len, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, bptt_len, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_len, \n",
    "                 pad_index,\n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "        \n",
    "        #vocab_len = 1  # FIXME\n",
    "        #pad_index = 0  # FIXME\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "        self.vocab_len = vocab_len\n",
    "        self.pad_index = pad_index\n",
    "  \n",
    "        self.embedding = nn.Embedding(vocab_len, d_model, padding_idx=pad_index)\n",
    "        self.register_buffer('position_encoding', self._position_encoding())\n",
    "        #self.embed_drop = nn.Dropout(p=dropout)\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, bptt_len, num_blocks_encoder, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, bptt_len, num_blocks_decoder, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_len, bias=False)\n",
    "        self.linear_softmax = nn.Softmax(dim=-1)\n",
    "           \n",
    "    def _position_encoding(self):\n",
    "        d_model = self.d_model\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(self.bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        \n",
    "        return stack.T\n",
    "    \n",
    "    def embed(self, indices):\n",
    "        \"\"\"Implements the embedding from Section 3.4 Embeddings and Softmax\"\"\"\n",
    "        this_bptt_len = indices.shape[-1]\n",
    "        pe = self.position_encoding[:this_bptt_len, :]\n",
    "\n",
    "        embedded = self.embedding(indices)\n",
    "        \n",
    "        #print('pe.shape:', pe.shape)\n",
    "        #print('embedded.shape:', embedded.shape)\n",
    "        return pe + embedded\n",
    "        \n",
    "    #def forward(self, encoder_in, encoder_out=None, decoder_in=[]):\n",
    "    def forward(self, encoder_out, decoder_in, pos=None, pre_embedded=False):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        encoder_out: (optional rank-1 tensor) passing this skips \n",
    "                     the encoder execution, and acts and if this were \n",
    "                     the indices the encoder produced.\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "\n",
    "        # Embed\n",
    "        if pre_embedded:\n",
    "            eo_embedded = encoder_out\n",
    "            di_embedded = decoder_in\n",
    "        else:\n",
    "            eo_embedded = self.embed(encoder_out) \n",
    "            di_embedded = self.embed(decoder_in)\n",
    "        \n",
    "        # Encode\n",
    "        #encoded = self.encoder(self.embed(encoder_in))\n",
    "        encoded = eo_embedded\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.decoder(encoded, di_embedded)\n",
    "\n",
    "        # Return predictions for next token\n",
    "        if pos is not None:\n",
    "            decoded = decoded[:, pos, :]\n",
    "        \n",
    "        y_pred = self.linear(decoded)\n",
    "        \n",
    "        if conf['final_softmax']:\n",
    "            y_pred = self.linear_softmax(y_pred)        \n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Build the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def load_dataset(path=conf['datafile'], batch_size=conf['minibatch_size'], bptt_len=conf['bptt_len']):\n",
    "    text_field = torchtext.data.Field(batch_first=True, pad_first=True, fix_length=bptt_len * 2,\n",
    "                                      eos_token='.', pad_token='_', unk_token='?')\n",
    "    dataset = LanguageModelingDataset(path=path, text_field=text_field)\n",
    "    text_field.build_vocab(dataset)\n",
    "    train_iter = torchtext.data.BPTTIterator(dataset=dataset, \n",
    "                                             batch_size=batch_size, \n",
    "                                             bptt_len=2 * bptt_len,\n",
    "                                             device=device, \n",
    "                                             shuffle=True)\n",
    "    return train_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and vocab\n",
    "#train_ds = load_dataset()\n",
    "dataloader = getattr(torchtext.datasets, conf['dataset'])\n",
    "train_ds, val_ds, test_ds = dataloader.iters(batch_size=conf['minibatch_size'], \n",
    "                                             bptt_len=2 * conf['bptt_len'], \n",
    "                                             device=device)\n",
    "vocab = train_ds.dataset.fields['text'].vocab\n",
    "\n",
    "pad_token = '_'\n",
    "pad_index = vocab.stoi[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Transformer(len(vocab), pad_index)\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "\n",
    "# Define the Loss\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "\n",
    "def compute_jacobian(inputs, output):\n",
    "    \"\"\"\n",
    "    :param inputs: Batch X Size (e.g. Depth X Width X Height)\n",
    "    :param output: Batch X Classes\n",
    "    :return: jacobian: Batch X Classes X Size\n",
    "    \"\"\"\n",
    "    assert inputs.requires_grad\n",
    "\n",
    "    num_classes = output.size()[1]\n",
    "\n",
    "    jacobian = torch.zeros(num_classes, *inputs.size())\n",
    "    grad_output = torch.zeros(*output.size())\n",
    "    if inputs.is_cuda:\n",
    "        grad_output = grad_output.to(inputs.device)\n",
    "        jacobian = jacobian.to(inputs.device)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        zero_gradients(inputs)\n",
    "        grad_output.zero_()\n",
    "        grad_output[:, i, :] = 1\n",
    "        output.backward(grad_output, retain_graph=True)\n",
    "        jacobian[i] = inputs.grad.data\n",
    "\n",
    "    jacobian = jacobian.squeeze()\n",
    "    jacobian = torch.transpose(jacobian, dim0=0, dim1=1)\n",
    "\n",
    "    return jacobian\n",
    "\n",
    "bptt_len = conf['bptt_len']\n",
    "#model = model.train()\n",
    "#model = model.train()\n",
    "\n",
    "\n",
    "size = (conf['minibatch_size'], conf['bptt_len'], conf['d_model'])\n",
    "#print('size=', size)\n",
    "eo = torch.ones(size, requires_grad=True)\n",
    "di = torch.tensor([[[1],[2.]]], requires_grad=True)\n",
    "assert di.shape == size\n",
    "\n",
    "#output = model.decoder.blocks[0].self_attn(di, di, di)\n",
    "#output = model.decoder.blocks[0].self_attn.attn_heads[0](di, di, di)\n",
    "#queries = model.decoder.blocks[0].self_attn.attn_heads[0].Wq(di)\n",
    "#keys = model.decoder.blocks[0].self_attn.attn_heads[0].Wk(di)\n",
    "#values = model.decoder.blocks[0].self_attn.attn_heads[0].Wv(di)\n",
    "\n",
    "# calculate compatibility function\n",
    "#scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "#scores = scores / sqrt(conf['d_key'])\n",
    "#scores = scores.tril()\n",
    "#attn = torch.matmul(scores, values)\n",
    "#output = attn\n",
    "\n",
    "#output = model.decoder.blocks[0].enc_attn(di, di, di)\n",
    "#decoded = model.decoder(eo_emb, di_emb)\n",
    "#y_pred = model.linear(decoded)\n",
    "#test = torch.tensor([[[1, 2.],[2., 3.]]], requires_grad=True)\n",
    "#model = model.eval()\n",
    "#output = model(eo, di)\n",
    "output = model(eo, di, pre_embedded=True)\n",
    "print('output:', output)\n",
    "\n",
    "#print('eo:', eo)\n",
    "#print('di:', di)\n",
    "#print('y_pred:', y_pred)\n",
    "#print('y_pred.shape:', y_pred.shape)\n",
    "\n",
    "j = compute_jacobian(di, output)\n",
    "print()\n",
    "print('squeezed jacobian:')\n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(dataset=train_ds):\n",
    "    bptt_len = conf['bptt_len']\n",
    "    for batch in dataset:\n",
    "        #print('batch:', batch)\n",
    "        if batch.text.shape[0] < bptt_len + 1:\n",
    "            continue\n",
    "        eo = batch.target.T[:,:bptt_len].contiguous()\n",
    "        y = batch.target.T[:,bptt_len:].contiguous()\n",
    "        di = batch.text.T[:,bptt_len:].contiguous()\n",
    "        di[:,0] = vocab.stoi['?']                                 \n",
    "        yield eo, di, y\n",
    "\n",
    "        \n",
    "def accuracy(output, expected_indices):\n",
    "    indices = torch.max(output, dim=-1)[1]\n",
    "    indices = indices.squeeze()\n",
    "    acc = (indices == expected_indices) / float(indices.numel())\n",
    "    acc = float(acc.sum())\n",
    "    return acc\n",
    "\n",
    "def run_minibatch(encoder_out, decoder_in, y, optimizer):\n",
    "    \"\"\"Runs one minibatch training and returns the loss and accuracy for that minibatch\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    acc = accuracy(y_pred, y)\n",
    "    loss = criterion(y_pred.transpose(-2, -1), y)\n",
    "    loss.backward()  # Not sure why, but this step logs a UserWarning\n",
    "    optimizer.step()\n",
    "    return loss.item(), acc\n",
    "\n",
    "def test_set_accuracy(model):\n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "        accuracies = []\n",
    "        minibatches = 0\n",
    "        for encoder_out, decoder_in, y in get_minibatches(test_ds):\n",
    "            y_pred = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "            accuracies.append(accuracy(y_pred, y))\n",
    "            minibatches += 1\n",
    "    acc = 100 * tensor(accuracies, device=device).float().mean().item()\n",
    "    return acc\n",
    "            \n",
    "def do_epoch(epoch, optimizer, model, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Runs one full training batch and returns the average loss,\n",
    "    accuracy, and duration time in seconds\"\"\"\n",
    "    model = model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    train_accuracies = []\n",
    "    for encoder_out, decoder_in, y in get_minibatches():\n",
    "        #print('eo.shape:', encoder_out.shape, 'di.shape', decoder_in.shape, 'y.shape:', y.shape)\n",
    "        loss, train_acc = run_minibatch(encoder_out, decoder_in, y, optimizer) \n",
    "        losses.append(loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "    #losses = [run_minibatch(*args) for args in get_minibatches(train_ds)]\n",
    "    tf = time.time()\n",
    "    if losses:\n",
    "        avg_loss = tensor(losses, device=device).float().mean().item()\n",
    "        avg_train_accuracy = 100 * tensor(train_accuracies, device=device).float().mean().item()\n",
    "    else:\n",
    "        avg_loss = 0\n",
    "        avg_train_accuracy = 0\n",
    "    avg_test_accuracy = test_set_accuracy(model)\n",
    "    return (avg_loss, avg_train_accuracy, avg_test_accuracy, tf-t0)\n",
    "\n",
    "def train(optimizer, num_epochs=conf['epochs_per_loop'], start_epoch=0, model=model, \n",
    "          vocab=vocab, criterion=criterion):\n",
    "    \"\"\"Runs num_epochs training batches and prints out results\"\"\"\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        loss, train_accuracy, test_accuracy, seconds = do_epoch(epoch, optimizer, model)\n",
    "        if wandb:\n",
    "            wandb.log({'epoch': epoch,\n",
    "                       'loss': loss,\n",
    "                       'train_accuracy': train_accuracy,\n",
    "                       'test_accuracy': test_accuracy,\n",
    "                       'seconds': seconds})\n",
    "        print('epoch:', epoch, '(%.1fs)' % seconds, 'loss=%f' % loss, 'train_accuracy=%.1f%%' % (train_accuracy), 'test_accuracy=%.1f%%' % (test_accuracy))   \n",
    "    return epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Optimizer\n",
    "#optimizer_class = getattr(torch.optim, conf['optimizer']) \n",
    "#lr = conf['learning_rate']\n",
    "#optimizer = optimizer_class(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = conf['learning_rate'] * 10 \n",
    "warmup_steps = conf['warmup_steps']\n",
    "\n",
    "for _ in range(conf['total_training_loops']):\n",
    "    #lr = lr/10\n",
    "    lr = (conf['d_model']**-.5) * min(epoch**-.5, epoch * (warmup_steps**-1.5))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=lr)\n",
    "    #print('lr = %.4f' % lr)\n",
    "    epoch = train(optimizer=optimizer, num_epochs=conf['epochs_per_loop'], start_epoch=epoch)\n",
    "    save_file_name = './my-transformer_%s_%s-layer_%s-epochs.pt' % (conf['dataset'], conf['num_blocks_decoder'], epoch)\n",
    "    torch.save(model.module.state_dict(), save_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.module.state_dict(), './my-transformer-wikitext-2-test-21.4_pct.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Helper Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_alphabet_prompts():\n",
    "    \"\"\"Creates prompts from the alphabet for auto-regression\"\"\"\n",
    "    bptt_len = conf['bptt_len']\n",
    "    letters = list(string.ascii_lowercase)\n",
    "    num_letters = len(letters)\n",
    "\n",
    "    for i in range(num_letters - bptt_len - 1):\n",
    "        j = i + bptt_len\n",
    "        prompt = ''.join(letters[i:j])\n",
    "        expected = ''.join(letters[j:j+bptt_len])\n",
    "        yield prompt, expected\n",
    "        \n",
    "    \n",
    "def alphabet_sample(show_wrong=True):\n",
    "    \"\"\"Samples strings beginning with each letter of the alphabet\"\"\"\n",
    "    total = 0.\n",
    "    right = 0.\n",
    "    log = []\n",
    "    for prompt, y in get_alphabet_prompts():\n",
    "        out = sample(prompt=prompt, deterministic=True, prnt=False)\n",
    "        total += 1\n",
    "        #print('prompt:', prompt, 'out:', out, 'y:', y)\n",
    "        if out == y:\n",
    "            right += 1\n",
    "        if out[0] == y[0]:\n",
    "            log.append(' ')\n",
    "        else:\n",
    "            log.append(out[0])\n",
    "    print('  accuracy = %.1f' % (100. * right/total))\n",
    "    #print(''.join(log) )\n",
    "    #accuracy = 100 * accuracy / num_letters\n",
    "    #if wandb:\n",
    "    #    wandb.log({'alphabet_accuracy': accuracy})\n",
    "    #print('%.1f%% ' % accuracy + ''.join(result))\n",
    "        \n",
    "\n",
    "def accuracy():\n",
    "    eval_model = model.eval()\n",
    "    for encoder_out, decoder_in, y in get_minibatches():\n",
    "        y_pred = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "        _, pred_indices = torch.max(y_pred, dim=-1)\n",
    "        pred_indices.squeeze()\n",
    "        accuracy_m = y == pred_indices \n",
    "        accuracy = 100 * accuracy_m.sum() / float(y.numel())\n",
    "        print('accuracy', float(accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "eos_token = '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(tokens):\n",
    "    \"\"\"Takse a string and returns a tensor of vocab indices for the tokens\"\"\"\n",
    "    indices = list([vocab.stoi[t] for t in tokens])\n",
    "    return torch.tensor([indices]).to(device)\n",
    "\n",
    "def tokenize(indices):\n",
    "    \"Takes a tensor of token indices and returns a string\"\n",
    "    tokens = [vocab.itos[i] for i in indices.squeeze()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_next_token(encoder_out, decoder_in, pos, model=model, deterministic=False):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`.\"\"\"\n",
    "    \n",
    "    decoder_out = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    \n",
    "    if deterministic:\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "    else:\n",
    "        probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "        m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "        _, indices = torch.max(m.sample(), dim=-1)\n",
    "\n",
    "    next_index = int(indices[0,pos])\n",
    "    return next_index, vocab.itos[next_index]\n",
    "\n",
    "def sample(prompt, deterministic=False, vocab=vocab, prnt=True):\n",
    "    \"\"\"Auto-regresses using prompt to create the encoder_out tensor\"\"\"\n",
    "    bptt_len = conf['bptt_len']\n",
    "    prompt_tokens = prompt.split()\n",
    "    assert len(prompt_tokens) == bptt_len, 'Prompt strings must be %s tokens long' % bptt_len    \n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "\n",
    "        encoder_out = numericalize(prompt_tokens)\n",
    "        decoder_in = numericalize([unk_token] + ([pad_token] * (bptt_len - 1)))\n",
    "        out = []\n",
    "        #print('eo = ', encoder_out)\n",
    "        #print('eo.shape = ', encoder_out.shape)\n",
    "        #print('di = ', decoder_in)\n",
    "        #print('di.shape = ', decoder_in.shape)\n",
    "\n",
    "        next_token = None\n",
    "        next_index = None\n",
    "        for pos in range(bptt_len):\n",
    "            next_index, next_token = get_next_token(encoder_out, decoder_in, pos=pos, \n",
    "                                                    model=eval_model, deterministic=deterministic)\n",
    "            if next_token in (eos_token, pad_token):\n",
    "                break\n",
    "            if next_token is not None:\n",
    "                out.append(next_token)\n",
    "                if pos+1 < bptt_len:\n",
    "                    decoder_in[0, pos+1] = next_index\n",
    "        \n",
    "    out = ' '.join(out)\n",
    "    if prnt:\n",
    "        print(prompt + '\\n --> \\n' + out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = \"\"\"Robert <unk> is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John <unk> in 2002 . In 2004 <unk> landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> <unk> Factory in London . He was directed by John <unk> and starred alongside Ben <unk> , Shane <unk> , Harry Kent , Fraser <unk> , Sophie Stanton and Dominic Hall .\"\"\".split()\n",
    "bptt_prompt = prompt_tokens[:bptt_len]\n",
    "prompt = ' '.join(bptt_prompt)\n",
    "#print(prompt)\n",
    "#print(len(prompt.split()))\n",
    "\n",
    "for _ in range(10):\n",
    "    print('--------------')\n",
    "    out = sample(' '.join(prompt_tokens[:bptt_len]))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def predict(encoder_out, decoder_in):\n",
    "    model_eval = model.eval()\n",
    "    eo = numericalize(encoder_out)\n",
    "    di = numericalize(decoder_in)\n",
    "    decoder_out = model_eval(encoder_out=eo, decoder_in=di)\n",
    "    _, indices = torch.max(decoder_out, dim=-1)\n",
    "    out_tokens = tokenize(indices)\n",
    "    return out_tokens\n",
    "\n",
    "for i in range(len(string.ascii_lowercase) - 2*bptt_len +1):\n",
    "    j = i + bptt_len\n",
    "    k = j + bptt_len\n",
    "    eo = string.ascii_lowercase[i:j]\n",
    "    for q in range(bptt_len):\n",
    "        di = '?' + string.ascii_lowercase[j:j+q] + '_' * (bptt_len - q -1)\n",
    "        out = predict(eo, di)\n",
    "        out = out\n",
    "        acc = out[:q+1] == string.ascii_lowercase[j:j+q+1]\n",
    "        print('%r  %r  ->  %r  %r' % (eo, di, out, acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_eval = model.eval()\n",
    "for eo, di, y in get_minibatches():\n",
    "    for i in range(eo.shape[0]):\n",
    "        eo_s = eo[i,:].unsqueeze(0)\n",
    "        di_s = di[i,:].unsqueeze(0)\n",
    "        y_s = y[i,:].unsqueeze(0)\n",
    "        #print(eo_s)\n",
    "        decoder_out = model(encoder_out=eo_s, decoder_in=di_s)\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "        eo_t, di_t, y_t = tokenize(eo_s), tokenize(di_s), tokenize(y_s)\n",
    "        out = tokenize(indices)\n",
    "        acc = out == y_t\n",
    "        print('eo=%r  di=%r  ->  out=%r  y=%r  (%r)' % (eo_t, di_t, out, y_t, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_accuracy(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being\"\"\"\n",
    "\n",
    "for _ in range(5):\n",
    "    sample(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"There was the problematization of madness and illness arising out of social and medical practices , and defining a certain pattern of “ normalization “ ; a problematization of life , language , and labor in discursive practices that conformed\"\"\"\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"========================\")\n",
    "    sample(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
