{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer\n",
    "\n",
    "I wrote this to help me understand _Attention Is All You Need_: https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict, Counter\n",
    "import multiprocessing.pool\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import sqrt, sin, cos, floor, mean, exp\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "import torchtext.datasets\n",
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.profiler import AdvancedProfiler\n",
    "#from pytorch_lightning.callbacks.lr_logger import LearningRateLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.1: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Implements section 3.2.1\"\"\"\n",
    "    def __init__(self, d_model, d_key):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_key = d_key\n",
    "\n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "        scores = scores / sqrt(self.d_key)\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, float('-inf'))\n",
    "            \n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Section 3.2.2\"\n",
    "    def __init__(self, d_model, heads):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        attn_heads = [AttentionHead(d_model, d_key) for _ in range(heads)]\n",
    "        self.attn_heads = nn.ModuleList(attn_heads)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values, mask=mask) \n",
    "                      for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        ret = self.Wo(head_attn)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"Section 3.3\"\n",
    "    def __init__(self, d_model, multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff, bias=False), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"Section 3.1, Encoder\"\n",
    "    def __init__(self, d_model, heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(d_model=d_model, heads=heads)\n",
    "        self.attn_drop = nn.Dropout(p=dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.attn(x, x, x)\n",
    "        a1 = self.attn_drop(a1)\n",
    "        a1 = self.attn_norm(x + a1) \n",
    "\n",
    "        a2 = self.ffn(a1)\n",
    "        a2 = self.ffn_drop(a2)\n",
    "        a2 = self.ffn_norm(a1 + a2)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"Section 3.1, Decoder\"\n",
    "    def __init__(self, d_model, heads, dropout, encoder_attention=True):\n",
    "        super().__init__()\n",
    "           \n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, heads)\n",
    "        self.self_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        if encoder_attention:\n",
    "            self.enc_attn = MultiHeadAttention(d_model, heads)\n",
    "            self.enc_attn_drop = nn.Dropout(p=dropout)\n",
    "            self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "        else:\n",
    "            self.enc_attn = None\n",
    "            \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out=None, self_attn_mask=None):\n",
    "        a1 = self.self_attn(x, x, x, self_attn_mask)\n",
    "        a1 = self.self_attn_drop(a1)\n",
    "        a1 = x + a1  # residual\n",
    "        a1 = self.self_attn_norm(a1) \n",
    "        \n",
    "        if self.enc_attn is None:\n",
    "            a2 = a1\n",
    "        else:\n",
    "            a2 = self.enc_attn(a1, encoder_out, encoder_out)\n",
    "            a2 = self.enc_attn_drop(a2)\n",
    "            a2 = a1 + a2  # residual\n",
    "            a2 = self.enc_attn_norm(a2)\n",
    "\n",
    "        a3 = self.ffn(a2)\n",
    "        a3 = self.ffn_drop(a3)\n",
    "        a3 = a2 + a3  # residual\n",
    "        a3 = self.ffn_norm(a3)\n",
    "        \n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_blocks, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_blocks, dropout, encoder_attention=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, dropout, encoder_attention) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, decoder_in, encoder_out=None, self_attn_mask=None):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out, self_attn_mask)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        bptt_len = hparams.bptt_len\n",
    "        d_model = hparams.d_model\n",
    "        decoder_layers = hparams.decoder_layers \n",
    "        dropout = hparams.dropout\n",
    "        encoder_layers = hparams.encoder_layers\n",
    "        encoder_decoder_attention = encoder_layers > 0\n",
    "        heads = hparams.attn_heads \n",
    "      \n",
    "        self.prepare_data()\n",
    "        \n",
    "        self.vocab = self.train_dataset.fields['text'].vocab\n",
    "        vocab_len = len(self.vocab)\n",
    "        pad_token = '<pad>'\n",
    "        pad_index = self.vocab.stoi[pad_token]       \n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_len, d_model, padding_idx=pad_index)\n",
    "        self.register_buffer('position_encoding', self._position_encoding(bptt_len, d_model))\n",
    "        self.register_buffer('self_attn_mask', self._make_mask(bptt_len))\n",
    "                                            \n",
    "        self.encoder = Encoder(d_model, heads, encoder_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, decoder_layers, dropout, encoder_decoder_attention)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_len, bias=False)\n",
    "           \n",
    "        num_params = sum([np.prod(p.shape) for p in self.parameters()])\n",
    "        print(f\"Model has {num_params:,d} parameters\")\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--attn_heads', type=int, default=8)\n",
    "        parser.add_argument('--bptt_len', type=int, default=360)\n",
    "        parser.add_argument('--d_model', type=int, default=512)\n",
    "        parser.add_argument('--dataset', type=str, default='WikiText103')\n",
    "        #parser.add_argument('--dataset', type=str, default='WikiText2')\n",
    "        parser.add_argument('--dropout', type=float, default=0.1)\n",
    "        parser.add_argument('--encoder_layers', type=int, default=0)\n",
    "        parser.add_argument('--decoder_layers', type=int, default=6)\n",
    "        parser.add_argument('--max_vocab_size', type=int, default=60000)\n",
    "        parser.add_argument('--minibatch_size', type=int, default=16)\n",
    "        parser.add_argument('--warmup_steps', type=int, default=2500)\n",
    "        parser.add_argument('--lr_multiplier', type=float, default=1)\n",
    "        return parser\n",
    "            \n",
    "    @classmethod\n",
    "    def _make_mask(cls, bptt_len):\n",
    "        return torch.ones([bptt_len, bptt_len]).tril()\n",
    "\n",
    "    @classmethod\n",
    "    def _position_encoding(cls, bptt_len, d_model):\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        \n",
    "        return stack.T\n",
    "\n",
    "    def embed(self, indices):\n",
    "        \"\"\"Implements the embedding from Section 3.4 Embeddings and Softmax\"\"\"\n",
    "        this_bptt_len = indices.shape[-1]\n",
    "        pe = self.position_encoding[:this_bptt_len, :]\n",
    "\n",
    "        embedded = self.embedding(indices)\n",
    "        \n",
    "        return pe + embedded\n",
    "        \n",
    "    def forward(self, decoder_in=None, encoder_in=None, pos=None):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "\n",
    "        this_bptt_len = decoder_in.shape[-1]\n",
    "\n",
    "        # Encode\n",
    "        if self.hparams.encoder_layers > 0:\n",
    "            ei_embedded = self.embed(encoder_in)\n",
    "            encoded = self.encoder(ei_embedded)\n",
    "        else:\n",
    "            encoded = None\n",
    "        \n",
    "        # Decode\n",
    "        if self.hparams.decoder_layers > 0:\n",
    "            di_embedded = self.embed(decoder_in)\n",
    "            self_attn_mask = self.self_attn_mask[:this_bptt_len, :this_bptt_len]\n",
    "            decoded = self.decoder(di_embedded, encoded, self_attn_mask)\n",
    "        else:\n",
    "            decoded = encoded\n",
    "        \n",
    "        # Return predictions for next token\n",
    "        if pos is not None:\n",
    "            decoded = decoded[:, pos, :]\n",
    "        \n",
    "        y_hat = self.linear(decoded)\n",
    "                \n",
    "        return y_hat\n",
    "\n",
    "    def _accuracy(self, output, expected_indices):\n",
    "        indices = torch.max(output, dim=-2)[1]\n",
    "        indices = indices.squeeze()\n",
    "        acc = (indices == expected_indices) / float(indices.numel())\n",
    "        acc = acc.sum() * 100\n",
    "        return acc\n",
    "\n",
    "    def _step(self, batch, batch_idx):\n",
    "        x = batch.text.T\n",
    "        y = batch.target.T \n",
    "        y_hat = self(decoder_in=x)\n",
    "        y_hat = y_hat.transpose(-2, -1)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = self._accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch, batch_idx)\n",
    "        perplexity = torch.exp(loss)\n",
    "        schedulers = self.trainer.lr_schedulers[0]\n",
    "        lr = schedulers['scheduler'].optimizer.param_groups[0]['lr']\n",
    "        logs = {'train_loss': loss, \n",
    "                'train_perplexity': perplexity, \n",
    "                'train_accuracy': accuracy, \n",
    "                'learning_rate': lr,\n",
    "               }\n",
    "        return {'loss': loss, 'log': logs}\n",
    "            \n",
    "    def _scheduler_lr(self, step):\n",
    "        \"\"\"return the learning rate for this step.\"\"\"\n",
    "        step = step + 1  # handle step 0\n",
    "        lr = (self.hparams.d_model**-.5) * min(step**-.5, step * (self.hparams.warmup_steps**-1.5))\n",
    "        return lr\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=self.hparams.lr_multiplier)\n",
    "        schedulers = [{\n",
    "                         'scheduler': LambdaLR(optimizer, lr_lambda=self._scheduler_lr), \n",
    "                         'interval': 'step',\n",
    "                         'frequency': 1\n",
    "                      }]\n",
    "        return [optimizer], schedulers\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch, batch_idx)\n",
    "        logs = {'val_loss': loss, 'val_accuracy': accuracy}\n",
    "        return logs\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        perplexity = torch.exp(avg_loss)\n",
    "        avg_accuracy = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n",
    "        logs = {'val_loss': avg_loss, 'val_accuracy': avg_accuracy, 'val_perplexity': perplexity}\n",
    "        return {'val_loss': avg_loss, 'log': logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self._step(batch, batch_idx)\n",
    "        logs = {'test_loss': loss, 'test_accuracy': accuracy}\n",
    "        return logs\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        perplexity = torch.exp(avg_loss)\n",
    "        avg_accuracy = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n",
    "        logs = {'test_loss': avg_loss, 'test_accuracy': avg_accuracy, 'test_perplexity': perplexity}\n",
    "        return {'test_loss': avg_loss, 'log': logs}\n",
    "\n",
    "    def prepare_data(self):    \n",
    "        dataloader = getattr(torchtext.datasets, self.hparams.dataset)\n",
    "        TEXT = torchtext.data.Field()\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = dataloader.splits(TEXT)\n",
    "        TEXT.build_vocab(self.train_dataset, max_size=self.hparams.max_vocab_size)\n",
    "\n",
    "    def _make_bptt_iterator(self, dataset):\n",
    "        device = self.embedding.weight.device\n",
    "        return torchtext.data.BPTTIterator(dataset, \n",
    "                                           batch_size=self.hparams.minibatch_size, \n",
    "                                           bptt_len=self.hparams.bptt_len,\n",
    "                                           device=device)\n",
    "    def train_dataloader(self):\n",
    "        #Load the dataset\n",
    "        return self._make_bptt_iterator(self.train_dataset)\n",
    " \n",
    "    def val_dataloader(self):\n",
    "        #Load val dataset\n",
    "        return self._make_bptt_iterator(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        #Load test data\n",
    "        return self._make_bptt_iterator(self.test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--random_seed', type=int, default=8)\n",
    "#parser.add_argument('--cuda_device_id', type=int, default=0)\n",
    "parser = Transformer.add_model_specific_args(parser)\n",
    "#parser = Trainer.add_argparse_args(parser)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "assert args.d_model % args.attn_heads == 0, \\\n",
    "    f'attn_heads=%s does not evenly divide d_model=%s' % (args.attn_heads, \n",
    "                                                          args.d_model)\n",
    "args.d_key = args.d_model / args.attn_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the RNGs for repeatability\n",
    "if args.random_seed:\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(args.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = 'perplexity-47_heads-8_decoder-layers-6_dmodel-512_bptt-360.ckpt'\n",
    "#PATH = 'pl_checkpoints-expt2/_ckpt_epoch_4.ckpt'\n",
    "#model = Transformer.load_from_checkpoint(checkpoint_path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Transformer(args)\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir tb_logs/my_transformer --bind_all serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=WandbLogger(project='my-transformer')\n",
    "\n",
    "checkpoint_dir = './checkpoints/wandb-%s/' % logger.experiment.id\n",
    "\n",
    "trainer = Trainer(gpus=1,\n",
    "                  #distributed_backend='dp',\n",
    "                  max_epochs=10, \n",
    "                  val_check_interval=0.05,\n",
    "                  profiler=True,\n",
    "                  #profiler=AdvancedProfiler(),\n",
    "                  checkpoint_callback=ModelCheckpoint(filepath=checkpoint_dir),\n",
    "                  early_stop_callback=EarlyStopping(patience=0),\n",
    "                  logger=logger, \n",
    "                  #logger=TensorBoardLogger(\"tb_logs\", name=\"my_transformer\")\n",
    "                 )\n",
    "\n",
    "trainer.fit(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.profiler.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_checkpoint(\"6-layers_512-d_model_bptt-60.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "def numericalize(string, model, pad=True):\n",
    "    \"\"\"Takse a string and returns a tensor of indices for the tokens\"\"\"\n",
    "    bptt_len = args.bptt_len\n",
    "    TEXT_FIELD = model.train_dataset.fields['text']\n",
    "    tokens = TEXT_FIELD.tokenize(string)\n",
    "    tokens = tokens[-bptt_len:]\n",
    "    t = TEXT_FIELD.numericalize([tokens]).T\n",
    "    num_tokens = t.shape[1]\n",
    "    if pad:\n",
    "        pad_len = max(0, bptt_len - num_tokens)\n",
    "        pad_t = torch.zeros([1, pad_len]).long()\n",
    "        t = torch.cat((t, pad_t), dim=1)\n",
    "    return t, num_tokens\n",
    "\n",
    "def tokenize(indices, vocab):\n",
    "    \"Takes a tensor of token indices and returns a string\"\n",
    "    vocab = model.train_dataset.fields['text'].vocab\n",
    "    tokens = [vocab.itos[i] for i in indices.squeeze()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_next_token(decoder_in, vocab, pos, model=model, deterministic=False,\n",
    "                  ):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`.\"\"\"\n",
    "    \n",
    "    #print('decoder_in=', decoder_in)\n",
    "    #print('pos=', pos)\n",
    "    #print('decoder_in[0,pos]=', decoder_in[0,pos])\n",
    "    #if pos + 1 < decoder_in.shape[1]:\n",
    "        #print('decoder_in[0,pos+1]=', decoder_in[0,pos+1])\n",
    "    decoder_out = model(decoder_in=decoder_in)\n",
    "    #print('decoder_out=', decoder_out)\n",
    "    #print('decoder_out[o,pos,:]=', decoder_out[0,pos,:])\n",
    "    \n",
    "    if deterministic:\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "    else:\n",
    "        probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "        m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "        _, indices = torch.max(m.sample(), dim=-1)\n",
    "\n",
    "    next_index = int(indices[0,pos])\n",
    "    return next_index, vocab.itos[next_index]\n",
    "\n",
    "def sample(prompt, model, deterministic=False, prnt=True):\n",
    "    \"\"\"Auto-regresses using prompt to create the encoder_out tensor\"\"\"\n",
    "    bptt_len = args.bptt_len\n",
    "    vocab = model.train_dataset.fields['text'].vocab\n",
    "    di, num_tokens = numericalize(prompt, model=model)\n",
    "    #di = di.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "        out = []\n",
    "        next_token = None\n",
    "        next_index = None\n",
    "        pos = num_tokens - 1\n",
    "        #print(pos)\n",
    "        for _ in range(2 * bptt_len):\n",
    "            #print('di =', di)\n",
    "            next_index, next_token = get_next_token(di, vocab, pos=pos, model=eval_model, deterministic=deterministic)\n",
    "            #print('next_index =', next_index)\n",
    "            #print('next_token =', next_token)\n",
    "            if next_token in (eos_token, pad_token):\n",
    "                break\n",
    "            if next_token is not None:\n",
    "                out.append(next_token)\n",
    "                pos += 1\n",
    "                if pos >= di.shape[1]:\n",
    "                    di = torch.roll(di, -1, -1)\n",
    "                    pos -= 1\n",
    "                di[0, pos] = next_index\n",
    "        \n",
    "    out = ' '.join(out)\n",
    "    if prnt:\n",
    "        print(prompt + '\\n --> \\n' + out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()\n",
    "for i, _ in enumerate(range(5)):\n",
    "    print()\n",
    "    print('Completion #%s:' % i)\n",
    "    #print('-' * 20)\n",
    "    out = sample(prompt, model=model, deterministic=False, prnt=False)\n",
    "    print(prompt, '-->', out)\n",
    "    #print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Albert Einstein ( 14 March 1879 @â€“@ 18 April 1955 ) was a German @-@ born theoretical physicist who developed the theory of relativity , one of the two pillars of modern physics ( alongside quantum mechanics ) . His work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()\n",
    "for i, _ in enumerate(range(5)):\n",
    "    print()\n",
    "    print('Completion #%s:' % i)\n",
    "    #print('-' * 20)\n",
    "    out = sample(prompt, model=model, deterministic=False, prnt=False)\n",
    "    print(prompt, '-->', out)\n",
    "    #print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['The',\n",
    "           'Of',\n",
    "           'To',\n",
    "           'In',\n",
    "           'A', \n",
    "           'Was',\n",
    "           'The',\n",
    "           'On',\n",
    "           'That',\n",
    "           'For',\n",
    "           'As',\n",
    "           'With']\n",
    "for prompt in prompts:\n",
    "    #print('Prompt:')\n",
    "    #print(\"=\" * 40)\n",
    "    #print(prompt, '...')\n",
    "    #print(\"=\" * 40)\n",
    "    for i, _ in enumerate(range(1)):\n",
    "        print()\n",
    "        #print('Completion #%s:' % i)\n",
    "        #print('-' * 20)\n",
    "        out = sample(prompt, model=model, deterministic=False, prnt=False)\n",
    "        print(prompt, '-->', out)\n",
    "        #print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"5 m (\"\"\"\n",
    "sample(prompt, model=model, deterministic=True, prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"Rollerblade is a brand of inline skates owned by Nordica, part of the Tecnica Group of Giavera del Montello, Treviso, Italy.[4][5]The company was started by Scott Olson (b. 1960) and Brennan Olson (b. 1964) in Minneapolis as Ole's Innovative Sports; when they sold the company, it became Rollerblade, Inc.[6] and has changed hands over time between Nordica, Benetton Group and Tecnica.[7]Even though the long established Roces company was the first to\"\"\"\n",
    "\n",
    "for i, _ in enumerate(range(5)):\n",
    "    print()\n",
    "    print('Completion #%s:' % i)\n",
    "    #print('-' * 20)\n",
    "    out = sample(prompt, model=model, deterministic=False, prnt=False)\n",
    "    print(prompt, '-->', out)\n",
    "    #print('-' * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
