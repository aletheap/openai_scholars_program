{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            'embed_dim': 512,\n",
    "            'key_dim': 64,\n",
    "\n",
    "    \n",
    "            'batch_size': 400,\n",
    "            #'dataset': 'imagenette2-320',\n",
    "            'datadir': '/home/apower/data/text/wikitext-2,\n",
    "            'dropout': 0.1,\n",
    "            'init_gain': 5,\n",
    "            'initializer': None,\n",
    "            'learning_rate': 0.1,\n",
    "            'load_workers': os.cpu_count(), \n",
    "            'max_epochs': 1000,\n",
    "            'optimizer': 'SGD',\n",
    "            'random_seed': 1,\n",
    "            'training_loops': 4,\n",
    "            'cuda_device_ids': [0, 1, 2],\n",
    "num_hidden_nodes = 300\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "learning_rate = 0.1\n",
    "embedding_dim = 300\n",
    "bptt_len = 5\n",
    "data_dir = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my-language-model.ipynb  my-transformer.ipynb  rnn.py  Untitled.ipynb  \u001b[0m\u001b[01;34mwandb\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8):\n",
    "        super().__init__()\n",
    "        k_d = embedding_dim / attention_heads\n",
    "        self.Wq = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.Wk = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.Wv = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, in_vectors):\n",
    "        # in_vectors.shape = (number of vectors, embedding_dimension)\n",
    "        queries = torch.matmul(in_vectors, self.Wq) #shape = (heads, num vectors, k_d)\n",
    "        keys = torch.matmul(in_vectors, self.Wk) #shape = (heads, num vectors, k_d)\n",
    "        values = torch.matmul(in_vectors, self.Wv) #shape = (heads, num vectors, k_d)\n",
    "        k_d = keys.size[2]\n",
    "\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, 1,2)) #shape = (heads, num vectors, num vectors)\n",
    "        normalized_scores = self.softmax(scores / torch.sqrt(k_d)) #shape = (heads, num vectors, num vectors)\n",
    "        Zi = torch.matmul(normalized_scores, values)  #shape = (heads, num vectors, k_d)\n",
    "        Z = torch.squeeze(torch.cat(torch.split(Zi, 1, dim=0), 2)) #shape = (num vectors, embedding_dim)\n",
    "\n",
    "        return Z  # shape = (num vectors, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(embedding_dim=embedding_dim, attention_heads=attention_heads)\n",
    "        self.ffnn = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, in_vectors):\n",
    "        a1 = nn.LayerNorm(in_vectors + self.attention(in_vectors))\n",
    "        a2 = nn.LayerNorm(a1 + self.ffnn(a1))\n",
    "        return a2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8, num_blocks=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = torch.randn(vocab_len, embedding_dim)\n",
    "        \n",
    "        blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            blocks.append(EncoderBlock(embedding_dim=embedding_dim, attention_heads=attention_heads))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        vectors = blah blah tokens  # FIXME: embed tokens\n",
    "        positional_vectors = blah blah position  #FIXME: positional offsets\n",
    "        return self.blocks(vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention():\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8):\n",
    "        super().__init__()\n",
    "        k_d = embedding_dim / attention_heads\n",
    "        self.Wq = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.Wk = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.Wv = torch.randn((attention_heads, embedding_dim, k_d))\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, in_vectors, encoder_vectors):\n",
    "        # in_vectors.shape = (number of vectors, embedding_dimension)\n",
    "        queries = torch.matmul(in_vectors, self.Wq) #shape = (heads, num vectors, k_d)\n",
    "        keys = torch.matmul(encoder_vectors, self.Wk) #shape = (heads, num vectors, k_d)\n",
    "        values = torch.matmul(encoder_vectors, self.Wv) #shape = (heads, num vectors, k_d)\n",
    "        k_d = keys.size[2]\n",
    "\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, 1,2)) #shape = (heads, num vectors, num vectors)\n",
    "        normalized_scores = self.softmax(scores / torch.sqrt(k_d)) #shape = (heads, num vectors, num vectors)\n",
    "        Zi = torch.matmul(normalized_scores, values)  #shape = (heads, num vectors, k_d)\n",
    "        Z = torch.squeeze(torch.cat(torch.split(Zi, 1, dim=0), 2)) #shape = (num vectors, embedding_dim)\n",
    "\n",
    "        return Z  # shape = (num vectors, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # FIXME: mask out future self-attention\n",
    "        self.self_attention = SelfAttention(embedding_dim=embedding_dim, attention_heads=attention_heads)\n",
    "        self.enc_attention = EncoderDecoderAttention(embedding_dim=embedding_dim, attention_heads=attention_heads)\n",
    "        self.ffnn = torch.nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, (in_vectors, encoder_vectors)):\n",
    "        a1 = nn.LayerNorm(in_vectors + self.self_attention(in_vectors))\n",
    "        a2 = nn.LayerNorm(a1 + self.enc_attention(a1, encoder_vectors))\n",
    "        a3 = nn.LayerNorm(a2 + self.ffnn(a1))\n",
    "        return (a3, encoder_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, attention_heads=8, num_blocks=6):\n",
    "        super().__init__()\n",
    "                \n",
    "        blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            blocks.append(DecoderBlock(embedding_dim=embedding_dim, attention_heads=attention_heads))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, encoder_vectors):\n",
    "        return self.blocks((encoder_vectors, encoder_vectors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encode = Encoder(embedding_dim=512, attention_heads=8, num_blocks=6)\n",
    "        self.decode = Decoder(embedding_dim=512, attention_heads=8, num_blocks=6)\n",
    "\n",
    "    def embed(self, words):\n",
    "        ...\n",
    "        return embedded_vectors\n",
    "    \n",
    "    def forward(self, words):\n",
    "        embedded_vectors = self.embed(words)\n",
    "        return self.decode(self.encode(embedded_vectors))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
