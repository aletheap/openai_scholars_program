{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import sqrt, sin, cos\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/aletheap/my-transformer\" target=\"_blank\">https://app.wandb.ai/aletheap/my-transformer</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/aletheap/my-transformer/runs/cvyncisv\" target=\"_blank\">https://app.wandb.ai/aletheap/my-transformer/runs/cvyncisv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.32 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/aletheap/my-transformer/runs/cvyncisv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = {\n",
    "        'attn_heads': 4,\n",
    "        'bptt_len': 25,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],\n",
    "        'cuda_device_ids': [3],\n",
    "        'd_model': 20,\n",
    "        #'datafile': './city_names.txt', # from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "        'datafile': './corncob_lowercase.txt',  # from: http://www.mieliestronk.com/corncob_lowercase.txt\n",
    "        'dropout': 0.1,\n",
    "        'learning_rate': 0.1,\n",
    "        'epochs_per_loop': 50,\n",
    "        'total_training_loops': 30,\n",
    "        #'max_epochs': 100,\n",
    "        'num_blocks_encoder': 1,\n",
    "        'num_blocks_decoder': 2,\n",
    "        'minibatch_size': 50000,\n",
    "        #'optimizer': 'Adam'\n",
    "        'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "           \n",
    "        #'batch_size': 400,\n",
    "        #'dataset': 'imagenette2-320',\n",
    "        #'init_gain': 5,\n",
    "        #'initializer': None,\n",
    "        #'load_workers': os.cpu_count(), \n",
    "        #'training_loops': 4,\n",
    "        #'cuda_device_ids': [0, 1, 2],\n",
    "        #'num_hidden_nodes': 300,\n",
    "        }\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "assert conf['d_key'] == int(conf['d_key']), 'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], conf['d_model'])\n",
    "\n",
    "# Set up the RNGs\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "\n",
    "# Logging\n",
    "wandb.init(project=\"my-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, mask=True, d_model=conf['d_model'], d_key=conf['d_key'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_key = d_key\n",
    "        self.bptt_len = bptt_len\n",
    "\n",
    "        if mask:\n",
    "            self.mask = nn.Parameter((np.NINF * torch.ones([bptt_len, bptt_len])).triu(1), requires_grad=False)\n",
    "        else:\n",
    "            self.mask = None\n",
    "        \n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) / sqrt(self.d_key) #shape = (heads, bptt_len, bptt_len)\n",
    "        #assert scores.shape == torch.Size([self.bptt_len, self.bptt_len])\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if self.mask is not None:\n",
    "            scores = scores.tril() + self.mask\n",
    "\n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "        #assert attn.shape == torch.Size([self.bptt_len, self.d_key])\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, mask=False, d_model=conf['d_model'], heads=conf['attn_heads'], bptt_len=conf['bptt_len']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(mask, d_model, d_key, bptt_len) for _ in range(heads)])\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values) for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        return self.Wo(head_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(False, d_model, heads, bptt_len)\n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.ffn_norm(a1 + self.ffn_dropout(self.ffn(a1)))\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 mask_all=False):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.self_attn = MultiHeadAttention(True, d_model, heads, bptt_len)\n",
    "        \n",
    "        self.self_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.enc_attn = MultiHeadAttention(mask_all, d_model, heads, bptt_len)\n",
    "        self.enc_attn_dropout = nn.Dropout(p=dropout)\n",
    "        self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_dropout = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out):\n",
    "        a1 = self.self_attn_norm(x + self.self_attn_dropout(self.self_attn(x, x, x)))\n",
    "        a2 = self.enc_attn_norm(a1 + self.enc_attn_dropout(self.enc_attn(a1, encoder_out, encoder_out)))\n",
    "        a3 = self.ffn_norm(a2 + self.ffn_dropout(self.ffn(a2)))\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, bptt_len, dropout) for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 bptt_len=conf['bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.bptt_len = bptt_len\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(vocab), d_model, padding_idx=vocab.stoi['<pad>'])\n",
    "        self.position_encoding = nn.Parameter(self._position_encoding(), requires_grad=False)\n",
    "        self.embed_dropout = nn.Dropout(p=dropout)\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, bptt_len, num_blocks_encoder, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, bptt_len, num_blocks_decoder, dropout)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, len(self.vocab))\n",
    "        #self.linear.weight = self.embedding.weight  # Section 3.4\n",
    "        self.linear_softmax = nn.Softmax(dim=-1)\n",
    "        #self.linear_dropout = nn.Dropout(p=dropout)\n",
    "        #self.linear_norm = nn.LayerNorm(len(vocab))\n",
    "           \n",
    "    def _position_encoding(self):\n",
    "        d_model = self.d_model\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(self.bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        assert stack.shape == torch.Size([self.d_model, self.bptt_len])\n",
    "        \n",
    "        return stack.T\n",
    "    \n",
    "    def embed(self, indices):\n",
    "        embedded = self.embedding(tensor(indices))\n",
    "        #assert embedded.shape == torch.Size([self.bptt_len, self.d_model])\n",
    "        return embedded + self.position_encoding\n",
    "        \n",
    "    #def forward(self, encoder_in=None, encoder_out=None, decoder_in=[]):\n",
    "    def forward(self, encoder_out, decoder_in):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        encoder_out: (optional rank-1 tensor) passing this skips \n",
    "                     the encoder execution, and acts and if this were \n",
    "                     the indices the encoder produced.\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "        \n",
    "        # Embed\n",
    "        #embedded = self.embed_dropout(self.embed(encoder_in))\n",
    "            \n",
    "        # Encode\n",
    "        #encoder_out = self.embed_dropout(self.embed(embedded))\n",
    "        \n",
    "        # Decode\n",
    "        encoder_out = self.embed(encoder_out)\n",
    "        #print('encoder_out = ', encoder_out)\n",
    "        decoder_in = self.embed(decoder_in)\n",
    "        #print('decoder_in = ', decoder_in)\n",
    "        decoder_out = self.decoder(encoder_out, decoder_in)\n",
    "        #print('decoder_out = ', decoder_out)\n",
    "        #print('decoded:', decoded)\n",
    "\n",
    "        # Return predictions for next token\n",
    "        y_pred = self.linear_softmax(self.linear(decoder_out))\n",
    "        return y_pred\n",
    "        #return self.linear_norm(self.linear_dropout(y_pred))\n",
    "        #y_pred = torch.matmul(decoded, self.embedding.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Vocab and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53488lines [00:00, 534872.86lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s):\n",
      "    cuda:3: GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "484586lines [00:00, 586833.57lines/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "# Make the vocabulary\n",
    "with open(conf['datafile'], 'r') as f:\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(f.read().replace('\\n','').lower())\n",
    "    vocab.stoi['<pad>'] = 0\n",
    "    vocab.stoi['<eos>'] = 1\n",
    "    vocab.itos[0] = '<pad>'\n",
    "    vocab.stoi[1] = '<eos>'\n",
    "with open(conf['datafile'], 'r') as f:\n",
    "    vocab.freqs['<eos>'] = len(f.readlines())\n",
    "\n",
    "# define the model\n",
    "model = Transformer(vocab)\n",
    "model = model.half().to(device)\n",
    "model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "optimizer = getattr(torch.optim, conf['optimizer'])(model.parameters(), lr=conf['learning_rate'])\n",
    "\n",
    "CE_freqs = [float(vocab.freqs[t]) for t in vocab.itos]\n",
    "CE_weight = [(0. if f == 0 else 1/f) for f in CE_freqs]\n",
    "CE_weight = torch.tensor(CE_weight, dtype=torch.half, device=device)\n",
    "##criterion = nn.CrossEntropyLoss(weight=CE_weight, ignore_index=vocab.stoi['<pad>'])\n",
    "criterion = nn.CrossEntropyLoss(weight=CE_weight)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_indices(indices, right_shift=False, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Takes a list of token `indices`, appends the index for the\n",
    "    <eos> token and pads the list on the right to the bptt_len length.\n",
    "    If you pass `right_shift=True`, then it also inserts the index for \n",
    "    the pad token to the beginning of the list and shifts everything \n",
    "    else to the right (still maintaining bptt_len.\"\"\"\n",
    "    indices = list(map(int, indices))\n",
    "    eos_index = vocab.stoi['<eos>']\n",
    "    pad_index = vocab.stoi['<pad>']\n",
    "    if (not indices) or (indices[-1] != eos_index):\n",
    "        indices.append(eos_index)\n",
    "    if right_shift:\n",
    "        indices.insert(0, pad_index)\n",
    "    indices = indices[:bptt_len]\n",
    "    pad_len = bptt_len - len(indices)\n",
    "    indices += [pad_index] * pad_len\n",
    "    return indices\n",
    "    \n",
    "def get_indices(string, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"takes a string, tokenizes it, and returns the (unpadded) \n",
    "    list of token incides for the tokens in the string. The output\n",
    "    of this method is suitable input for `pad_indices()`\"\"\"\n",
    "    tokens = list(string.strip().lower())\n",
    "    tokens = tokens[:bptt_len]\n",
    "    indices = list(map(lambda x: vocab.stoi[x], tokens))\n",
    "    return indices\n",
    "\n",
    "def _get_tensors(string, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"Takes a string returns the appropriately padded and shifted \n",
    "    `encoder_out`, `decoder_in`, and `y` tensors for it\"\"\"\n",
    "    indices = get_indices(string, vocab)\n",
    "    encoder_out = []\n",
    "    decoder_in = []\n",
    "    y = []\n",
    "    for i in range(len(indices)):\n",
    "        encoder_out.append(tensor(pad_indices(indices[:i])).unsqueeze(0))\n",
    "        decoder_in.append(tensor(pad_indices(indices[i:], right_shift=True)).unsqueeze(0))\n",
    "        y.append(tensor(pad_indices(indices[i:])).unsqueeze(0))\n",
    "    return encoder_out, decoder_in, y\n",
    "\n",
    "def get_data(minibatch_size=conf['minibatch_size'], vocab=vocab, data_file=conf['datafile']):\n",
    "    \"\"\"Reads the \\n separated training strings from the data file,\n",
    "    and returns file and returns a generator of `encoder_out`, \n",
    "    `encoder_in`, and `y` tensors. Each has a shape:\n",
    "    (minibatch_size, bptt_len, d_model)\"\"\"\n",
    "    global data\n",
    "    if not data:\n",
    "        with open(data_file,'r') as f:\n",
    "            strings = [line.strip().lower() for line in f.readlines()]\n",
    "        for string in strings:\n",
    "            encoder_out, decoder_in, y = _get_tensors(string)\n",
    "            data['encoder_out'].extend(encoder_out)\n",
    "            data['decoder_in'].extend(decoder_in)\n",
    "            data['y'].extend(y)\n",
    "        data['encoder_out'] = torch.cat(data['encoder_out']).to(device)\n",
    "        data['decoder_in'] = torch.cat(data['decoder_in']).to(device)\n",
    "        data['y'] = torch.cat(data['y']).to(device)\n",
    "\n",
    "    eo_split = torch.split(data['encoder_out'], minibatch_size, dim=0)\n",
    "    di_split = torch.split(data['decoder_in'], minibatch_size, dim=0)\n",
    "    y_split = torch.split(data['y'], minibatch_size, dim=0)\n",
    "    for encoder_out, decoder_in, y_split in zip(eo_split, di_split, y_split):\n",
    "        yield encoder_out, decoder_in, y_split \n",
    "    #batch_size = data['y'].shape[0]\n",
    "    #i = 0\n",
    "    #while i < batch_size:\n",
    "    #    j = i + minibatch_size\n",
    "    #    encoder_out = data['encoder_out'][i:j,:]\n",
    "    #    decoder_in = data['decoder_in'][i:j,:]\n",
    "    #    y = data['y'][i:j,:]\n",
    "    #    yield encoder_out, decoder_in, y\n",
    "    #    i = j\n",
    "        \n",
    "def train_data(encoder_out, decoder_in, y):\n",
    "    \"\"\"Runs one minibatch training and returns the loss for that minibatch\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    y_pred = torch.transpose(y_pred, -2, -1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #sys.stdout.write('.')\n",
    "    return loss.item()\n",
    "\n",
    "def do_epoch(epoch, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer):\n",
    "    \"\"\"Runs one full training batch and returns the average loss and \n",
    "    duration time in seconds\"\"\"\n",
    "    t0 = time.time()\n",
    "    losses = [train_data(*args) for args in get_data()]\n",
    "    tf = time.time()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return (avg_loss, tf-t0)\n",
    "\n",
    "def train(num_epochs=conf['epochs_per_loop'], start_epoch=0, model=model, vocab=vocab, criterion=criterion, optimizer=optimizer):\n",
    "    \"\"\"Runs num_epochs training batchs and prints out results\"\"\"\n",
    "    model = model.train()\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        loss, seconds = do_epoch(epoch, model, vocab, criterion, optimizer)\n",
    "        #print()\n",
    "        wandb.log({'epoch': epoch,\n",
    "                   'loss': loss,\n",
    "                   'seconds': seconds})\n",
    "        print(epoch, '(%.1fs)' % seconds, 'loss=%.4f' % loss)    \n",
    "    return epoch + 1\n",
    "\n",
    "\n",
    "induction_weight = CE_weight.unsqueeze(dim=0)\n",
    "\n",
    "def get_next_token(encoder_out, decoder_in, pos, model=model, vocab=vocab):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`. Uses a Multinomial distribution for sampling.\"\"\"\n",
    "    eval_model = model.eval()\n",
    "    encoder_out = tensor(pad_indices(get_indices(encoder_out))).to(device)\n",
    "    decoder_in = tensor(pad_indices(get_indices(decoder_in), right_shift=True)).to(device)\n",
    "    decoder_out = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "    #decoder_out = decoder_out * induction_weight\n",
    "    probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "    m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "    _, indices = torch.max(m.sample(), dim=-1)\n",
    "    #_, indices = torch.max(nn.functional.softmax(decoder_out, dim=1), dim=1)\n",
    "    index = int(indices[pos])\n",
    "    #print('index:', index, 'pos:', pos)\n",
    "    return vocab.itos[index]\n",
    "\n",
    "def sample_with_prompt(prompt, model=model, vocab=vocab, bptt_len=conf['bptt_len']):\n",
    "    \"\"\"samples a string the network beginning with prompt\"\"\"\n",
    "    encoder_out = prompt\n",
    "    decoder_out = ''\n",
    "    next_token = ''\n",
    "    \n",
    "    while next_token not in ('<pad>', '<eos>') and len(decoder_out) < (bptt_len - 1):\n",
    "        #print('prompt:', prompt, 'decoder_out:', decoder_out, 'next_index:', next_index, 'next_token:', next_token)\n",
    "        decoder_out += next_token\n",
    "        next_token = get_next_token(encoder_out, decoder_out, pos=len(decoder_out))\n",
    "        \n",
    "    return prompt + decoder_out\n",
    "\n",
    "def alphabet_sample():\n",
    "    \"\"\"samples strings beginning with each letter of the alphabet\"\"\"\n",
    "    for p in [chr(i) for i in range(ord('a'), ord('z')+1)]: \n",
    "        print('%r' % sample_with_prompt(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apower/anaconda3/envs/sandbox1/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'afmsiizbknlsauvcgqhkpdgui'\n",
      "'bvqchvazejeokxvspjxsneyau'\n",
      "'cbrxljuryazqhmkdsb'\n",
      "'didbm'\n",
      "'egxozn-aqomgrrzthtctjeiby'\n",
      "'fifwgennnyax'\n",
      "'gzg'\n",
      "'hbixgrztiythaaoj'\n",
      "'iyvg'\n",
      "'jyh'\n",
      "'kufzaftkwbmhwawzrliukohap'\n",
      "'lckcmwvjrhwkdbmzjscyf-'\n",
      "'mgjtnhedju'\n",
      "'nztytgabhxltx'\n",
      "'oa-hwixh-riwm-vxqccj'\n",
      "'ppfqxkiujdghaubxaoluvsapd'\n",
      "'qgbqcpbmcbyo'\n",
      "'rjdq-o-mt'\n",
      "'sr'\n",
      "'txpqawgy'\n",
      "'ufr'\n",
      "'veshdhmarbgvxexfpvwtahhna'\n",
      "'w'\n",
      "'xwnyxb'\n",
      "'yglafautthrjonk'\n",
      "'zrdusvocampcuu-zwohzejuww'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.32 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (25.6s) loss=3.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apower/anaconda3/envs/sandbox1/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (7.2s) loss=3.4617\n",
      "2 (7.3s) loss=3.4605\n",
      "3 (7.3s) loss=3.4598\n",
      "4 (7.3s) loss=3.4604\n",
      "5 (7.4s) loss=3.4598\n",
      "6 (7.4s) loss=3.4588\n",
      "7 (7.4s) loss=3.4586\n",
      "8 (7.4s) loss=3.4594\n",
      "9 (7.4s) loss=3.4572\n",
      "10 (7.4s) loss=3.4557\n",
      "11 (7.4s) loss=3.4557\n",
      "12 (7.4s) loss=3.4537\n",
      "13 (7.4s) loss=3.4529\n",
      "14 (7.5s) loss=3.4523\n",
      "15 (7.5s) loss=3.4488\n",
      "16 (7.5s) loss=3.4457\n",
      "17 (7.5s) loss=3.4412\n",
      "18 (7.5s) loss=3.4381\n",
      "19 (7.5s) loss=3.4318\n",
      "20 (7.5s) loss=3.4256\n",
      "21 (7.5s) loss=3.4199\n",
      "22 (7.5s) loss=3.4160\n",
      "23 (7.5s) loss=3.4062\n",
      "24 (7.5s) loss=3.3969\n",
      "25 (7.5s) loss=3.3896\n",
      "26 (7.5s) loss=3.3855\n",
      "27 (7.5s) loss=3.3814\n",
      "28 (7.5s) loss=3.3787\n",
      "29 (7.5s) loss=3.3770\n",
      "30 (7.5s) loss=3.3742\n",
      "31 (7.5s) loss=3.3750\n",
      "32 (7.5s) loss=3.3713\n",
      "33 (7.5s) loss=3.3725\n",
      "34 (7.5s) loss=3.3709\n",
      "35 (7.5s) loss=3.3701\n",
      "36 (7.5s) loss=3.3693\n",
      "37 (7.5s) loss=3.3684\n",
      "38 (7.5s) loss=3.3684\n",
      "39 (7.6s) loss=3.3691\n",
      "40 (7.5s) loss=3.3680\n",
      "41 (7.5s) loss=3.3678\n",
      "42 (7.5s) loss=3.3678\n",
      "43 (7.5s) loss=3.3666\n",
      "44 (7.5s) loss=3.3658\n",
      "45 (7.5s) loss=3.3658\n",
      "46 (7.5s) loss=3.3660\n",
      "47 (7.5s) loss=3.3662\n",
      "48 (7.5s) loss=3.3652\n",
      "49 (7.5s) loss=3.3658\n",
      "'axsjgpl'\n",
      "'bjmqf'\n",
      "'c'\n",
      "'dlwus-ovdip'\n",
      "'ebyaa'\n",
      "'fmwuzjcauvvwxnvajqmwqn'\n",
      "'gzwajfqsywhvhghlyf'\n",
      "'hlz'\n",
      "'iowtetykob'\n",
      "'j'\n",
      "'kvffyejwlzs-qyepjduwglez'\n",
      "'ljgmedjemfcu'\n",
      "'mrmamiwbphn'\n",
      "'ntqly'\n",
      "'omtfcltzlkmcjkj-zferhekej'\n",
      "'py-f'\n",
      "'qrmpikfosspevfmpvoww'\n",
      "'rzim'\n",
      "'suyz'\n",
      "'ts'\n",
      "'un'\n",
      "'vbboc-yuefhm'\n",
      "'wbg-onnnke-x-kdzudsieeuyk'\n",
      "'xipqmyvmqqwogqyzy'\n",
      "'y'\n",
      "'zyhdugaohwitq-tjngblazml'\n",
      "50 (7.5s) loss=3.3648\n",
      "51 (7.5s) loss=3.3664\n",
      "52 (7.5s) loss=3.3641\n",
      "53 (7.5s) loss=3.3664\n",
      "54 (7.5s) loss=3.3635\n",
      "55 (7.5s) loss=3.3617\n",
      "56 (7.5s) loss=3.3656\n",
      "57 (7.5s) loss=3.3639\n",
      "58 (7.5s) loss=3.3623\n",
      "59 (7.5s) loss=3.3623\n",
      "60 (7.5s) loss=3.3643\n",
      "61 (7.5s) loss=3.3625\n",
      "62 (7.5s) loss=3.3627\n",
      "63 (7.5s) loss=3.3611\n",
      "64 (7.5s) loss=3.3607\n",
      "65 (7.5s) loss=3.3605\n",
      "66 (7.5s) loss=3.3605\n",
      "67 (7.5s) loss=3.3590\n",
      "68 (7.5s) loss=3.3602\n",
      "69 (7.6s) loss=3.3602\n",
      "70 (7.5s) loss=3.3602\n",
      "71 (7.6s) loss=3.3594\n",
      "72 (7.5s) loss=3.3584\n",
      "73 (7.5s) loss=3.3576\n",
      "74 (7.5s) loss=3.3592\n",
      "75 (7.5s) loss=3.3572\n",
      "76 (7.5s) loss=3.3563\n",
      "77 (7.5s) loss=3.3564\n",
      "78 (7.5s) loss=3.3568\n",
      "79 (7.5s) loss=3.3566\n",
      "80 (7.5s) loss=3.3574\n",
      "81 (7.5s) loss=3.3557\n",
      "82 (7.5s) loss=3.3551\n",
      "83 (7.5s) loss=3.3559\n",
      "84 (7.5s) loss=3.3535\n",
      "85 (7.5s) loss=3.3557\n",
      "86 (7.5s) loss=3.3533\n",
      "87 (7.5s) loss=3.3535\n",
      "88 (7.5s) loss=3.3537\n",
      "89 (7.5s) loss=3.3533\n",
      "90 (7.5s) loss=3.3529\n",
      "91 (7.5s) loss=3.3529\n",
      "92 (7.5s) loss=3.3500\n",
      "93 (7.5s) loss=3.3496\n",
      "94 (7.5s) loss=3.3520\n",
      "95 (7.5s) loss=3.3504\n",
      "96 (7.5s) loss=3.3527\n",
      "97 (7.5s) loss=3.3496\n",
      "98 (7.5s) loss=3.3479\n",
      "99 (7.5s) loss=3.3479\n",
      "'asxqj'\n",
      "'bq'\n",
      "'cdvpuzswdykwmsminldsn'\n",
      "'dxvv'\n",
      "'eybmzezxqvvqwph'\n",
      "'fonfagv'\n",
      "'gmktvcwwig'\n",
      "'hausmwlfdydimyiqykayptyyp'\n",
      "'iferp-yb-bqcuzmgfv'\n",
      "'jefrnz'\n",
      "'k'\n",
      "'lymiam'\n",
      "'mnghqa'\n",
      "'nbfmednqgdkwznrwsmyatjrmo'\n",
      "'o--ggfyq'\n",
      "'pmjcjeblqztjjqryfks-yl-zb'\n",
      "'qzoe'\n",
      "'rs-kmactbntyfudsna-ghmya'\n",
      "'stkazmt-zwtzquizuw'\n",
      "'td'\n",
      "'uxfcxamhweuutzgqingkvehb'\n",
      "'vbun-ndneqorhczedwiongooc'\n",
      "'w'\n",
      "'xa-oqcucolytyykezf-pkpijp'\n",
      "'ytkdanewf'\n",
      "'z'\n",
      "100 (7.5s) loss=3.3477\n",
      "101 (7.5s) loss=3.3496\n",
      "102 (7.5s) loss=3.3508\n",
      "103 (7.5s) loss=3.3484\n",
      "104 (7.5s) loss=3.3475\n",
      "105 (7.5s) loss=3.3480\n",
      "106 (7.5s) loss=3.3469\n",
      "107 (7.5s) loss=3.3482\n",
      "108 (7.5s) loss=3.3469\n",
      "109 (7.5s) loss=3.3469\n",
      "110 (7.5s) loss=3.3479\n",
      "111 (7.5s) loss=3.3467\n",
      "112 (7.5s) loss=3.3463\n",
      "113 (7.5s) loss=3.3463\n",
      "114 (7.6s) loss=3.3457\n",
      "115 (7.6s) loss=3.3453\n",
      "116 (7.5s) loss=3.3441\n",
      "117 (7.5s) loss=3.3434\n",
      "118 (7.5s) loss=3.3439\n",
      "119 (7.6s) loss=3.3451\n",
      "120 (7.5s) loss=3.3410\n",
      "121 (7.6s) loss=3.3414\n",
      "122 (7.5s) loss=3.3418\n",
      "123 (7.5s) loss=3.3391\n",
      "124 (7.6s) loss=3.3398\n",
      "125 (7.5s) loss=3.3395\n",
      "126 (7.5s) loss=3.3363\n",
      "127 (7.5s) loss=3.3373\n",
      "128 (7.5s) loss=3.3354\n",
      "129 (7.5s) loss=3.3367\n",
      "130 (7.5s) loss=3.3354\n",
      "131 (7.6s) loss=3.3369\n",
      "132 (7.5s) loss=3.3355\n",
      "133 (7.5s) loss=3.3322\n",
      "134 (7.5s) loss=3.3330\n",
      "135 (7.5s) loss=3.3350\n",
      "136 (7.5s) loss=3.3334\n",
      "137 (7.5s) loss=3.3324\n",
      "138 (7.5s) loss=3.3309\n",
      "139 (7.6s) loss=3.3332\n",
      "140 (7.5s) loss=3.3324\n",
      "141 (7.5s) loss=3.3326\n",
      "142 (7.6s) loss=3.3324\n",
      "143 (7.6s) loss=3.3309\n",
      "144 (7.6s) loss=3.3330\n",
      "145 (7.6s) loss=3.3324\n",
      "146 (7.5s) loss=3.3324\n",
      "147 (7.5s) loss=3.3312\n",
      "148 (7.5s) loss=3.3318\n",
      "149 (7.5s) loss=3.3305\n",
      "'apjywcwtixgx'\n",
      "'bsmqql'\n",
      "'crrw'\n",
      "'dvajziadcwi-rkxpimugxfsqv'\n",
      "'e'\n",
      "'frxnzsvfmfnp'\n",
      "'gbfpmw'\n",
      "'htybg'\n",
      "'ifkyh'\n",
      "'jtqd-qnvoonwgp-clgqswlmsp'\n",
      "'kjk'\n",
      "'lhyxlevpvgvigorfzetqpwbke'\n",
      "'mrn-nqvfyogsygvmogifjk'\n",
      "'npvlyvs-sw'\n",
      "'olinoha'\n",
      "'pzjpyctdjemjoqy'\n",
      "'qvaxwusi-gyb'\n",
      "'rlcpaqllralhptitrydnlnodp'\n",
      "'sf'\n",
      "'tsg-jaqkulmzxblqougysmhyk'\n",
      "'udgawjt'\n",
      "'v'\n",
      "'wixybfsbs-kl'\n",
      "'xjdrnaovntymgon-pgjedjj'\n",
      "'ydfpxwjuxra'\n",
      "'zjx-lyotgqa-vlxjk'\n",
      "150 (7.5s) loss=3.3307\n",
      "151 (7.5s) loss=3.3299\n",
      "152 (7.5s) loss=3.3301\n",
      "153 (7.5s) loss=3.3311\n",
      "154 (7.6s) loss=3.3318\n",
      "155 (7.5s) loss=3.3289\n",
      "156 (7.5s) loss=3.3303\n",
      "157 (7.6s) loss=3.3299\n",
      "158 (7.6s) loss=3.3289\n",
      "159 (7.5s) loss=3.3316\n",
      "160 (7.6s) loss=3.3305\n",
      "161 (7.5s) loss=3.3297\n",
      "162 (7.5s) loss=3.3299\n",
      "163 (7.5s) loss=3.3305\n",
      "164 (7.5s) loss=3.3285\n",
      "165 (7.6s) loss=3.3293\n",
      "166 (7.5s) loss=3.3285\n",
      "167 (7.6s) loss=3.3293\n",
      "168 (7.6s) loss=3.3277\n",
      "169 (7.6s) loss=3.3279\n",
      "170 (7.6s) loss=3.3281\n",
      "171 (7.6s) loss=3.3256\n",
      "172 (7.6s) loss=3.3215\n",
      "173 (7.6s) loss=3.3197\n",
      "174 (7.6s) loss=3.3199\n",
      "175 (7.6s) loss=3.3184\n",
      "176 (7.6s) loss=3.3197\n",
      "177 (7.6s) loss=3.3145\n",
      "178 (7.5s) loss=3.3150\n",
      "179 (7.6s) loss=3.3160\n",
      "180 (7.6s) loss=3.3137\n",
      "181 (7.6s) loss=3.3146\n",
      "182 (7.6s) loss=3.3141\n",
      "183 (7.6s) loss=3.3123\n",
      "184 (7.6s) loss=3.3131\n",
      "185 (7.6s) loss=3.3129\n",
      "186 (7.6s) loss=3.3115\n",
      "187 (7.6s) loss=3.3111\n",
      "188 (7.6s) loss=3.3131\n",
      "189 (7.6s) loss=3.3121\n",
      "190 (7.6s) loss=3.3113\n",
      "191 (7.6s) loss=3.3096\n",
      "192 (7.6s) loss=3.3113\n",
      "193 (7.6s) loss=3.3100\n",
      "194 (7.6s) loss=3.3102\n",
      "195 (7.6s) loss=3.3109\n",
      "196 (7.6s) loss=3.3111\n",
      "197 (7.6s) loss=3.3111\n",
      "198 (7.6s) loss=3.3123\n",
      "199 (7.6s) loss=3.3127\n",
      "'atfsdxnuh-vslrtuowugdj-hp'\n",
      "'bm'\n",
      "'clvm-kddktq'\n",
      "'dgs--temruor-ktxobkixqonk'\n",
      "'epegsf-xguluzupvvwbn-xxb'\n",
      "'fakrabdkoil'\n",
      "'gbgsiqkswcmnd'\n",
      "'he'\n",
      "'ik-r-'\n",
      "'jsqevzavabipb'\n",
      "'krwvzi'\n",
      "'lndowsgmrlwmet'\n",
      "'myfiibnmujayogcljjwvtqria'\n",
      "'nfawpockkjewux-wbyrxltmec'\n",
      "'oree'\n",
      "'pusryqrdevsdxl-ongfwg'\n",
      "'qbikmjhntxpfrq'\n",
      "'rfzjmsyqjmhkbf'\n",
      "'scinnof'\n",
      "'tegyoaajyheveih-gjyv'\n",
      "'uzfk'\n",
      "'vfdz-'\n",
      "'wufvdogy'\n",
      "'x'\n",
      "'y-yetx'\n",
      "'ztihoizoe--rzdafzrgatwykh'\n",
      "200 (7.5s) loss=3.3123\n",
      "201 (7.5s) loss=3.3104\n",
      "202 (7.5s) loss=3.3139\n",
      "203 (7.6s) loss=3.3107\n",
      "204 (7.6s) loss=3.3090\n",
      "205 (7.6s) loss=3.3076\n",
      "206 (7.6s) loss=3.3076\n",
      "207 (7.6s) loss=3.3084\n",
      "208 (7.6s) loss=3.3053\n",
      "209 (7.6s) loss=3.3035\n",
      "210 (7.6s) loss=3.3031\n",
      "211 (7.6s) loss=3.3016\n",
      "212 (7.6s) loss=3.3047\n",
      "213 (7.6s) loss=3.3018\n",
      "214 (7.6s) loss=3.3008\n",
      "215 (7.6s) loss=3.2996\n",
      "216 (7.6s) loss=3.2990\n",
      "217 (7.6s) loss=3.2994\n",
      "218 (7.6s) loss=3.2994\n",
      "219 (7.6s) loss=3.2963\n",
      "220 (7.6s) loss=3.2947\n",
      "221 (7.6s) loss=3.2957\n",
      "222 (7.6s) loss=3.2973\n",
      "223 (7.6s) loss=3.2932\n",
      "224 (7.6s) loss=3.2932\n",
      "225 (7.6s) loss=3.2920\n",
      "226 (7.6s) loss=3.2902\n",
      "227 (7.6s) loss=3.2906\n",
      "228 (7.6s) loss=3.2918\n",
      "229 (7.6s) loss=3.2930\n",
      "230 (7.6s) loss=3.2891\n",
      "231 (7.6s) loss=3.2928\n",
      "232 (7.6s) loss=3.2896\n",
      "233 (7.6s) loss=3.2891\n",
      "234 (7.6s) loss=3.2893\n",
      "235 (7.6s) loss=3.2908\n",
      "236 (7.6s) loss=3.2910\n",
      "237 (7.6s) loss=3.2889\n",
      "238 (7.6s) loss=3.2900\n",
      "239 (7.6s) loss=3.2908\n",
      "240 (7.6s) loss=3.2904\n",
      "241 (7.6s) loss=3.2893\n",
      "242 (7.6s) loss=3.2891\n",
      "243 (7.6s) loss=3.2895\n",
      "244 (7.6s) loss=3.2889\n",
      "245 (7.6s) loss=3.2881\n",
      "246 (7.6s) loss=3.2873\n",
      "247 (7.6s) loss=3.2889\n",
      "248 (7.6s) loss=3.2869\n",
      "249 (7.6s) loss=3.2895\n",
      "'acxwhyjulmyv'\n",
      "'b'\n",
      "'cryvaabbidsq'\n",
      "'dqxhyz'\n",
      "'eqwublgy'\n",
      "'fdsbhkzngvkss'\n",
      "'gjbvmfyllhstufbejyulwswqa'\n",
      "'hasxxjgipag'\n",
      "'iuarqmpylygflspxm-'\n",
      "'j'\n",
      "'kasvkwncmtocllyt'\n",
      "'lsmhcepynb'\n",
      "'moxa'\n",
      "'n-clkdtjxsgwl'\n",
      "'o'\n",
      "'pewck-wv'\n",
      "'qjrjoebzppznymk-cv-pwugqd'\n",
      "'rvgvpglblstlsimugs-jciciu'\n",
      "'sogefxl-yuvsbjfh'\n",
      "'t-ngqrjpyfp'\n",
      "'uim-hxywg'\n",
      "'vgm-zvinuskw-pyxtfhywbl'\n",
      "'w'\n",
      "'xyfupuhabww'\n",
      "'yegydypjo-yij-tpsvdlngygf'\n",
      "'zxpyucqs'\n",
      "250 (7.6s) loss=3.2881\n",
      "251 (7.5s) loss=3.2869\n",
      "252 (7.5s) loss=3.2867\n",
      "253 (7.6s) loss=3.2881\n",
      "254 (7.5s) loss=3.2877\n",
      "255 (7.6s) loss=3.2844\n",
      "256 (7.6s) loss=3.2828\n",
      "257 (7.6s) loss=3.2859\n",
      "258 (7.6s) loss=3.2873\n",
      "259 (7.6s) loss=3.2830\n",
      "260 (7.6s) loss=3.2820\n",
      "261 (7.6s) loss=3.2846\n",
      "262 (7.6s) loss=3.2818\n",
      "263 (7.6s) loss=3.2848\n",
      "264 (7.6s) loss=3.2801\n",
      "265 (7.6s) loss=3.2805\n",
      "266 (7.6s) loss=3.2809\n",
      "267 (7.6s) loss=3.2777\n",
      "268 (7.6s) loss=3.2768\n",
      "269 (7.6s) loss=3.2807\n",
      "270 (7.6s) loss=3.2785\n",
      "271 (7.6s) loss=3.2791\n",
      "272 (7.6s) loss=3.2764\n",
      "273 (7.6s) loss=3.2787\n",
      "274 (7.6s) loss=3.2771\n",
      "275 (7.6s) loss=3.2709\n",
      "276 (7.6s) loss=3.2781\n",
      "277 (7.6s) loss=3.2768\n",
      "278 (7.6s) loss=3.2764\n",
      "279 (7.6s) loss=3.2689\n",
      "280 (7.6s) loss=3.2705\n",
      "281 (7.6s) loss=3.2758\n",
      "282 (7.6s) loss=3.2717\n",
      "283 (7.6s) loss=3.2715\n",
      "284 (7.6s) loss=3.2725\n",
      "285 (7.6s) loss=3.2715\n",
      "286 (7.6s) loss=3.2654\n",
      "287 (7.6s) loss=3.2709\n",
      "288 (7.6s) loss=3.2727\n",
      "289 (7.6s) loss=3.2736\n",
      "290 (7.6s) loss=3.2686\n",
      "291 (7.6s) loss=3.2701\n",
      "292 (7.6s) loss=3.2709\n",
      "293 (7.6s) loss=3.2684\n",
      "294 (7.6s) loss=3.2691\n",
      "295 (7.6s) loss=3.2686\n",
      "296 (7.6s) loss=3.2639\n",
      "297 (7.6s) loss=3.2625\n",
      "298 (7.6s) loss=3.2613\n",
      "299 (7.6s) loss=3.2650\n",
      "'agkdyuaukx---yfv'\n",
      "'bqxkevtag-ziinertxlk-eeeb'\n",
      "'ctzci'\n",
      "'dob-lz'\n",
      "'enfzbryyowalvdpauiyggscc-'\n",
      "'fv'\n",
      "'goaoblpky-l'\n",
      "'h'\n",
      "'ipy-bi'\n",
      "'jvcyxpss'\n",
      "'ksctfytcmuulpgb'\n",
      "'laccfevvab'\n",
      "'mbv-wry'\n",
      "'nv-e-xetcpoq-t'\n",
      "'oqrsfkyudlwutfpxfewzp'\n",
      "'psybxajyldmfiprpdvmfg'\n",
      "'qplcqstpl'\n",
      "'ragiry'\n",
      "'sssvlybwwmmhyeptsgjdgu-yp'\n",
      "'tdbwh-s-op-earmysfnj-xhzw'\n",
      "'uuul'\n",
      "'v'\n",
      "'wcyyyvehyfyfsocnfn'\n",
      "'x-ubcaynbgghwjdawgwgwnbxv'\n",
      "'ygj'\n",
      "'zyauivnnjvfjyyhft'\n",
      "300 (7.5s) loss=3.2533\n",
      "301 (7.5s) loss=3.2566\n",
      "302 (7.5s) loss=3.2660\n",
      "303 (7.5s) loss=3.2621\n",
      "304 (7.6s) loss=3.2578\n",
      "305 (7.6s) loss=3.2629\n",
      "306 (7.6s) loss=3.2543\n",
      "307 (7.6s) loss=3.2594\n",
      "308 (7.6s) loss=3.2551\n",
      "309 (7.6s) loss=3.2598\n",
      "310 (7.6s) loss=3.2596\n",
      "311 (7.6s) loss=3.2531\n",
      "312 (7.6s) loss=3.2512\n",
      "313 (7.6s) loss=3.2508\n",
      "314 (7.6s) loss=3.2518\n",
      "315 (7.6s) loss=3.2510\n",
      "316 (7.6s) loss=3.2510\n",
      "317 (7.6s) loss=3.2574\n",
      "318 (7.6s) loss=3.2488\n",
      "319 (7.6s) loss=3.2459\n",
      "320 (7.6s) loss=3.2471\n",
      "321 (7.6s) loss=3.2484\n",
      "322 (7.6s) loss=3.2457\n",
      "323 (7.6s) loss=3.2477\n",
      "324 (7.6s) loss=3.2504\n",
      "325 (7.6s) loss=3.2465\n",
      "326 (7.6s) loss=3.2498\n",
      "327 (7.6s) loss=3.2488\n",
      "328 (7.6s) loss=3.2437\n",
      "329 (7.6s) loss=3.2453\n",
      "330 (7.6s) loss=3.2453\n",
      "331 (7.6s) loss=3.2463\n",
      "332 (7.6s) loss=3.2420\n",
      "333 (7.6s) loss=3.2449\n",
      "334 (7.6s) loss=3.2437\n",
      "335 (7.6s) loss=3.2447\n",
      "336 (7.6s) loss=3.2389\n",
      "337 (7.6s) loss=3.2381\n",
      "338 (7.6s) loss=3.2404\n",
      "339 (7.6s) loss=3.2428\n",
      "340 (7.6s) loss=3.2373\n",
      "341 (7.6s) loss=3.2357\n",
      "342 (7.6s) loss=3.2385\n",
      "343 (7.6s) loss=3.2367\n",
      "344 (7.6s) loss=3.2342\n",
      "345 (7.6s) loss=3.2334\n",
      "346 (7.6s) loss=3.2350\n",
      "347 (7.6s) loss=3.2365\n",
      "348 (7.6s) loss=3.2375\n",
      "349 (7.6s) loss=3.2375\n",
      "'aqghi'\n",
      "'bgjxyryczbffkdsdzqwbh'\n",
      "'clgixyecwqfdgdhknjcktuthk'\n",
      "'dmxjaftscmk-ojhpmts-abrxt'\n",
      "'ecydsxjyytvhet'\n",
      "'flkvtx'\n",
      "'gio'\n",
      "'hqvflx'\n",
      "'ivng'\n",
      "'jzzkbxfqj-fvwigu'\n",
      "'kylvatltjdgaki'\n",
      "'lyqmvitthdgkd'\n",
      "'mxstvvzrtcjs'\n",
      "'n'\n",
      "'owhohlnosvml-aamizheufirw'\n",
      "'pzyq'\n",
      "'qh'\n",
      "'rzytnljgh'\n",
      "'swp'\n",
      "'tyyixxopkxyslhhopfiygbpep'\n",
      "'utuhihso'\n",
      "'v'\n",
      "'w-'\n",
      "'xgvlyypulujyodoonfgcgmfov'\n",
      "'yywazebdgyuamgzlpcspi'\n",
      "'zuthzpszmrxv'\n",
      "350 (7.5s) loss=3.2379\n",
      "351 (7.5s) loss=3.2375\n",
      "352 (7.6s) loss=3.2350\n",
      "353 (7.6s) loss=3.2344\n",
      "354 (7.6s) loss=3.2348\n",
      "355 (7.6s) loss=3.2350\n",
      "356 (7.6s) loss=3.2361\n",
      "357 (7.6s) loss=3.2363\n",
      "358 (7.6s) loss=3.2367\n",
      "359 (7.6s) loss=3.2332\n",
      "360 (7.6s) loss=3.2373\n",
      "361 (7.6s) loss=3.2352\n",
      "362 (7.6s) loss=3.2348\n",
      "363 (7.6s) loss=3.2354\n",
      "364 (7.6s) loss=3.2324\n",
      "365 (7.6s) loss=3.2383\n",
      "366 (7.6s) loss=3.2314\n",
      "367 (7.6s) loss=3.2326\n",
      "368 (7.6s) loss=3.2336\n",
      "369 (7.6s) loss=3.2383\n",
      "370 (7.6s) loss=3.2305\n",
      "371 (7.6s) loss=3.2354\n",
      "372 (7.6s) loss=3.2297\n",
      "373 (7.6s) loss=3.2346\n",
      "374 (7.6s) loss=3.2293\n",
      "375 (7.6s) loss=3.2332\n",
      "376 (7.6s) loss=3.2285\n",
      "377 (7.6s) loss=3.2316\n",
      "378 (7.6s) loss=3.2223\n",
      "379 (7.6s) loss=3.2311\n",
      "380 (7.6s) loss=3.2258\n",
      "381 (7.6s) loss=3.2332\n",
      "382 (7.6s) loss=3.2314\n",
      "383 (7.6s) loss=3.2279\n",
      "384 (7.6s) loss=3.2256\n",
      "385 (7.6s) loss=3.2295\n",
      "386 (7.6s) loss=3.2221\n",
      "387 (7.6s) loss=3.2324\n",
      "388 (7.6s) loss=3.2275\n",
      "389 (7.6s) loss=3.2301\n",
      "390 (7.6s) loss=3.2242\n",
      "391 (7.6s) loss=3.2330\n",
      "392 (7.6s) loss=3.2268\n",
      "393 (7.6s) loss=3.2254\n",
      "394 (7.6s) loss=3.2262\n",
      "395 (7.6s) loss=3.2904\n",
      "396 (7.6s) loss=3.2590\n",
      "397 (7.6s) loss=3.2324\n",
      "398 (7.6s) loss=3.2305\n",
      "399 (7.6s) loss=3.2338\n",
      "'axswldl'\n",
      "'bwedp'\n",
      "'cyjd'\n",
      "'dg'\n",
      "'enrtnp-vhwjs-xu'\n",
      "'fffztera'\n",
      "'gcy'\n",
      "'hcblnuihwwvpustif'\n",
      "'i'\n",
      "'ju'\n",
      "'kuqppemturgp'\n",
      "'latapvrqie'\n",
      "'mnopanoz--itejqazakmdtgg'\n",
      "'nkamoktxxymtg-phtj-kr'\n",
      "'oth'\n",
      "'pucw-p-s-'\n",
      "'qvvhwvzcuhulofcavvo'\n",
      "'r'\n",
      "'sv-gdqu'\n",
      "'t-ukscpvzdwmjivfrgju'\n",
      "'uvgjzngypmeosmfzbkugwhugy'\n",
      "'vvtrfijtcitdyinpufdxmsgou'\n",
      "'whcwpmogjapu-mp-'\n",
      "'xhzc'\n",
      "'yikx'\n",
      "'zmf'\n",
      "400 (7.6s) loss=3.2295\n",
      "401 (7.6s) loss=3.2340\n",
      "402 (7.6s) loss=3.2289\n",
      "403 (7.6s) loss=3.2340\n",
      "404 (7.6s) loss=3.2318\n",
      "405 (7.6s) loss=3.2287\n",
      "406 (7.6s) loss=3.2314\n",
      "407 (7.6s) loss=3.2328\n",
      "408 (7.6s) loss=3.2271\n",
      "409 (7.6s) loss=3.2320\n",
      "410 (7.6s) loss=3.2287\n",
      "411 (7.6s) loss=3.2295\n",
      "412 (7.6s) loss=3.2324\n",
      "413 (7.6s) loss=3.2277\n",
      "414 (7.6s) loss=3.2381\n",
      "415 (7.6s) loss=3.2275\n",
      "416 (7.6s) loss=3.2281\n",
      "417 (7.6s) loss=3.2281\n",
      "418 (7.6s) loss=3.2275\n",
      "419 (7.6s) loss=3.2287\n",
      "420 (7.6s) loss=3.2258\n",
      "421 (7.6s) loss=3.2293\n",
      "422 (7.6s) loss=3.2316\n",
      "423 (7.6s) loss=3.2361\n",
      "424 (7.6s) loss=3.2260\n",
      "425 (7.6s) loss=3.2283\n",
      "426 (7.6s) loss=3.2281\n",
      "427 (7.6s) loss=3.2285\n",
      "428 (7.6s) loss=3.2277\n",
      "429 (7.6s) loss=3.2332\n",
      "430 (7.6s) loss=3.2371\n",
      "431 (7.6s) loss=3.2342\n",
      "432 (7.6s) loss=3.2307\n",
      "433 (7.6s) loss=3.2354\n",
      "434 (7.6s) loss=3.2324\n",
      "435 (7.6s) loss=3.2307\n",
      "436 (7.6s) loss=3.2367\n",
      "437 (7.6s) loss=3.2330\n",
      "438 (7.6s) loss=3.2293\n",
      "439 (7.6s) loss=3.2271\n",
      "440 (7.6s) loss=3.2324\n",
      "441 (7.6s) loss=3.2318\n",
      "442 (7.6s) loss=3.2270\n",
      "443 (7.6s) loss=3.2359\n",
      "444 (7.6s) loss=3.2406\n",
      "445 (7.6s) loss=3.2326\n",
      "446 (7.6s) loss=3.2332\n",
      "447 (7.6s) loss=3.2344\n",
      "448 (7.6s) loss=3.2340\n",
      "449 (7.6s) loss=3.2352\n",
      "'akvlxzpophmgmuhbqjpxo'\n",
      "'bliyrbiy'\n",
      "'ckxfbwzglzdzoymssvmgwdzhf'\n",
      "'dozofckbqphp-evs-jpv-hxwn'\n",
      "'ed'\n",
      "'fkuugso'\n",
      "'gyosbcqeveubahh'\n",
      "'h'\n",
      "'iaflpgixyjocmhwlgrbzl-oex'\n",
      "'jdgkzpnxsmxogq'\n",
      "'kz-firaiupriogirabckfgjon'\n",
      "'lfbcygbhbasdz'\n",
      "'mqlartbm-kfdt'\n",
      "'nymcf'\n",
      "'o'\n",
      "'pyjxmljuplqnhwourd-dycjm'\n",
      "'qhw'\n",
      "'roajlbwifvawolikddkzrhxet'\n",
      "'sjvuuulgvlgbrfjgufioerjnh'\n",
      "'t'\n",
      "'unedpyxzi'\n",
      "'vazd'\n",
      "'wvft-axgxkffkwfxtyliglymy'\n",
      "'xsxbphu'\n",
      "'yxivobzwbjvkoqwaihfulsrl-'\n",
      "'zbvg--dgtj'\n",
      "450 (7.5s) loss=3.2393\n",
      "451 (7.5s) loss=3.2352\n",
      "452 (7.5s) loss=3.2332\n",
      "453 (7.6s) loss=3.2289\n",
      "454 (7.6s) loss=3.2244\n",
      "455 (7.6s) loss=3.2369\n",
      "456 (7.6s) loss=3.2291\n",
      "457 (7.6s) loss=3.2309\n",
      "458 (7.6s) loss=3.2291\n",
      "459 (7.6s) loss=3.2400\n",
      "460 (7.6s) loss=3.2299\n",
      "461 (7.6s) loss=3.2404\n",
      "462 (7.6s) loss=3.2313\n",
      "463 (7.6s) loss=3.2281\n",
      "464 (7.6s) loss=3.2350\n",
      "465 (7.6s) loss=3.2303\n",
      "466 (7.6s) loss=3.2309\n",
      "467 (7.6s) loss=3.2248\n",
      "468 (7.6s) loss=3.2322\n",
      "469 (7.6s) loss=3.2273\n",
      "470 (7.6s) loss=3.2287\n",
      "471 (7.6s) loss=3.2338\n",
      "472 (7.6s) loss=3.2375\n",
      "473 (7.6s) loss=3.2340\n",
      "474 (7.6s) loss=3.2402\n",
      "475 (7.6s) loss=3.2260\n",
      "476 (7.6s) loss=3.2227\n",
      "477 (7.6s) loss=3.2234\n",
      "478 (7.6s) loss=3.2385\n",
      "479 (7.6s) loss=3.2232\n",
      "480 (7.6s) loss=3.2334\n",
      "481 (7.6s) loss=3.2250\n",
      "482 (7.6s) loss=3.2277\n",
      "483 (7.6s) loss=3.2357\n",
      "484 (7.6s) loss=3.2295\n",
      "485 (7.6s) loss=3.2369\n",
      "486 (7.6s) loss=3.2348\n",
      "487 (7.6s) loss=3.2279\n",
      "488 (7.6s) loss=3.2379\n",
      "489 (7.6s) loss=3.2303\n",
      "490 (7.6s) loss=3.2236\n",
      "491 (7.6s) loss=3.2373\n",
      "492 (7.6s) loss=3.2334\n",
      "493 (7.6s) loss=3.2293\n",
      "494 (7.6s) loss=3.2242\n",
      "495 (7.6s) loss=3.2307\n",
      "496 (7.6s) loss=3.2316\n",
      "497 (7.6s) loss=3.2275\n",
      "498 (7.6s) loss=3.2377\n",
      "499 (7.6s) loss=3.2268\n",
      "'apnecggf-fnyyudpscgtnozsp'\n",
      "'be'\n",
      "'cruathhtwqryaubx'\n",
      "'davyi'\n",
      "'esgxwlhndzv'\n",
      "'fl'\n",
      "'gxtuvmlprhf'\n",
      "'huxfbtnlbndinzigniaxr-'\n",
      "'inawifdulzjmptdp'\n",
      "'jehp'\n",
      "'kbaqf'\n",
      "'lvqg'\n",
      "'mpxycpvpsnrhvijdhqlif-pcu'\n",
      "'nlbk'\n",
      "'ofjay'\n",
      "'pou'\n",
      "'qbwzkghvfmyuqyesaktlmjfyn'\n",
      "'rhajdqqp'\n",
      "'scycetcp'\n",
      "'tabfouj'\n",
      "'udw-dtzsa'\n",
      "'vhrzdneeklhqadqujdlr'\n",
      "'wweghzq-n-oyug--ozqigfutk'\n",
      "'xjoqwakwklevihmfdtkl'\n",
      "'yyczvotfkaf'\n",
      "'znhelnf-fg'\n",
      "500 (7.5s) loss=3.2428\n",
      "501 (7.5s) loss=3.2240\n",
      "502 (7.6s) loss=3.2330\n",
      "503 (7.6s) loss=3.2277\n",
      "504 (7.6s) loss=3.2346\n",
      "505 (7.6s) loss=3.2182\n",
      "506 (7.6s) loss=3.2410\n",
      "507 (7.6s) loss=3.2277\n",
      "508 (7.6s) loss=3.2334\n",
      "509 (7.6s) loss=3.2217\n",
      "510 (7.6s) loss=3.2301\n",
      "511 (7.6s) loss=3.2322\n",
      "512 (7.6s) loss=3.2301\n",
      "513 (7.6s) loss=3.2346\n",
      "514 (7.6s) loss=3.2193\n",
      "515 (7.6s) loss=3.2293\n",
      "516 (7.6s) loss=3.2285\n",
      "517 (7.6s) loss=3.2314\n",
      "518 (7.6s) loss=3.2250\n",
      "519 (7.6s) loss=3.2369\n",
      "520 (7.6s) loss=3.2359\n",
      "521 (7.6s) loss=3.2383\n",
      "522 (7.6s) loss=3.2256\n",
      "523 (7.6s) loss=3.2307\n",
      "524 (7.6s) loss=3.2232\n",
      "525 (7.6s) loss=3.2309\n",
      "526 (7.6s) loss=3.2158\n",
      "527 (7.6s) loss=3.2279\n",
      "528 (7.6s) loss=3.2354\n",
      "529 (7.6s) loss=3.2391\n",
      "530 (7.6s) loss=3.2180\n",
      "531 (7.6s) loss=3.2223\n",
      "532 (7.6s) loss=3.2258\n",
      "533 (7.6s) loss=3.2375\n",
      "534 (7.6s) loss=3.2199\n",
      "535 (7.6s) loss=3.2285\n",
      "536 (7.6s) loss=3.2352\n",
      "537 (7.6s) loss=3.2271\n",
      "538 (7.6s) loss=3.2182\n",
      "539 (7.6s) loss=3.2244\n",
      "540 (7.6s) loss=3.3301\n",
      "541 (7.6s) loss=3.2357\n",
      "542 (7.6s) loss=3.2252\n",
      "543 (7.6s) loss=3.2150\n",
      "544 (7.6s) loss=3.2311\n",
      "545 (7.6s) loss=3.2205\n",
      "546 (7.6s) loss=3.2307\n",
      "547 (7.6s) loss=3.2223\n",
      "548 (7.6s) loss=3.2166\n",
      "549 (7.6s) loss=3.2061\n",
      "'akuskn'\n",
      "'b-siuarmgm'\n",
      "'cpmbxsmeyonk'\n",
      "'dry'\n",
      "'etrqf-whn'\n",
      "'fhrad'\n",
      "'gzbqdfppknjnbidgrbusbp--'\n",
      "'hqfymhnvdk'\n",
      "'iiedyhzsciunzixrskf'\n",
      "'jdhtowjcounzkyznrlyu'\n",
      "'kmt-qkswbfwtk-kuk'\n",
      "'lcq'\n",
      "'m-omhevozbipdrnlhnkatmesy'\n",
      "'nnlidnyoiwcyiohhxwtkcdkj'\n",
      "'o'\n",
      "'pgwp'\n",
      "'qnpvqewnojnqyv-jrmnkvwtda'\n",
      "'rahahmmdphnokilrzhk'\n",
      "'seincef-rfmzrxan'\n",
      "'to'\n",
      "'uoejagjwrpgdfggmhnubxning'\n",
      "'vgrnardfyfipaynpox'\n",
      "'whlxxfvi'\n",
      "'xxd-hsrav-f-x'\n",
      "'yg'\n",
      "'zltnjtmcqthfoldvbb'\n",
      "550 (7.6s) loss=3.2152\n",
      "551 (7.5s) loss=3.2047\n",
      "552 (7.6s) loss=3.2145\n",
      "553 (7.6s) loss=3.2070\n",
      "554 (7.6s) loss=3.2113\n",
      "555 (7.6s) loss=3.2043\n",
      "556 (7.6s) loss=3.2127\n",
      "557 (7.6s) loss=3.2596\n",
      "558 (7.6s) loss=3.2014\n",
      "559 (7.6s) loss=3.2014\n",
      "560 (7.6s) loss=3.1992\n",
      "561 (7.6s) loss=3.2145\n",
      "562 (7.6s) loss=3.1938\n",
      "563 (7.6s) loss=3.2209\n",
      "564 (7.6s) loss=3.3221\n",
      "565 (7.6s) loss=3.2004\n",
      "566 (7.6s) loss=3.2055\n",
      "567 (7.6s) loss=3.1934\n",
      "568 (7.6s) loss=3.1875\n",
      "569 (7.6s) loss=3.1988\n",
      "570 (7.6s) loss=3.1914\n",
      "571 (7.6s) loss=3.1879\n",
      "572 (7.6s) loss=3.2084\n",
      "573 (7.6s) loss=3.2885\n",
      "574 (7.6s) loss=3.2547\n",
      "575 (7.6s) loss=3.2371\n",
      "576 (7.6s) loss=3.2293\n",
      "577 (7.6s) loss=3.2197\n",
      "578 (7.6s) loss=3.2197\n",
      "579 (7.6s) loss=3.2135\n",
      "580 (7.6s) loss=3.2174\n",
      "581 (7.6s) loss=3.1900\n",
      "582 (7.6s) loss=3.1830\n",
      "583 (7.6s) loss=3.1830\n",
      "584 (7.6s) loss=3.1795\n",
      "585 (7.6s) loss=3.1777\n",
      "586 (7.6s) loss=3.1928\n",
      "587 (7.6s) loss=3.1732\n",
      "588 (7.6s) loss=3.1863\n",
      "589 (7.6s) loss=3.1750\n",
      "590 (7.6s) loss=3.2207\n",
      "591 (7.6s) loss=3.1934\n",
      "592 (7.6s) loss=3.2225\n",
      "593 (7.6s) loss=3.1650\n",
      "594 (7.6s) loss=3.1912\n",
      "595 (7.6s) loss=3.1863\n",
      "596 (7.6s) loss=3.1916\n",
      "597 (7.6s) loss=3.2242\n",
      "598 (7.6s) loss=3.1656\n",
      "599 (7.6s) loss=3.1678\n",
      "'aclihsagvonmw-uabn'\n",
      "'b'\n",
      "'czilo'\n",
      "'dppvwweqrvnhyha'\n",
      "'enqcfxpnegmoz'\n",
      "'f-rqyibmabkiefh'\n",
      "'gxkcugncsu-kiwdxjbjhjl'\n",
      "'hn-n'\n",
      "'iufxocan'\n",
      "'jrlf-fyv'\n",
      "'kihgqdckvcguurkeg'\n",
      "'lmaa'\n",
      "'mni-pdtc'\n",
      "'nustjpwq-n-apeqgrzlzdnte'\n",
      "'o'\n",
      "'pn'\n",
      "'qljxebxiplccobi'\n",
      "'rddhzhelieonb'\n",
      "'srhwadgmfleuqdkwlku'\n",
      "'tyimixrpapwdkesslfjlheasj'\n",
      "'uzjwflofgy-opt'\n",
      "'vjb'\n",
      "'wormroxlzu'\n",
      "'xwujhicuqekxb'\n",
      "'yj-neyfzhbrxdn'\n",
      "'zgssbaxgcphocqlmycgkp'\n",
      "600 (7.5s) loss=3.1779\n",
      "601 (7.5s) loss=3.1979\n",
      "602 (7.6s) loss=3.1691\n",
      "603 (7.6s) loss=3.1754\n",
      "604 (7.6s) loss=3.1707\n",
      "605 (7.6s) loss=3.1611\n",
      "606 (7.6s) loss=3.1701\n",
      "607 (7.6s) loss=3.1588\n",
      "608 (7.6s) loss=3.1605\n",
      "609 (7.6s) loss=3.2146\n",
      "610 (7.6s) loss=3.1584\n",
      "611 (7.6s) loss=3.1543\n",
      "612 (7.6s) loss=3.1508\n",
      "613 (7.6s) loss=3.1549\n",
      "614 (7.6s) loss=3.1461\n",
      "615 (7.6s) loss=3.1795\n",
      "616 (7.6s) loss=3.2529\n",
      "617 (7.6s) loss=3.1486\n",
      "618 (7.6s) loss=3.1527\n",
      "619 (7.6s) loss=3.1502\n",
      "620 (7.6s) loss=3.1451\n",
      "621 (7.6s) loss=3.1416\n",
      "622 (7.6s) loss=3.1432\n",
      "623 (7.6s) loss=3.1342\n",
      "624 (7.6s) loss=3.1410\n",
      "625 (7.6s) loss=3.1408\n",
      "626 (7.6s) loss=3.1420\n",
      "627 (7.6s) loss=3.1465\n",
      "628 (7.6s) loss=3.1324\n",
      "629 (7.6s) loss=3.1318\n",
      "630 (7.6s) loss=3.1322\n",
      "631 (7.6s) loss=3.1588\n",
      "632 (7.6s) loss=3.1408\n",
      "633 (7.6s) loss=3.1465\n",
      "634 (7.6s) loss=3.1299\n",
      "635 (7.6s) loss=3.1381\n",
      "636 (7.6s) loss=3.1436\n",
      "637 (7.6s) loss=3.1307\n",
      "638 (7.6s) loss=3.1316\n",
      "639 (7.6s) loss=3.1293\n",
      "640 (7.6s) loss=3.1322\n",
      "641 (7.6s) loss=3.1328\n",
      "642 (7.6s) loss=3.1285\n",
      "643 (7.6s) loss=3.1313\n",
      "644 (7.6s) loss=3.1418\n",
      "645 (7.6s) loss=3.1320\n",
      "646 (7.6s) loss=3.1232\n",
      "647 (7.6s) loss=3.1270\n",
      "648 (7.6s) loss=3.1219\n",
      "649 (7.6s) loss=3.1270\n",
      "'a-hxzqnr-bpebkfriglzsnih'\n",
      "'bcj'\n",
      "'ceeptoarvg-hxbye-b'\n",
      "'dpke-xktntwne'\n",
      "'ekibgeypwj'\n",
      "'fse-xmhgckodnhmpgnnupyrez'\n",
      "'gcwrhewqwwzg'\n",
      "'hudkvnr'\n",
      "'izd'\n",
      "'j'\n",
      "'kzsmbumenvsmot'\n",
      "'l'\n",
      "'miqtqmatctxikrhkya'\n",
      "'npepaen'\n",
      "'oxgs'\n",
      "'psiy'\n",
      "'qqisbzcin'\n",
      "'r'\n",
      "'suc'\n",
      "'t'\n",
      "'ueqwvpqazzsqipjn'\n",
      "'v-kiiagdupggqfomrqghensdk'\n",
      "'wh'\n",
      "'xreenr'\n",
      "'yxfmb'\n",
      "'zxrgckybpfnozkz'\n",
      "650 (7.5s) loss=3.1232\n",
      "651 (7.5s) loss=3.1307\n",
      "652 (7.6s) loss=3.1264\n",
      "653 (7.6s) loss=3.1201\n",
      "654 (7.6s) loss=3.1227\n",
      "655 (7.6s) loss=3.1174\n",
      "656 (7.6s) loss=3.1187\n",
      "657 (7.6s) loss=3.1240\n",
      "658 (7.6s) loss=3.1324\n",
      "659 (7.6s) loss=3.1201\n",
      "660 (7.6s) loss=3.1266\n",
      "661 (7.6s) loss=3.1148\n",
      "662 (7.6s) loss=3.1187\n",
      "663 (7.6s) loss=3.1203\n",
      "664 (7.6s) loss=3.1232\n",
      "665 (7.6s) loss=3.1168\n",
      "666 (7.6s) loss=3.1187\n",
      "667 (7.6s) loss=3.1215\n",
      "668 (7.6s) loss=3.1354\n",
      "669 (7.6s) loss=3.1164\n",
      "670 (7.6s) loss=3.1186\n",
      "671 (7.6s) loss=3.1193\n",
      "672 (7.6s) loss=3.1258\n",
      "673 (7.6s) loss=3.1264\n",
      "674 (7.6s) loss=3.1180\n",
      "675 (7.6s) loss=3.1273\n",
      "676 (7.6s) loss=3.1105\n",
      "677 (7.6s) loss=3.1176\n",
      "678 (7.6s) loss=3.1230\n",
      "679 (7.6s) loss=3.1158\n",
      "680 (7.6s) loss=3.1113\n",
      "681 (7.6s) loss=3.1174\n",
      "682 (7.6s) loss=3.1137\n",
      "683 (7.6s) loss=3.1139\n",
      "684 (7.6s) loss=3.1084\n",
      "685 (7.6s) loss=3.1090\n",
      "686 (7.6s) loss=3.1086\n",
      "687 (7.6s) loss=3.1092\n",
      "688 (7.6s) loss=3.1127\n",
      "689 (7.6s) loss=3.1045\n",
      "690 (7.6s) loss=3.1063\n",
      "691 (7.6s) loss=3.1061\n",
      "692 (7.6s) loss=3.1072\n",
      "693 (7.6s) loss=3.1127\n",
      "694 (7.6s) loss=3.1047\n",
      "695 (7.6s) loss=3.1033\n",
      "696 (7.6s) loss=3.1076\n",
      "697 (7.6s) loss=3.1035\n",
      "698 (7.6s) loss=3.1063\n",
      "699 (7.6s) loss=3.1125\n",
      "'ayponbshj'\n",
      "'bixtsmannlqp-'\n",
      "'cncy'\n",
      "'dbp'\n",
      "'egfrijymntqnwbsawxrzphwgn'\n",
      "'fxcjiynnehnntroiccyc-e'\n",
      "'gj'\n",
      "'h'\n",
      "'ixlpajsbbtwnunxfv-akinghn'\n",
      "'jjn'\n",
      "'knnhjoorzkkudy'\n",
      "'lhwdnclymytsmqcsrxelpy'\n",
      "'mwftrjvwcdnnw-jojfkxnn'\n",
      "'nx'\n",
      "'oictnrlhqk-hmbgixr'\n",
      "'pehnghxngbfgv'\n",
      "'qbxr'\n",
      "'rrezdpkwxm-ftgoyyuqxiimto'\n",
      "'snxk'\n",
      "'tgnoyedeoo'\n",
      "'uvms'\n",
      "'vcsa-gpymynshos-yofnelbip'\n",
      "'w'\n",
      "'xiuanvc'\n",
      "'ycn'\n",
      "'zjkeuygjvke'\n",
      "700 (7.5s) loss=3.1084\n",
      "701 (7.5s) loss=3.0986\n",
      "702 (7.6s) loss=3.1027\n",
      "703 (7.6s) loss=3.0994\n",
      "704 (7.6s) loss=3.1053\n",
      "705 (7.6s) loss=3.1035\n",
      "706 (7.6s) loss=3.1027\n",
      "707 (7.6s) loss=3.1043\n",
      "708 (7.6s) loss=3.1039\n",
      "709 (7.6s) loss=3.1070\n",
      "710 (7.6s) loss=3.1078\n",
      "711 (7.6s) loss=3.1021\n",
      "712 (7.6s) loss=3.1014\n",
      "713 (7.6s) loss=3.0998\n",
      "714 (7.6s) loss=3.1047\n",
      "715 (7.6s) loss=3.1059\n",
      "716 (7.6s) loss=3.1125\n",
      "717 (7.6s) loss=3.0990\n",
      "718 (7.6s) loss=3.1004\n",
      "719 (7.6s) loss=3.0977\n",
      "720 (7.6s) loss=3.1012\n",
      "721 (7.6s) loss=3.1090\n",
      "722 (7.6s) loss=3.1027\n",
      "723 (7.6s) loss=3.1014\n",
      "724 (7.6s) loss=3.0984\n",
      "725 (7.6s) loss=3.0992\n",
      "726 (7.6s) loss=3.1078\n",
      "727 (7.6s) loss=3.1268\n",
      "728 (7.6s) loss=3.1016\n",
      "729 (7.6s) loss=3.0988\n",
      "730 (7.6s) loss=3.0973\n",
      "731 (7.6s) loss=3.0992\n",
      "732 (7.6s) loss=3.1010\n",
      "733 (7.6s) loss=3.0977\n",
      "734 (7.6s) loss=3.0916\n",
      "735 (7.6s) loss=3.0990\n",
      "736 (7.6s) loss=3.0969\n",
      "737 (7.6s) loss=3.0926\n",
      "738 (7.6s) loss=3.0943\n",
      "739 (7.6s) loss=3.0928\n",
      "740 (7.6s) loss=3.0934\n",
      "741 (7.6s) loss=3.0928\n",
      "742 (7.6s) loss=3.0971\n",
      "743 (7.6s) loss=3.1086\n",
      "744 (7.6s) loss=3.0998\n",
      "745 (7.6s) loss=3.1010\n",
      "746 (7.6s) loss=3.0918\n",
      "747 (7.6s) loss=3.0904\n",
      "748 (7.6s) loss=3.0893\n",
      "749 (7.6s) loss=3.1143\n",
      "'auzeajqojmppohxnrk-cju-wq'\n",
      "'bv'\n",
      "'czggmlymde-'\n",
      "'dldqg---amt-gcce'\n",
      "'eolhqtntqgvruvciiqxug-kek'\n",
      "'fwlccxfqzrlhdofdaqlwllfoh'\n",
      "'gmtg-jfcolslki'\n",
      "'hfbl-'\n",
      "'igflnfuez-hwgatdyb'\n",
      "'jvh-siylq'\n",
      "'km'\n",
      "'lvifcfqbpzsqudwaxx-lmttu'\n",
      "'mlcxfi'\n",
      "'nxtjgjhpsmgupquahwwgpdpra'\n",
      "'opjgzeackxlcrupjseydvadof'\n",
      "'p-mcctdv'\n",
      "'qmmafwxfdzdcxpadvl-wgfzlb'\n",
      "'rxythixkcflxeehtnsx'\n",
      "'sl-iohzhnrorjhmms'\n",
      "'t'\n",
      "'ustakcwvfklnejzb-qvh'\n",
      "'vorxpfrnrsrshq'\n",
      "'wnressmlc'\n",
      "'xsu'\n",
      "'yokrxnryxhbxt'\n",
      "'zylluxycqlosg'\n",
      "750 (7.5s) loss=3.1244\n",
      "751 (7.5s) loss=3.1078\n",
      "752 (7.6s) loss=3.0988\n",
      "753 (7.6s) loss=3.1004\n",
      "754 (7.6s) loss=3.0979\n",
      "755 (7.6s) loss=3.0998\n",
      "756 (7.6s) loss=3.0930\n",
      "757 (7.6s) loss=3.0824\n",
      "758 (7.6s) loss=3.0879\n",
      "759 (7.6s) loss=3.0936\n",
      "760 (7.6s) loss=3.0863\n",
      "761 (7.6s) loss=3.0971\n",
      "762 (7.6s) loss=3.0988\n",
      "763 (7.6s) loss=3.0895\n",
      "764 (7.6s) loss=3.0969\n",
      "765 (7.6s) loss=3.0982\n",
      "766 (7.6s) loss=3.0902\n",
      "767 (7.6s) loss=3.1105\n",
      "768 (7.6s) loss=3.0863\n",
      "769 (7.6s) loss=3.0830\n",
      "770 (7.6s) loss=3.0770\n",
      "771 (7.6s) loss=3.0809\n",
      "772 (7.6s) loss=3.0791\n",
      "773 (7.6s) loss=3.0928\n",
      "774 (7.6s) loss=3.0777\n",
      "775 (7.6s) loss=3.0768\n",
      "776 (7.6s) loss=3.0912\n",
      "777 (7.6s) loss=3.1006\n",
      "778 (7.6s) loss=3.0848\n",
      "779 (7.6s) loss=3.0977\n",
      "780 (7.6s) loss=3.0773\n",
      "781 (7.6s) loss=3.0770\n",
      "782 (7.6s) loss=3.0797\n",
      "783 (7.6s) loss=3.0723\n",
      "784 (7.6s) loss=3.0795\n",
      "785 (7.6s) loss=3.0785\n",
      "786 (7.6s) loss=3.0865\n",
      "787 (7.6s) loss=3.0770\n",
      "788 (7.6s) loss=3.0768\n",
      "789 (7.6s) loss=3.0775\n",
      "790 (7.6s) loss=3.0875\n",
      "791 (7.6s) loss=3.0738\n",
      "792 (7.6s) loss=3.0721\n",
      "793 (7.6s) loss=3.0740\n",
      "794 (7.6s) loss=3.0795\n",
      "795 (7.6s) loss=3.0719\n",
      "796 (7.6s) loss=3.0734\n",
      "797 (7.6s) loss=3.1205\n",
      "798 (7.6s) loss=3.0855\n",
      "799 (7.6s) loss=3.0742\n",
      "'a'\n",
      "'bpj'\n",
      "'cermvge'\n",
      "'dslevnblizkikt'\n",
      "'ejthpbaebsclopa'\n",
      "'ffdtzzht-elhvflyhlh'\n",
      "'gjsqpiynslwbsvuudkzmgghf'\n",
      "'hivs'\n",
      "'iamielbpex'\n",
      "'jeitsamlzra'\n",
      "'kb'\n",
      "'lzcfzgxyglji'\n",
      "'mctpcefolgbtefnunoolpnjlb'\n",
      "'nfmeg'\n",
      "'ovtota'\n",
      "'pgnylanj'\n",
      "'qtrvmml-wlxdrdhxjupaqxlew'\n",
      "'r'\n",
      "'szenzizowyp'\n",
      "'txikbwpjkdzlczkanllcjfztm'\n",
      "'upuf-kll'\n",
      "'vfbd'\n",
      "'wpclcl'\n",
      "'xc'\n",
      "'y-ooyqexd'\n",
      "'zlzqzzyfqk-gfhrlmljipllkc'\n",
      "800 (7.6s) loss=3.0736\n",
      "801 (7.5s) loss=3.0873\n",
      "802 (7.6s) loss=3.0828\n",
      "803 (7.6s) loss=3.0705\n",
      "804 (7.6s) loss=3.0744\n",
      "805 (7.6s) loss=3.0742\n",
      "806 (7.6s) loss=3.0873\n",
      "807 (7.6s) loss=3.0783\n",
      "808 (7.6s) loss=3.0738\n",
      "809 (7.6s) loss=3.0715\n",
      "810 (7.6s) loss=3.0686\n",
      "811 (7.6s) loss=3.0730\n",
      "812 (7.6s) loss=3.0818\n",
      "813 (7.6s) loss=3.0746\n",
      "814 (7.6s) loss=3.0736\n",
      "815 (7.6s) loss=3.0713\n",
      "816 (7.6s) loss=3.0703\n",
      "817 (7.6s) loss=3.0693\n",
      "818 (7.6s) loss=3.0695\n",
      "819 (7.6s) loss=3.0764\n",
      "820 (7.6s) loss=3.0771\n",
      "821 (7.6s) loss=3.0732\n",
      "822 (7.6s) loss=3.0705\n",
      "823 (7.6s) loss=3.0732\n",
      "824 (7.6s) loss=3.0838\n",
      "825 (7.6s) loss=3.0887\n",
      "826 (7.6s) loss=3.0715\n",
      "827 (7.6s) loss=3.0729\n",
      "828 (7.6s) loss=3.0684\n",
      "829 (7.6s) loss=3.0695\n",
      "830 (7.6s) loss=3.0764\n",
      "831 (7.6s) loss=3.0760\n",
      "832 (7.6s) loss=3.0734\n",
      "833 (7.6s) loss=3.0686\n",
      "834 (7.6s) loss=3.1121\n",
      "835 (7.6s) loss=3.0744\n",
      "836 (7.6s) loss=3.0697\n",
      "837 (7.6s) loss=3.0682\n",
      "838 (7.6s) loss=3.0689\n",
      "839 (7.6s) loss=3.0701\n",
      "840 (7.6s) loss=3.1080\n",
      "841 (7.6s) loss=3.0859\n",
      "842 (7.6s) loss=3.0684\n",
      "843 (7.6s) loss=3.1484\n",
      "844 (7.6s) loss=3.1354\n",
      "845 (7.6s) loss=3.1355\n",
      "846 (7.6s) loss=3.0766\n",
      "847 (7.6s) loss=3.1395\n",
      "848 (7.6s) loss=3.1027\n",
      "849 (7.6s) loss=3.0877\n",
      "'akgbrbywve-ivqduqazjhyfi-'\n",
      "'bufzrhm'\n",
      "'cvadfv'\n",
      "'dgwpggjsjfpbljtelrlwllilv'\n",
      "'eztdok'\n",
      "'fdal-f-key-rgvmddrthws'\n",
      "'gzqqlmheu'\n",
      "'hdpkoqxrownykl'\n",
      "'ig'\n",
      "'jokpk'\n",
      "'kn-nisfpgkqanllnz-yym-bcq'\n",
      "'lbsd'\n",
      "'m'\n",
      "'nrquctzthadowhlnjvu-pkspx'\n",
      "'obadkdzclflgbvrekl'\n",
      "'pomxlqlwvyzcjcwcluvcqr'\n",
      "'qvawajnceh'\n",
      "'r'\n",
      "'sdhul-hqwzqmeapgmswgsqqsx'\n",
      "'tf'\n",
      "'ukwawjwpcmobsbwfufivovnip'\n",
      "'vhxlsy-wsdcvyqvlfrrazpd'\n",
      "'w'\n",
      "'xbkyfoceosaoouocgnqulm-ea'\n",
      "'ygnr'\n",
      "'zh'\n",
      "850 (7.6s) loss=3.0951\n",
      "851 (7.5s) loss=3.0809\n",
      "852 (7.6s) loss=3.0902\n",
      "853 (7.6s) loss=3.1006\n",
      "854 (7.6s) loss=3.0877\n",
      "855 (7.6s) loss=3.1094\n",
      "856 (7.6s) loss=3.0922\n",
      "857 (7.6s) loss=3.0820\n",
      "858 (7.6s) loss=3.0773\n",
      "859 (7.6s) loss=3.0867\n",
      "860 (7.6s) loss=3.3318\n",
      "861 (7.6s) loss=3.3750\n",
      "862 (7.6s) loss=3.3088\n",
      "863 (7.6s) loss=3.2594\n",
      "864 (7.6s) loss=3.2260\n",
      "865 (7.6s) loss=3.1857\n",
      "866 (7.6s) loss=3.1799\n",
      "867 (7.6s) loss=3.1768\n",
      "868 (7.6s) loss=3.1549\n",
      "869 (7.6s) loss=3.1705\n",
      "870 (7.6s) loss=3.2143\n",
      "871 (7.6s) loss=3.1750\n",
      "872 (7.6s) loss=3.1598\n",
      "873 (7.6s) loss=3.1523\n",
      "874 (7.6s) loss=3.1527\n",
      "875 (7.6s) loss=3.1490\n",
      "876 (7.6s) loss=3.1430\n",
      "877 (7.6s) loss=3.1430\n",
      "878 (7.6s) loss=3.1455\n",
      "879 (7.6s) loss=3.1416\n",
      "880 (7.6s) loss=3.1369\n",
      "881 (7.6s) loss=3.1391\n",
      "882 (7.6s) loss=3.1412\n",
      "883 (7.6s) loss=3.1404\n",
      "884 (7.6s) loss=3.1383\n",
      "885 (7.6s) loss=3.1357\n",
      "886 (7.6s) loss=3.1385\n",
      "887 (7.6s) loss=3.1352\n",
      "888 (7.6s) loss=3.1303\n",
      "889 (7.6s) loss=3.1428\n",
      "890 (7.6s) loss=3.1270\n",
      "891 (7.6s) loss=3.1416\n",
      "892 (7.6s) loss=3.1291\n",
      "893 (7.6s) loss=3.1332\n",
      "894 (7.6s) loss=3.1287\n",
      "895 (7.6s) loss=3.1361\n",
      "896 (7.6s) loss=3.1285\n",
      "897 (7.6s) loss=3.1262\n",
      "898 (7.6s) loss=3.1270\n",
      "899 (7.6s) loss=3.1244\n",
      "'aahmdpypr'\n",
      "'bmjz'\n",
      "'cldwnskfetvpzouyoyl'\n",
      "'drehmfaosrgymxisendvxmnmn'\n",
      "'euvbgbncceb'\n",
      "'flmpro-uiu'\n",
      "'gasgac'\n",
      "'htgqd'\n",
      "'izwulp-vlth'\n",
      "'j'\n",
      "'kaaaee'\n",
      "'l-djcxolor--tlcaoi'\n",
      "'mogwx'\n",
      "'nz-zqz'\n",
      "'oiytofiuqmlpze-xdflv'\n",
      "'pqa-'\n",
      "'qiiimdg'\n",
      "'rbz'\n",
      "'sgbric'\n",
      "'tz'\n",
      "'umfrbsi'\n",
      "'vgbpcmuqjvhclsflyva-ls'\n",
      "'w-ej-nfidjryd'\n",
      "'xjd'\n",
      "'ywisallod'\n",
      "'zhfqgpliiygs'\n",
      "900 (7.6s) loss=3.1287\n",
      "901 (7.6s) loss=3.1260\n",
      "902 (7.6s) loss=3.1217\n",
      "903 (7.6s) loss=3.1201\n",
      "904 (7.6s) loss=3.1234\n",
      "905 (7.6s) loss=3.1320\n",
      "906 (7.6s) loss=3.1215\n",
      "907 (7.6s) loss=3.1240\n",
      "908 (7.6s) loss=3.1238\n",
      "909 (7.6s) loss=3.1229\n",
      "910 (7.6s) loss=3.1238\n",
      "911 (7.6s) loss=3.1172\n",
      "912 (7.6s) loss=3.1170\n",
      "913 (7.6s) loss=3.1164\n",
      "914 (7.6s) loss=3.1162\n",
      "915 (7.6s) loss=3.1268\n",
      "916 (7.6s) loss=3.1209\n",
      "917 (7.6s) loss=3.1207\n",
      "918 (7.6s) loss=3.1168\n",
      "919 (7.6s) loss=3.1139\n",
      "920 (7.6s) loss=3.1289\n",
      "921 (7.6s) loss=3.1187\n",
      "922 (7.6s) loss=3.1150\n",
      "923 (7.6s) loss=3.1176\n",
      "924 (7.6s) loss=3.1168\n",
      "925 (7.6s) loss=3.1197\n",
      "926 (7.6s) loss=3.1133\n",
      "927 (7.6s) loss=3.1141\n",
      "928 (7.6s) loss=3.1184\n",
      "929 (7.6s) loss=3.1150\n",
      "930 (7.6s) loss=3.1127\n",
      "931 (7.6s) loss=3.1170\n",
      "932 (7.6s) loss=3.1160\n",
      "933 (7.6s) loss=3.1104\n",
      "934 (7.6s) loss=3.1137\n",
      "935 (7.6s) loss=3.1098\n",
      "936 (7.6s) loss=3.1094\n",
      "937 (7.6s) loss=3.1129\n",
      "938 (7.6s) loss=3.1129\n",
      "939 (7.6s) loss=3.1109\n",
      "940 (7.6s) loss=3.1143\n",
      "941 (7.6s) loss=3.1139\n",
      "942 (7.6s) loss=3.1139\n",
      "943 (7.6s) loss=3.1105\n",
      "944 (7.6s) loss=3.1066\n",
      "945 (7.6s) loss=3.1123\n",
      "946 (7.6s) loss=3.1076\n",
      "947 (7.6s) loss=3.1121\n",
      "948 (7.6s) loss=3.1115\n",
      "949 (7.6s) loss=3.1072\n",
      "'apgpuo-hltdllnqbvlldlsj'\n",
      "'btgygv'\n",
      "'ckyrwthpsjaqde-qto'\n",
      "'dcelhnip'\n",
      "'etfjoojmla'\n",
      "'fkdylvuvrueuhckchaucpzdhe'\n",
      "'g-rogleqzyfiwmlzzjqhepvcq'\n",
      "'hfxn'\n",
      "'iwlaqx-hr'\n",
      "'jcglrchsztsucpbhncboogki'\n",
      "'kidlq'\n",
      "'lcs-e-osa'\n",
      "'mvcwibkevljqe-ipxtedkl'\n",
      "'n---lomi-gnsgyfemj-b'\n",
      "'omsqnnkmfzm'\n",
      "'pdncfbfslpjwloucwplojii-x'\n",
      "'qzy'\n",
      "'rszfeqm-yz--xsdzx'\n",
      "'smkcusnmalrwnfvwllhcmmmlx'\n",
      "'tpdzn'\n",
      "'uemqcbqdaglehdlhplnkbgcwl'\n",
      "'vbepixymml-xwdbsqvbsnanck'\n",
      "'w---xuwulthhslctr-'\n",
      "'xfml'\n",
      "'ymvueaahd-xmrzsl'\n",
      "'znkkbowpw-qmyrzloyvytls'\n",
      "950 (7.5s) loss=3.1057\n",
      "951 (7.5s) loss=3.1082\n",
      "952 (7.6s) loss=3.1215\n",
      "953 (7.6s) loss=3.1123\n",
      "954 (7.6s) loss=3.1086\n",
      "955 (7.6s) loss=3.1100\n",
      "956 (7.6s) loss=3.1123\n",
      "957 (7.6s) loss=3.1074\n",
      "958 (7.6s) loss=3.1074\n",
      "959 (7.6s) loss=3.1066\n",
      "960 (7.6s) loss=3.1068\n",
      "961 (7.6s) loss=3.1094\n",
      "962 (7.6s) loss=3.1078\n",
      "963 (7.6s) loss=3.1068\n",
      "964 (7.6s) loss=3.1080\n",
      "965 (7.6s) loss=3.1037\n",
      "966 (7.6s) loss=3.1029\n",
      "967 (7.6s) loss=3.1059\n",
      "968 (7.6s) loss=3.1037\n",
      "969 (7.6s) loss=3.1041\n",
      "970 (7.6s) loss=3.1020\n",
      "971 (7.6s) loss=3.1084\n",
      "972 (7.6s) loss=3.1021\n",
      "973 (7.6s) loss=3.1049\n",
      "974 (7.6s) loss=3.1045\n",
      "975 (7.6s) loss=3.1031\n",
      "976 (7.6s) loss=3.1029\n",
      "977 (7.6s) loss=3.1043\n",
      "978 (7.6s) loss=3.1039\n",
      "979 (7.6s) loss=3.1035\n",
      "980 (7.6s) loss=3.1018\n",
      "981 (7.6s) loss=3.1023\n",
      "982 (7.6s) loss=3.1027\n",
      "983 (7.6s) loss=3.1023\n",
      "984 (7.6s) loss=3.1021\n",
      "985 (7.6s) loss=3.1037\n",
      "986 (7.6s) loss=3.1018\n",
      "987 (7.6s) loss=3.1016\n",
      "988 (7.6s) loss=3.1145\n",
      "989 (7.6s) loss=3.1156\n",
      "990 (7.6s) loss=3.1045\n",
      "991 (7.6s) loss=3.1014\n",
      "992 (7.6s) loss=3.0996\n",
      "993 (7.6s) loss=3.1023\n",
      "994 (7.6s) loss=3.1234\n",
      "995 (7.6s) loss=3.1059\n",
      "996 (7.6s) loss=3.1080\n",
      "997 (7.6s) loss=3.1033\n",
      "998 (7.6s) loss=3.1043\n",
      "999 (7.6s) loss=3.1098\n",
      "'auitzcsvwbtjazbgp'\n",
      "'bggxgeeftpqpbvnr'\n",
      "'cjwafonkqccdbabsfod'\n",
      "'dniwllk-nenlv-bjeqgnf'\n",
      "'ehzpxqdwdocfhbhmkhnjrmxmu'\n",
      "'flajyqqnn-kjpyq-dwubhuwtn'\n",
      "'g'\n",
      "'huhn'\n",
      "'ikqrp'\n",
      "'jeqjztpfgknzt'\n",
      "'k'\n",
      "'luoxjndqfykohrliphwbe'\n",
      "'mncjrcgyxrdqpp'\n",
      "'nwpnupuugbufnotxfiyozt'\n",
      "'olbzb'\n",
      "'pi'\n",
      "'qit'\n",
      "'rxcbutmdd'\n",
      "'snsgnwkbbpzaxqlwje'\n",
      "'twufp'\n",
      "'ujqoxnuxggdjekg'\n",
      "'vpnwyuaktgnpczemsc'\n",
      "'ws'\n",
      "'xk-jyihzcnngquclnnhepfwpp'\n",
      "'ys-pbnfme'\n",
      "'zswtwuwmtpejnrmgi'\n",
      "1000 (7.6s) loss=3.1049\n",
      "1001 (7.5s) loss=3.1006\n",
      "1002 (7.6s) loss=3.1008\n",
      "1003 (7.6s) loss=3.1051\n",
      "1004 (7.6s) loss=3.0998\n",
      "1005 (7.6s) loss=3.1049\n",
      "1006 (7.6s) loss=3.1033\n",
      "1007 (7.6s) loss=3.1023\n",
      "1008 (7.6s) loss=3.0975\n",
      "1009 (7.6s) loss=3.1006\n",
      "1010 (7.6s) loss=3.0988\n",
      "1011 (7.6s) loss=3.1018\n",
      "1012 (7.6s) loss=3.1016\n",
      "1013 (7.6s) loss=3.1051\n",
      "1014 (7.6s) loss=3.1035\n",
      "1015 (7.6s) loss=3.0967\n",
      "1016 (7.6s) loss=3.1004\n",
      "1017 (7.6s) loss=3.1016\n",
      "1018 (7.6s) loss=3.1043\n",
      "1019 (7.6s) loss=3.1027\n",
      "1020 (7.6s) loss=3.0986\n",
      "1021 (7.6s) loss=3.0969\n",
      "1022 (7.6s) loss=3.0996\n",
      "1023 (7.6s) loss=3.0988\n",
      "1024 (7.6s) loss=3.0988\n",
      "1025 (7.6s) loss=3.0975\n",
      "1026 (7.6s) loss=3.0998\n",
      "1027 (7.6s) loss=3.0979\n",
      "1028 (7.6s) loss=3.0984\n",
      "1029 (7.6s) loss=3.1023\n",
      "1030 (7.6s) loss=3.0975\n",
      "1031 (7.6s) loss=3.0973\n",
      "1032 (7.6s) loss=3.0980\n",
      "1033 (7.6s) loss=3.1039\n",
      "1034 (7.6s) loss=3.0959\n",
      "1035 (7.6s) loss=3.0988\n",
      "1036 (7.6s) loss=3.0969\n",
      "1037 (7.6s) loss=3.0959\n",
      "1038 (7.6s) loss=3.0959\n",
      "1039 (7.6s) loss=3.0963\n",
      "1040 (7.6s) loss=3.0990\n",
      "1041 (7.6s) loss=3.0963\n",
      "1042 (7.6s) loss=3.0939\n",
      "1043 (7.6s) loss=3.0961\n",
      "1044 (7.6s) loss=3.0980\n",
      "1045 (7.6s) loss=3.0965\n",
      "1046 (7.6s) loss=3.0998\n",
      "1047 (7.6s) loss=3.0918\n",
      "1048 (7.6s) loss=3.0928\n",
      "1049 (7.7s) loss=3.1008\n",
      "'abfhhazhyx'\n",
      "'budoqapjreqz'\n",
      "'cb'\n",
      "'dgjtjvwnmttzfvhnfpaprbimu'\n",
      "'e'\n",
      "'fkqynnc'\n",
      "'ggthrwuhuutrheab-'\n",
      "'hhuhyiqqjufjujt'\n",
      "'ivgl-'\n",
      "'jwnznfv'\n",
      "'kgnjqx-uko'\n",
      "'lcqntwjgjrrrqayaakvga-nbl'\n",
      "'msvhnlkpyvymfk-'\n",
      "'n'\n",
      "'ottqjeznmtxwpxpnrqleptf'\n",
      "'pydtza-quekelqla-tru-wbth'\n",
      "'q-rlh-d-cnueqntzynqguzwtn'\n",
      "'rjovdghmigdfeebixuwafncvp'\n",
      "'sj'\n",
      "'twyx--gvrcm'\n",
      "'upmgwrpnwwmynsbeleapn'\n",
      "'vjifcnlk-amm'\n",
      "'whpugyhfmnjyiclvjpsxsqseh'\n",
      "'xcmdlhqnxwzznurfx'\n",
      "'ym-ehnqyzun'\n",
      "'zn'\n",
      "1050 (7.6s) loss=3.0959\n",
      "1051 (7.6s) loss=3.0965\n",
      "1052 (7.6s) loss=3.0996\n",
      "1053 (7.6s) loss=3.1004\n",
      "1054 (7.6s) loss=3.0928\n",
      "1055 (7.6s) loss=3.0953\n",
      "1056 (7.6s) loss=3.0973\n",
      "1057 (7.6s) loss=3.0992\n",
      "1058 (7.6s) loss=3.0930\n",
      "1059 (7.6s) loss=3.0945\n",
      "1060 (7.6s) loss=3.0951\n",
      "1061 (7.6s) loss=3.0924\n",
      "1062 (7.6s) loss=3.0939\n",
      "1063 (7.6s) loss=3.0973\n",
      "1064 (7.6s) loss=3.0912\n",
      "1065 (7.6s) loss=3.0988\n",
      "1066 (7.6s) loss=3.0920\n",
      "1067 (7.6s) loss=3.0969\n",
      "1068 (7.6s) loss=3.0953\n",
      "1069 (7.6s) loss=3.0877\n",
      "1070 (7.6s) loss=3.0887\n",
      "1071 (7.6s) loss=3.0955\n",
      "1072 (7.6s) loss=3.0889\n",
      "1073 (7.6s) loss=3.0932\n",
      "1074 (7.6s) loss=3.0963\n",
      "1075 (7.6s) loss=3.0959\n",
      "1076 (7.6s) loss=3.0908\n",
      "1077 (7.6s) loss=3.0941\n",
      "1078 (7.6s) loss=3.0982\n",
      "1079 (7.6s) loss=3.0898\n",
      "1080 (7.6s) loss=3.0875\n",
      "1081 (7.6s) loss=3.1006\n",
      "1082 (7.6s) loss=3.1240\n",
      "1083 (7.6s) loss=3.0973\n",
      "1084 (7.6s) loss=3.0973\n",
      "1085 (7.6s) loss=3.0961\n",
      "1086 (7.6s) loss=3.0967\n",
      "1087 (7.6s) loss=3.0936\n",
      "1088 (7.6s) loss=3.0887\n",
      "1089 (7.6s) loss=3.0928\n",
      "1090 (7.6s) loss=3.0959\n",
      "1091 (7.6s) loss=3.0918\n",
      "1092 (7.6s) loss=3.0955\n",
      "1093 (7.6s) loss=3.0912\n",
      "1094 (7.6s) loss=3.0879\n",
      "1095 (7.6s) loss=3.0900\n",
      "1096 (7.6s) loss=3.0904\n",
      "1097 (7.6s) loss=3.0896\n",
      "1098 (7.6s) loss=3.0959\n",
      "1099 (7.6s) loss=3.0904\n",
      "'ankpuzutnmyfmengqehott'\n",
      "'b'\n",
      "'c'\n",
      "'dhyuz-k'\n",
      "'ez-cnnttiuhuuzi-csisdwseo'\n",
      "'ff-iiy'\n",
      "'ghalmvngc-'\n",
      "'h'\n",
      "'injpy'\n",
      "'j'\n",
      "'kamldjpdlldk'\n",
      "'lwg-aiufsoawl'\n",
      "'midr'\n",
      "'nnoyepe-'\n",
      "'odruxwszjoridgequcn-vnjnx'\n",
      "'pna-knybqfancgekbywjmpnfd'\n",
      "'qncyitpinzjriszzpcemudvzr'\n",
      "'regtnoldzbomgnzo'\n",
      "'senwhqntmnxuuymrciak'\n",
      "'tynk'\n",
      "'ukzlqln-ztsxdwebdxmtjeivu'\n",
      "'vghun'\n",
      "'wg'\n",
      "'xbxvfynny-bm'\n",
      "'yhiqrogumnvtdfmikkfew'\n",
      "'zcvopb'\n",
      "1100 (7.6s) loss=3.0867\n",
      "1101 (7.6s) loss=3.0848\n",
      "1102 (7.6s) loss=3.0889\n",
      "1103 (7.6s) loss=3.0883\n",
      "1104 (7.6s) loss=3.0922\n",
      "1105 (7.6s) loss=3.0924\n",
      "1106 (7.6s) loss=3.0889\n",
      "1107 (7.6s) loss=3.0836\n",
      "1108 (7.6s) loss=3.0834\n",
      "1109 (7.6s) loss=3.0951\n",
      "1110 (7.6s) loss=3.0908\n",
      "1111 (7.6s) loss=3.0877\n",
      "1112 (7.6s) loss=3.0832\n",
      "1113 (7.6s) loss=3.0801\n",
      "1114 (7.6s) loss=3.0799\n",
      "1115 (7.6s) loss=3.0842\n",
      "1116 (7.6s) loss=3.0811\n",
      "1117 (7.6s) loss=3.0826\n",
      "1118 (7.6s) loss=3.0840\n",
      "1119 (7.6s) loss=3.0826\n",
      "1120 (7.6s) loss=3.0916\n",
      "1121 (7.6s) loss=3.0822\n",
      "1122 (7.6s) loss=3.0865\n",
      "1123 (7.6s) loss=3.0775\n",
      "1124 (7.6s) loss=3.0795\n",
      "1125 (7.6s) loss=3.0770\n",
      "1126 (7.6s) loss=3.0820\n",
      "1127 (7.6s) loss=3.0826\n",
      "1128 (7.6s) loss=3.0818\n",
      "1129 (7.6s) loss=3.0812\n",
      "1130 (7.6s) loss=3.0785\n",
      "1131 (7.6s) loss=3.0805\n",
      "1132 (7.6s) loss=3.0828\n",
      "1133 (7.6s) loss=3.0875\n",
      "1134 (7.6s) loss=3.0824\n",
      "1135 (7.6s) loss=3.0799\n",
      "1136 (7.6s) loss=3.0887\n",
      "1137 (7.6s) loss=3.0822\n",
      "1138 (7.6s) loss=3.0801\n",
      "1139 (7.6s) loss=3.0791\n",
      "1140 (7.6s) loss=3.0828\n",
      "1141 (7.6s) loss=3.0775\n",
      "1142 (7.6s) loss=3.0766\n",
      "1143 (7.6s) loss=3.0777\n",
      "1144 (7.6s) loss=3.0779\n",
      "1145 (7.6s) loss=3.0732\n",
      "1146 (7.6s) loss=3.0760\n",
      "1147 (7.6s) loss=3.0789\n",
      "1148 (7.6s) loss=3.0785\n",
      "1149 (7.6s) loss=3.0791\n",
      "'azwccknbsrw-nnj'\n",
      "'bhgqlddvndqiwxusxitsdnlnw'\n",
      "'crndnknxqvjqueagqohbqxwrd'\n",
      "'ditnitmedhvkdlfrxcqljnupo'\n",
      "'enerwlydbqvrozsjhx'\n",
      "'fnjpjtwdlseiptf'\n",
      "'gunn'\n",
      "'hzdmrbayknftswyfczimenlku'\n",
      "'iso-c-iszaxsxtziy'\n",
      "'jrnchkedbkjdnornzkngpgb-q'\n",
      "'kqseee'\n",
      "'lovn'\n",
      "'m'\n",
      "'nkwavlnzpnzowkoqlohozdyhq'\n",
      "'o-kbplnrvxfvdagggp-nhpclm'\n",
      "'pzdtxtb'\n",
      "'qv'\n",
      "'rpeud'\n",
      "'skyoktnquvvxqq-styknmgbjp'\n",
      "'tygwhhjatndzinyp'\n",
      "'uof-b-riunnrmifmxoiitccit'\n",
      "'vjdejnznh'\n",
      "'wwavl-yzcsxsjeaqnzg'\n",
      "'xzuhnfnhdlzuvzutizc'\n",
      "'y'\n",
      "'zxhzuylnv'\n",
      "1150 (7.6s) loss=3.0812\n",
      "1151 (7.5s) loss=3.0791\n",
      "1152 (7.6s) loss=3.0896\n",
      "1153 (7.6s) loss=3.0799\n",
      "1154 (7.6s) loss=3.0742\n",
      "1155 (7.6s) loss=3.0740\n",
      "1156 (7.6s) loss=3.0756\n",
      "1157 (7.6s) loss=3.0912\n",
      "1158 (7.6s) loss=3.0771\n",
      "1159 (7.6s) loss=3.0764\n",
      "1160 (7.6s) loss=3.0777\n",
      "1161 (7.6s) loss=3.0746\n",
      "1162 (7.6s) loss=3.0859\n",
      "1163 (7.6s) loss=3.0766\n",
      "1164 (7.6s) loss=3.0736\n",
      "1165 (7.6s) loss=3.0754\n",
      "1166 (7.6s) loss=3.0730\n",
      "1167 (7.6s) loss=3.0709\n",
      "1168 (7.6s) loss=3.0742\n",
      "1169 (7.6s) loss=3.0832\n",
      "1170 (7.6s) loss=3.0754\n",
      "1171 (7.6s) loss=3.0750\n",
      "1172 (7.6s) loss=3.0721\n",
      "1173 (7.6s) loss=3.0756\n",
      "1174 (7.6s) loss=3.0707\n",
      "1175 (7.6s) loss=3.0734\n",
      "1176 (7.6s) loss=3.0752\n",
      "1177 (7.6s) loss=3.0758\n",
      "1178 (7.6s) loss=3.0826\n",
      "1179 (7.6s) loss=3.0760\n",
      "1180 (7.6s) loss=3.0760\n",
      "1181 (7.6s) loss=3.0822\n",
      "1182 (7.6s) loss=3.0719\n",
      "1183 (7.6s) loss=3.0729\n",
      "1184 (7.6s) loss=3.0744\n",
      "1185 (7.6s) loss=3.0750\n",
      "1186 (7.7s) loss=3.0748\n",
      "1187 (7.6s) loss=3.0717\n",
      "1188 (7.6s) loss=3.0766\n",
      "1189 (7.6s) loss=3.0736\n",
      "1190 (7.6s) loss=3.0736\n",
      "1191 (7.6s) loss=3.0781\n",
      "1192 (7.6s) loss=3.0740\n",
      "1193 (7.6s) loss=3.0738\n",
      "1194 (7.6s) loss=3.0715\n",
      "1195 (7.6s) loss=3.0738\n",
      "1196 (7.6s) loss=3.0736\n",
      "1197 (7.6s) loss=3.0711\n",
      "1198 (7.6s) loss=3.0697\n",
      "1199 (7.6s) loss=3.0695\n",
      "'aacnszkmnn'\n",
      "'bbbhkwgbz'\n",
      "'cevgqyrddeofuraclz'\n",
      "'dnn-icoxevln-tnowwtzxuz-r'\n",
      "'ercytstrnytzfhipjszluitrj'\n",
      "'fconlc-mognw'\n",
      "'gvqthhnknipnmhwz'\n",
      "'hrtcbjtnnnmgnftc--rb-vm'\n",
      "'ixryxgufglmsic-liyzl'\n",
      "'jkxcfnnnpf'\n",
      "'krbnv-hnnnpzr'\n",
      "'lzggchqhtpoyvzy'\n",
      "'mtngbj'\n",
      "'nzrcyftqvis'\n",
      "'o-x-nwj-ksaupmql'\n",
      "'pnokzfl'\n",
      "'qh'\n",
      "'rhjkc-onr-zsql'\n",
      "'sdwuukjnvnt'\n",
      "'tscxknmcxlqexonzkynyrvijn'\n",
      "'ufyziuavzsowd-lxzifxn-l'\n",
      "'vhjupwaiikfs-'\n",
      "'wwarikxs'\n",
      "'xztqsunhokzuwtiuq-gjxa'\n",
      "'ybqp'\n",
      "'zvyutpfwdw'\n",
      "1200 (7.5s) loss=3.0703\n",
      "1201 (7.5s) loss=3.0738\n",
      "1202 (7.6s) loss=3.0699\n",
      "1203 (7.6s) loss=3.0756\n",
      "1204 (7.6s) loss=3.0695\n",
      "1205 (7.6s) loss=3.0719\n",
      "1206 (7.6s) loss=3.0824\n",
      "1207 (7.6s) loss=3.0771\n",
      "1208 (7.6s) loss=3.0836\n",
      "1209 (7.6s) loss=3.0805\n",
      "1210 (7.6s) loss=3.0703\n",
      "1211 (7.6s) loss=3.0725\n",
      "1212 (7.6s) loss=3.0781\n",
      "1213 (7.6s) loss=3.0699\n",
      "1214 (7.6s) loss=3.0695\n",
      "1215 (7.6s) loss=3.0717\n",
      "1216 (7.6s) loss=3.0705\n",
      "1217 (7.6s) loss=3.0783\n",
      "1218 (7.6s) loss=3.0781\n",
      "1219 (7.6s) loss=3.0891\n",
      "1220 (7.6s) loss=3.0713\n",
      "1221 (7.6s) loss=3.0701\n",
      "1222 (7.6s) loss=3.0691\n",
      "1223 (7.6s) loss=3.0738\n",
      "1224 (7.6s) loss=3.0715\n",
      "1225 (7.6s) loss=3.0779\n",
      "1226 (7.6s) loss=3.0664\n",
      "1227 (7.6s) loss=3.0699\n",
      "1228 (7.6s) loss=3.0721\n",
      "1229 (7.6s) loss=3.0799\n",
      "1230 (7.6s) loss=3.0766\n",
      "1231 (7.6s) loss=3.0717\n",
      "1232 (7.6s) loss=3.0709\n",
      "1233 (7.6s) loss=3.0695\n",
      "1234 (7.6s) loss=3.0703\n",
      "1235 (7.6s) loss=3.0713\n",
      "1236 (7.6s) loss=3.0758\n",
      "1237 (7.6s) loss=3.0814\n",
      "1238 (7.6s) loss=3.0719\n",
      "1239 (7.6s) loss=3.0707\n",
      "1240 (7.6s) loss=3.0666\n",
      "1241 (7.6s) loss=3.0678\n",
      "1242 (7.6s) loss=3.0689\n",
      "1243 (7.6s) loss=3.0734\n",
      "1244 (7.6s) loss=3.0656\n",
      "1245 (7.6s) loss=3.0738\n",
      "1246 (7.6s) loss=3.0740\n",
      "1247 (7.6s) loss=3.0727\n",
      "1248 (7.6s) loss=3.0688\n",
      "1249 (7.6s) loss=3.0695\n",
      "'aosrv'\n",
      "'bvsgnbdatj'\n",
      "'coylejbqfcmnannafgtkwcgu-'\n",
      "'demfzdg'\n",
      "'e-snrrgcdgnvud'\n",
      "'flldnbfrkutaqjnhllvsbsnh'\n",
      "'gxkxnj'\n",
      "'hgvjdmc'\n",
      "'itemnxtkhcze'\n",
      "'jspa'\n",
      "'kui'\n",
      "'lfn'\n",
      "'mhyaopqmecxlnzzosimheaqsk'\n",
      "'nszjio'\n",
      "'oqitwrgpurahebrttnmmhzr'\n",
      "'pfuemknlcnfpfzp'\n",
      "'qpqtcnzhs---'\n",
      "'rsn'\n",
      "'svkkafstmvyv'\n",
      "'trshdkchaqgunbnyjhftvgpsm'\n",
      "'uthaozvkdpwdagiohosfxzy'\n",
      "'vbjdbsbafmra'\n",
      "'wopexxrc-'\n",
      "'xbgql-akkg'\n",
      "'yykalc'\n",
      "'z'\n",
      "1250 (7.6s) loss=3.0873\n",
      "1251 (7.6s) loss=3.0740\n",
      "1252 (7.6s) loss=3.0693\n",
      "1253 (7.6s) loss=3.0684\n",
      "1254 (7.6s) loss=3.0674\n",
      "1255 (7.6s) loss=3.0682\n",
      "1256 (7.6s) loss=3.0658\n",
      "1257 (7.6s) loss=3.0691\n",
      "1258 (7.6s) loss=3.0688\n",
      "1259 (7.6s) loss=3.0742\n",
      "1260 (7.6s) loss=3.0686\n",
      "1261 (7.6s) loss=3.0664\n",
      "1262 (7.6s) loss=3.0666\n",
      "1263 (7.6s) loss=3.0682\n",
      "1264 (7.6s) loss=3.0791\n",
      "1265 (7.6s) loss=3.0754\n",
      "1266 (7.6s) loss=3.0705\n",
      "1267 (7.6s) loss=3.0648\n",
      "1268 (7.6s) loss=3.0697\n",
      "1269 (7.6s) loss=3.0646\n",
      "1270 (7.6s) loss=3.0650\n",
      "1271 (7.6s) loss=3.0887\n",
      "1272 (7.6s) loss=3.0713\n",
      "1273 (7.6s) loss=3.0719\n",
      "1274 (7.6s) loss=3.0672\n",
      "1275 (7.6s) loss=3.0639\n",
      "1276 (7.6s) loss=3.0689\n",
      "1277 (7.6s) loss=3.0658\n",
      "1278 (7.6s) loss=3.0758\n",
      "1279 (7.6s) loss=3.0781\n",
      "1280 (7.6s) loss=3.0682\n",
      "1281 (7.6s) loss=3.0713\n",
      "1282 (7.6s) loss=3.0701\n",
      "1283 (7.6s) loss=3.0664\n",
      "1284 (7.6s) loss=3.0666\n",
      "1285 (7.6s) loss=3.0674\n",
      "1286 (7.6s) loss=3.0617\n",
      "1287 (7.6s) loss=3.0656\n",
      "1288 (7.6s) loss=3.0672\n",
      "1289 (7.6s) loss=3.0672\n",
      "1290 (7.6s) loss=3.0672\n",
      "1291 (7.6s) loss=3.0684\n",
      "1292 (7.6s) loss=3.0814\n",
      "1293 (7.6s) loss=3.0723\n",
      "1294 (7.6s) loss=3.0643\n",
      "1295 (7.6s) loss=3.0633\n",
      "1296 (7.6s) loss=3.0658\n",
      "1297 (7.6s) loss=3.0658\n",
      "1298 (7.6s) loss=3.0658\n",
      "1299 (7.6s) loss=3.0686\n",
      "'aofzcjyjkki-'\n",
      "'bbid'\n",
      "'cqenuxcxpmvgfvjrn'\n",
      "'dsoznks'\n",
      "'e-ogrufxys-fttfu'\n",
      "'f'\n",
      "'gfssnfgtbnijhsbhqffu--yi'\n",
      "'htny'\n",
      "'ikti'\n",
      "'jxjexnsavrdcaultorb'\n",
      "'kk-yppjzrroj'\n",
      "'ld-wny'\n",
      "'mwnfj-ac-t-yanndvvenlenib'\n",
      "'nnamp-kfr'\n",
      "'obf'\n",
      "'p'\n",
      "'q-iooncntcvxwfrnqgvoy-ftc'\n",
      "'r'\n",
      "'scmeojhku'\n",
      "'tmi-rmd'\n",
      "'u-veadzpcemaaahxnnnildjzn'\n",
      "'vcjn-xkn'\n",
      "'wuanjaminsnkisqwhlywegach'\n",
      "'x'\n",
      "'yjchjkllfpwnqqpjra'\n",
      "'zofsonhd-nocexzypghgkedjg'\n",
      "1300 (7.6s) loss=3.0641\n",
      "1301 (7.6s) loss=3.0621\n",
      "1302 (7.6s) loss=3.0617\n",
      "1303 (7.6s) loss=3.0631\n",
      "1304 (7.6s) loss=3.0666\n",
      "1305 (7.6s) loss=3.0674\n",
      "1306 (7.6s) loss=3.0602\n",
      "1307 (7.6s) loss=3.0641\n",
      "1308 (7.6s) loss=3.0682\n",
      "1309 (7.6s) loss=3.0631\n",
      "1310 (7.6s) loss=3.0648\n",
      "1311 (7.6s) loss=3.0656\n",
      "1312 (7.6s) loss=3.0611\n",
      "1313 (7.6s) loss=3.0658\n",
      "1314 (7.6s) loss=3.0625\n",
      "1315 (7.6s) loss=3.0633\n",
      "1316 (7.6s) loss=3.0637\n",
      "1317 (7.6s) loss=3.0666\n",
      "1318 (7.6s) loss=3.0688\n",
      "1319 (7.6s) loss=3.0664\n",
      "1320 (7.6s) loss=3.0629\n",
      "1321 (7.6s) loss=3.0633\n",
      "1322 (7.6s) loss=3.0637\n",
      "1323 (7.6s) loss=3.0641\n",
      "1324 (7.6s) loss=3.0633\n",
      "1325 (7.6s) loss=3.0627\n",
      "1326 (7.6s) loss=3.0602\n",
      "1327 (7.6s) loss=3.0635\n",
      "1328 (7.6s) loss=3.0635\n",
      "1329 (7.6s) loss=3.0602\n",
      "1330 (7.6s) loss=3.0582\n",
      "1331 (7.6s) loss=3.0594\n",
      "1332 (7.6s) loss=3.0629\n",
      "1333 (7.6s) loss=3.0582\n",
      "1334 (7.6s) loss=3.0635\n",
      "1335 (7.6s) loss=3.0637\n",
      "1336 (7.6s) loss=3.0623\n",
      "1337 (7.6s) loss=3.0625\n",
      "1338 (7.6s) loss=3.0854\n",
      "1339 (7.6s) loss=3.0645\n",
      "1340 (7.6s) loss=3.0613\n",
      "1341 (7.6s) loss=3.0646\n",
      "1342 (7.6s) loss=3.0623\n",
      "1343 (7.6s) loss=3.0637\n",
      "1344 (7.6s) loss=3.0627\n",
      "1345 (7.6s) loss=3.0631\n",
      "1346 (7.6s) loss=3.0742\n",
      "1347 (7.6s) loss=3.0729\n",
      "1348 (7.6s) loss=3.0662\n",
      "1349 (7.6s) loss=3.0621\n",
      "'aawhc'\n",
      "'bab'\n",
      "'cnxhe'\n",
      "'d'\n",
      "'evwd'\n",
      "'fagzgpetyi-zpx-p-nooiowvy'\n",
      "'gymx'\n",
      "'hwwkri'\n",
      "'irvrdtqr'\n",
      "'jcfvnwklrhgnhg-ffkolulgnz'\n",
      "'koy'\n",
      "'lmibeqemgzmxifmnebvhcjllf'\n",
      "'mtfveldirpfpekvnsz'\n",
      "'nedozyclyqnllfrmkzo'\n",
      "'ogaukfrls-'\n",
      "'pzbarvkdvjf-cwtqpb'\n",
      "'qwguqgmganlhrebskjtuntzsg'\n",
      "'rltpgxlftb'\n",
      "'sxji'\n",
      "'tqn'\n",
      "'urihxnizofvuyhhnljkhqhd'\n",
      "'vpknqtxd-sbmnnvptyjwkuzxd'\n",
      "'wthjkxjcpetbkm-vijqy'\n",
      "'xmjetbncuebzxooeyeomg-p'\n",
      "'yujamgwhgdwmo-xux-iqyv'\n",
      "'z'\n",
      "1350 (7.5s) loss=3.0641\n",
      "1351 (7.6s) loss=3.0621\n",
      "1352 (7.6s) loss=3.0648\n",
      "1353 (7.6s) loss=3.0617\n",
      "1354 (7.6s) loss=3.0605\n",
      "1355 (7.6s) loss=3.0592\n",
      "1356 (7.7s) loss=3.0604\n",
      "1357 (7.6s) loss=3.0600\n",
      "1358 (7.6s) loss=3.0672\n",
      "1359 (7.6s) loss=3.0658\n",
      "1360 (7.6s) loss=3.0598\n",
      "1361 (7.6s) loss=3.0609\n",
      "1362 (7.6s) loss=3.0590\n",
      "1363 (7.6s) loss=3.0611\n",
      "1364 (7.6s) loss=3.0635\n",
      "1365 (7.6s) loss=3.0668\n",
      "1366 (7.6s) loss=3.0586\n",
      "1367 (7.6s) loss=3.0564\n",
      "1368 (7.6s) loss=3.0600\n",
      "1369 (7.6s) loss=3.0621\n",
      "1370 (7.6s) loss=3.0617\n",
      "1371 (7.6s) loss=3.0639\n",
      "1372 (7.6s) loss=3.0631\n",
      "1373 (7.6s) loss=3.0617\n",
      "1374 (7.6s) loss=3.0721\n",
      "1375 (7.6s) loss=3.0682\n",
      "1376 (7.6s) loss=3.0590\n",
      "1377 (7.6s) loss=3.0590\n",
      "1378 (7.6s) loss=3.0604\n",
      "1379 (7.6s) loss=3.0615\n",
      "1380 (7.6s) loss=3.0568\n",
      "1381 (7.6s) loss=3.0594\n",
      "1382 (7.6s) loss=3.0568\n",
      "1383 (7.6s) loss=3.0602\n",
      "1384 (7.6s) loss=3.0613\n",
      "1385 (7.6s) loss=3.0627\n",
      "1386 (7.6s) loss=3.0576\n",
      "1387 (7.6s) loss=3.0562\n",
      "1388 (7.6s) loss=3.0607\n",
      "1389 (7.6s) loss=3.0570\n",
      "1390 (7.6s) loss=3.0609\n",
      "1391 (7.6s) loss=3.0576\n",
      "1392 (7.6s) loss=3.0617\n",
      "1393 (7.6s) loss=3.0678\n",
      "1394 (7.6s) loss=3.0607\n",
      "1395 (7.6s) loss=3.0588\n",
      "1396 (7.6s) loss=3.0539\n",
      "1397 (7.6s) loss=3.0562\n",
      "1398 (7.6s) loss=3.0561\n",
      "1399 (7.6s) loss=3.0584\n",
      "'aznfldoqdexpwkxhwzmcfeluf'\n",
      "'bvhpkubhwbimufqxtpjyiq-gm'\n",
      "'ceheivvfkatkdkfty-w'\n",
      "'dihsv-'\n",
      "'etiqrvuhjwjodibot'\n",
      "'fwebghphofimic-hhyst-jrpe'\n",
      "'gtncusaozpoeeqjyvojkdbkmi'\n",
      "'htnust'\n",
      "'iyw'\n",
      "'jpcbjfpmtuhcrmnxafhnstjxo'\n",
      "'kvbkbcyvc'\n",
      "'lnjeh-godvvqpo--xsvvaxaxw'\n",
      "'mpnnznv'\n",
      "'n'\n",
      "'owy'\n",
      "'puplnqmnzo'\n",
      "'qfxnuxnanfzmtneu'\n",
      "'rnsvbikpeperqwckhfnrzpazn'\n",
      "'skhltdf'\n",
      "'tbqovjrxnodtajnuyvmfgftiz'\n",
      "'ulnnolnoptdzcfjifrziydzqy'\n",
      "'v'\n",
      "'wblvby-cfd'\n",
      "'xkfzttecifpit'\n",
      "'yyvgmngyd'\n",
      "'zjcdfbjuoept'\n",
      "1400 (7.5s) loss=3.0590\n",
      "1401 (7.5s) loss=3.0668\n",
      "1402 (7.6s) loss=3.0590\n",
      "1403 (7.6s) loss=3.0562\n",
      "1404 (7.6s) loss=3.0578\n",
      "1405 (7.6s) loss=3.0580\n",
      "1406 (7.6s) loss=3.0586\n",
      "1407 (7.6s) loss=3.0574\n",
      "1408 (7.6s) loss=3.0609\n",
      "1409 (7.6s) loss=3.0562\n",
      "1410 (7.6s) loss=3.0562\n",
      "1411 (7.6s) loss=3.0574\n",
      "1412 (7.6s) loss=3.0594\n",
      "1413 (7.6s) loss=3.0609\n",
      "1414 (7.6s) loss=3.0600\n",
      "1415 (7.6s) loss=3.0576\n",
      "1416 (7.6s) loss=3.0607\n",
      "1417 (7.6s) loss=3.0619\n",
      "1418 (7.6s) loss=3.0566\n",
      "1419 (7.6s) loss=3.0586\n",
      "1420 (7.6s) loss=3.0568\n",
      "1421 (7.6s) loss=3.0629\n",
      "1422 (7.6s) loss=3.0764\n",
      "1423 (7.6s) loss=3.0691\n",
      "1424 (7.6s) loss=3.0670\n",
      "1425 (7.6s) loss=3.0619\n",
      "1426 (7.6s) loss=3.0619\n",
      "1427 (7.6s) loss=3.0584\n",
      "1428 (7.6s) loss=3.0572\n",
      "1429 (7.6s) loss=3.0557\n",
      "1430 (7.6s) loss=3.0570\n",
      "1431 (7.6s) loss=3.0602\n",
      "1432 (7.6s) loss=3.0580\n",
      "1433 (7.6s) loss=3.0547\n",
      "1434 (7.6s) loss=3.0580\n",
      "1435 (7.6s) loss=3.0574\n",
      "1436 (7.6s) loss=3.0578\n",
      "1437 (7.6s) loss=3.0566\n",
      "1438 (7.6s) loss=3.0557\n",
      "1439 (7.6s) loss=3.0578\n",
      "1440 (7.6s) loss=3.0637\n",
      "1441 (7.6s) loss=3.0535\n",
      "1442 (7.6s) loss=3.0600\n",
      "1443 (7.6s) loss=3.0576\n",
      "1444 (7.6s) loss=3.0570\n",
      "1445 (7.6s) loss=3.0545\n",
      "1446 (7.6s) loss=3.0533\n",
      "1447 (7.6s) loss=3.0559\n",
      "1448 (7.6s) loss=3.0596\n",
      "1449 (7.6s) loss=3.0658\n",
      "'asbogbzxyduwqmlkwp-b'\n",
      "'bkgr'\n",
      "'cjurykg'\n",
      "'dwqnmtnzzntp'\n",
      "'eus-xvbgduavummje-tjqxvco'\n",
      "'fxs-dryilxtikkqtfjz-vuejn'\n",
      "'gzsmo-zbhlibaxvrdsiqlzhnp'\n",
      "'hirsv'\n",
      "'i-uhd'\n",
      "'jzm'\n",
      "'kv'\n",
      "'l'\n",
      "'mcylwnndiksceqgrn-fgnkzzv'\n",
      "'nonmjzmycxl'\n",
      "'owfmsuveekvjxkbuq-jn'\n",
      "'p-vyknnaaca-gdh'\n",
      "'qsjgh'\n",
      "'rcsvffqhwbxayqkpqkv'\n",
      "'sqnnqcbd-wzeqooeuiuonrhbn'\n",
      "'tzf'\n",
      "'ucnwbv'\n",
      "'vx-vv'\n",
      "'wtmamv'\n",
      "'xpmqnye'\n",
      "'yegf'\n",
      "'zfh-m'\n",
      "1450 (7.6s) loss=3.0727\n",
      "1451 (7.6s) loss=3.0572\n",
      "1452 (7.6s) loss=3.0576\n",
      "1453 (7.6s) loss=3.0568\n",
      "1454 (7.6s) loss=3.0557\n",
      "1455 (7.6s) loss=3.0562\n",
      "1456 (7.6s) loss=3.0561\n",
      "1457 (7.6s) loss=3.0570\n",
      "1458 (7.6s) loss=3.0568\n",
      "1459 (7.6s) loss=3.0543\n",
      "1460 (7.6s) loss=3.0545\n",
      "1461 (7.6s) loss=3.0580\n",
      "1462 (7.6s) loss=3.0605\n",
      "1463 (7.6s) loss=3.0582\n",
      "1464 (7.6s) loss=3.0537\n",
      "1465 (7.6s) loss=3.0529\n",
      "1466 (7.6s) loss=3.0553\n",
      "1467 (7.6s) loss=3.0566\n",
      "1468 (7.6s) loss=3.0561\n",
      "1469 (7.6s) loss=3.0572\n",
      "1470 (7.6s) loss=3.0562\n",
      "1471 (7.6s) loss=3.0518\n",
      "1472 (7.6s) loss=3.0502\n",
      "1473 (7.6s) loss=3.0547\n",
      "1474 (7.6s) loss=3.0547\n",
      "1475 (7.6s) loss=3.0535\n",
      "1476 (7.6s) loss=3.0537\n",
      "1477 (7.6s) loss=3.0539\n",
      "1478 (7.6s) loss=3.0527\n",
      "1479 (7.6s) loss=3.0504\n",
      "1480 (7.6s) loss=3.0529\n",
      "1481 (7.6s) loss=3.0539\n",
      "1482 (7.6s) loss=3.0590\n",
      "1483 (7.6s) loss=3.0531\n",
      "1484 (7.6s) loss=3.0545\n",
      "1485 (7.6s) loss=3.0555\n",
      "1486 (7.6s) loss=3.0518\n",
      "1487 (7.7s) loss=3.0512\n",
      "1488 (7.6s) loss=3.0541\n",
      "1489 (7.6s) loss=3.0547\n",
      "1490 (7.6s) loss=3.0584\n",
      "1491 (7.6s) loss=3.0523\n",
      "1492 (7.6s) loss=3.0514\n",
      "1493 (7.6s) loss=3.0547\n",
      "1494 (7.6s) loss=3.0537\n",
      "1495 (7.6s) loss=3.0518\n",
      "1496 (7.6s) loss=3.0584\n",
      "1497 (7.6s) loss=3.0506\n",
      "1498 (7.6s) loss=3.0502\n",
      "1499 (7.6s) loss=3.0479\n",
      "'at-hyqynnnyzuh'\n",
      "'bdjjxxsixfzfstkfwgxggiczb'\n",
      "'csm-waetqpfucdnfnbtkunpmx'\n",
      "'dm'\n",
      "'eisfvcpguhypinq'\n",
      "'f'\n",
      "'gjlqmbbkqflxvndkkinepdnhf'\n",
      "'hupyk'\n",
      "'icvtmjzx-vuqm-c'\n",
      "'jxvlcofday'\n",
      "'kaxqblvqujezan'\n",
      "'lowansmg'\n",
      "'mazvko-jwzyargatczqnfxt-i'\n",
      "'nfsbuymnxayvobhqbyecfqrck'\n",
      "'ofvyanevljvzrfenajqnbcudn'\n",
      "'pob'\n",
      "'qqlngtegetxeqdztad-vrynwn'\n",
      "'rcgpnvmmwkspglipzgonrofnm'\n",
      "'swuhsnj'\n",
      "'tej'\n",
      "'uyzdos-tpser'\n",
      "'vgheyfhcw'\n",
      "'wbnxnyntn'\n",
      "'xj'\n",
      "'yoprg'\n",
      "'zpknnnwekebndk'\n"
     ]
    }
   ],
   "source": [
    "for _ in range(conf['total_training_loops']):\n",
    "    alphabet_sample()\n",
    "    epoch = train(conf['epochs_per_loop'], epoch)\n",
    "alphabet_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_with_prompt('New')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = model.eval()\n",
    "encoder_out = tensor(pad_indices(get_indices('N'))).to(device)\n",
    "decoder_in = tensor(pad_indices(get_indices(''), right_shift=True)).to(device)\n",
    "decoder_out = eval_model(encoder_out=encoder_out, decoder_in=decoder_in)\n",
    "_, indices = torch.max(nn.functional.softmax(decoder_out, dim=1), dim=1)\n",
    "indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
