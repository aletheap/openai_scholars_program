{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Transformer\n",
    "\n",
    "I wrote this to help me understand _Attention Is All You Need_: https://arxiv.org/abs/1706.03762\n",
    "\n",
    "I cut out the Encoder and I'm using it to generate English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import multiprocessing.pool\n",
    "from math import sqrt, sin, cos, floor\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torchtext\n",
    "import wandb\n",
    "from torchtext.data import RawField, ReversibleField, LabelField\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Config\n",
    "conf = {\n",
    "        'attn_heads': 8,\n",
    "        'training_bptt_len': 180,\n",
    "        #'cuda_device_ids': [3, 2, 1, 0],  # I need better GPU coolng first\n",
    "        'cuda_device_ids': [0],\n",
    "        'd_model': 512,\n",
    "        #'datafile': './city_names.txt', # from: https://www.britannica.com/topic/list-of-cities-and-towns-in-the-United-States-2023068\n",
    "        #'datafile': './corncob_lowercase.txt',  # from: http://www.mieliestronk.com/corncob_lowercase.txt\n",
    "        #'datafile': './alphabet_short.txt',  \n",
    "        #'datafile': './dummy_data.txt', \n",
    "        #'dataset': 'WikiText2',\n",
    "        'dataset': 'WikiText103',\n",
    "        'dropout': 0.1,\n",
    "        'initial_learning_rate': 1,\n",
    "        'total_epochs': 4,\n",
    "        'num_blocks_encoder': 0,\n",
    "        'num_blocks_decoder': 6,\n",
    "        'max_bptt_len': 1024,\n",
    "        'max_vocab_size': 60000,\n",
    "        #'minibatch_size': 32 * 16,\n",
    "        'minibatch_size': 16,\n",
    "        #'optimizer': 'Adam',  \n",
    "        #'optimizer': 'SGD',\n",
    "        'random_seed': 0,\n",
    "        'warmup_steps': 2500,\n",
    "        }\n",
    "\n",
    "\n",
    "# debugging\n",
    "#conf['attn_heads'] = 1\n",
    "#conf['d_model'] = 1\n",
    "#conf['bptt_len'] = 2\n",
    "#conf['datafile'] = './dummy_data.txt'  \n",
    "#conf['num_blocks_decoder'] = 1\n",
    "#conf['minibatch_size'] = 1\n",
    "#conf['total_epochs'] = 1\n",
    "\n",
    "\n",
    "# Make sure d_model, heads, and d_key are compatible\n",
    "assert conf['d_model'] % conf['attn_heads'] == 0, \\\n",
    "    f'attn_heads=%s does not evenly divide d_model=%s' % (conf['attn_heads'], \n",
    "                                                         conf['d_model'])\n",
    "conf['d_key'] = conf['d_model'] / conf['attn_heads']\n",
    "\n",
    "# Set up the RNGs for repeatability\n",
    "if conf['random_seed']:\n",
    "    torch.manual_seed(conf['random_seed'])\n",
    "    torch.cuda.manual_seed(conf['random_seed'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(conf['random_seed'])\n",
    "    \n",
    "    \n",
    "# Set up Cuda\n",
    "print(\"Using\", len(conf['cuda_device_ids']), \"GPU(s):\")\n",
    "for i in conf['cuda_device_ids']:\n",
    "    print(\"    cuda:%s:\" % i, torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device('cuda:' + str(conf['cuda_device_ids'][0]))\n",
    "\n",
    "print()\n",
    "\n",
    "# Logging\n",
    "experiment = None\n",
    "experiment = wandb.init(project=\"my-transformer\", config=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.1: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Implements section 3.2.1\"\"\"\n",
    "    def __init__(self, d_model=conf['d_model'], d_key=conf['d_key']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_key = d_key\n",
    "\n",
    "        # head projections\n",
    "        self.Wq = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_key, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_key, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        # project queries, keys, values\n",
    "        queries = self.Wq(queries)\n",
    "        keys = self.Wk(keys)\n",
    "        values = self.Wv(values)\n",
    "\n",
    "        # calculate compatibility function\n",
    "        scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "        scores = scores / sqrt(self.d_key)\n",
    "\n",
    "        # Filter out attention to future positions\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, float('-inf'))\n",
    "            \n",
    "        # softmax\n",
    "        scores = self.softmax(scores)\n",
    "        \n",
    "        # sum the weighted value vectors\n",
    "        attn = torch.matmul(scores, values)  # shape = (bptt_len, d_key)\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Section 3.2.2\"\n",
    "    def __init__(self, d_model=conf['d_model'], heads=conf['attn_heads']):\n",
    "        super().__init__()\n",
    "        d_key = int(d_model / heads)\n",
    "\n",
    "        attn_heads = [AttentionHead(d_model=d_model, d_key=d_key) for _ in range(heads)]\n",
    "        self.attn_heads = nn.ModuleList(attn_heads)\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False)\n",
    "                    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        head_attns = [h(queries=queries, keys=keys, values=values, mask=mask) \n",
    "                      for h in self.attn_heads]\n",
    "        head_attn = torch.cat(head_attns, dim=-1)\n",
    "        ret = self.Wo(head_attn)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"Section 3.3\"\n",
    "    def __init__(self, d_model=conf['d_model'], multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        d_ff = int(multiplier * d_model)\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_ff, bias=False), \n",
    "                                 nn.ReLU(), \n",
    "                                 nn.Linear(d_ff, d_model, bias=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder and Decoder Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"Section 3.1, Encoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = MultiHeadAttention(d_model=d_model, heads=heads)\n",
    "        self.attn_drop = nn.Dropout(p=dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.attn(x, x, x)\n",
    "        a1 = self.attn_drop(a1)\n",
    "        a1 = self.attn_norm(x + a1) \n",
    "\n",
    "        a2 = self.ffn(a1)\n",
    "        a2 = self.ffn_drop(a2)\n",
    "        a2 = self.ffn_norm(a1 + a2)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"Section 3.1, Decoder\"\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 encoder_attention=True):\n",
    "        super().__init__()\n",
    "           \n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, heads=heads)\n",
    "        self.self_attn_drop = nn.Dropout(p=dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        if encoder_attention:\n",
    "            self.enc_attn = MultiHeadAttention(d_model=d_model, heads=heads)\n",
    "            self.enc_attn_drop = nn.Dropout(p=dropout)\n",
    "            self.enc_attn_norm = nn.LayerNorm(d_model)\n",
    "        else:\n",
    "            self.enc_attn = None\n",
    "            \n",
    "        self.ffn = FFN(d_model)\n",
    "        self.ffn_drop = nn.Dropout(p=dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_out=None, self_attn_mask=None):\n",
    "        a1 = self.self_attn(x, x, x, self_attn_mask)\n",
    "        a1 = self.self_attn_drop(a1)\n",
    "        a1 = x + a1  # residual\n",
    "        a1 = self.self_attn_norm(a1) \n",
    "        \n",
    "        if self.enc_attn is None:\n",
    "            a2 = a1\n",
    "        else:\n",
    "            a2 = self.enc_attn(a1, encoder_out, encoder_out)\n",
    "            a2 = self.enc_attn_drop(a2)\n",
    "            a2 = a1 + a2  # residual\n",
    "            a2 = self.enc_attn_norm(a2)\n",
    "\n",
    "        a3 = self.ffn(a2)\n",
    "        a3 = self.ffn_drop(a3)\n",
    "        a3 = a2 + a3  # residual\n",
    "        a3 = self.ffn_norm(a3)\n",
    "        \n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 num_blocks=conf['num_blocks_encoder'],\n",
    "                 dropout=conf['dropout']):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(d_model, heads, dropout) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, x):\n",
    "        a = x\n",
    "        for block in self.blocks:\n",
    "            a = block(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 num_blocks=conf['num_blocks_decoder'],\n",
    "                 dropout=conf['dropout'],\n",
    "                 encoder_attention=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(d_model, heads, dropout, encoder_attention) \n",
    "                                     for _ in range(num_blocks)])\n",
    "            \n",
    "    def forward(self, decoder_in, encoder_out=None, self_attn_mask=None):\n",
    "        a = decoder_in\n",
    "        for block in self.blocks:\n",
    "            a = block(a, encoder_out, self_attn_mask)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_len, \n",
    "                 pad_index,\n",
    "                 d_model=conf['d_model'], \n",
    "                 heads=conf['attn_heads'], \n",
    "                 max_bptt_len=conf['max_bptt_len'],\n",
    "                 num_blocks_encoder=conf['num_blocks_encoder'],\n",
    "                 num_blocks_decoder=conf['num_blocks_decoder'], \n",
    "                 dropout=conf['dropout'],\n",
    "                 encoder_decoder_attention=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_len = vocab_len\n",
    "        self.pad_index = pad_index\n",
    "        self.max_bptt_len = max_bptt_len\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "  \n",
    "        self.embedding = nn.Embedding(vocab_len, d_model, padding_idx=pad_index)\n",
    "        self.register_buffer('position_encoding', self._position_encoding(max_bptt_len, d_model))\n",
    "        self.register_buffer('self_attn_mask', self._make_mask(max_bptt_len))\n",
    "                                            \n",
    "        #self.encoder = Encoder(d_model, heads, num_blocks_encoder, dropout)\n",
    "        self.decoder = Decoder(d_model, heads, num_blocks_decoder, dropout, encoder_decoder_attention)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, vocab_len, bias=False)\n",
    "           \n",
    "\n",
    "    @classmethod\n",
    "    def _make_mask(cls, bptt_len):\n",
    "        return torch.ones([bptt_len, bptt_len]).tril()\n",
    "\n",
    "    @classmethod\n",
    "    def _position_encoding(cls, bptt_len, d_model):\n",
    "        rows = [tensor([sin(pos/(10000**(i/d_model))) \n",
    "                        if i % 2 == 0 \n",
    "                        else \n",
    "                        cos(pos/(10000**((i-1)/d_model))) \n",
    "                        for i in range(d_model)])\n",
    "                for pos in range(bptt_len)]\n",
    "        stack = torch.stack(rows, dim=1)\n",
    "        \n",
    "        return stack.T\n",
    "\n",
    "    def embed(self, indices):\n",
    "        \"\"\"Implements the embedding from Section 3.4 Embeddings and Softmax\"\"\"\n",
    "        this_bptt_len = indices.shape[-1]\n",
    "        pe = self.position_encoding[:this_bptt_len, :]\n",
    "\n",
    "        embedded = self.embedding(indices)\n",
    "        \n",
    "        #print('pe.shape:', pe.shape)\n",
    "        #print('embedded.shape:', embedded.shape)\n",
    "        return pe + embedded\n",
    "        \n",
    "    #def forward(self, encoder_in, encoder_out=None, decoder_in=[]):\n",
    "    def forward(self, decoder_in, encoder_out=None, pos=None, pre_embedded=False):\n",
    "        \"\"\"parameters:\n",
    "        encoder_in:  (rank-1 tensor) vocab indices of encoder input token \n",
    "                     sequence\n",
    "        encoder_out: (optional rank-1 tensor) passing this skips \n",
    "                     the encoder execution, and acts and if this were \n",
    "                     the indices the encoder produced.\n",
    "        decoder_in:  (optional rank-1 tensor) vocab indices of prior \n",
    "                     decoder output for auto-regression. Right \n",
    "                     shifted by one position.\"\"\"\n",
    "\n",
    "        this_bptt_len = decoder_in.shape[-1]\n",
    "\n",
    "        if encoder_out:\n",
    "            assert self.encoder_decoder_attention, \\\n",
    "            \"encoder_out passed to model created without encoder-decoder attention\"\n",
    "\n",
    "        # Embed\n",
    "        if pre_embedded:\n",
    "            #eo_embedded = encoder_out\n",
    "            di_embedded = decoder_in\n",
    "        else:\n",
    "            #eo_embedded = self.embed(encoder_out) \n",
    "            di_embedded = self.embed(decoder_in)\n",
    "        \n",
    "        # Encode\n",
    "        #encoded = self.encoder(self.embed(encoder_in))\n",
    "        #encoded = eo_embedded\n",
    "        \n",
    "        # Decode\n",
    "        self_attn_mask = self.self_attn_mask[:this_bptt_len, :this_bptt_len]\n",
    "        decoded = self.decoder(di_embedded, self_attn_mask=self_attn_mask)\n",
    "        \n",
    "        # Return predictions for next token\n",
    "        if pos is not None:\n",
    "            decoded = decoded[:, pos, :]\n",
    "        \n",
    "        y_pred = self.linear(decoded)\n",
    "                \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader and vocab\n",
    "#train_ds = load_dataset()\n",
    "def get_dataloader():\n",
    "    dataloader = getattr(torchtext.datasets, conf.get('dataset', None))\n",
    "    TEXT = torchtext.data.Field()\n",
    "    train, val, test = dataloader.splits(TEXT)\n",
    "    TEXT.build_vocab(train, max_size=conf['max_vocab_size'])\n",
    "    return torchtext.data.BPTTIterator.splits((train, val, test), \n",
    "                                              batch_size=conf['minibatch_size'], \n",
    "                                              bptt_len=conf['training_bptt_len'] * 2,\n",
    "                                              device=device)\n",
    "\n",
    "\n",
    "train_ds, val_ds, test_ds = get_dataloader()\n",
    "train_ds_len = float(len(list(iter(train_ds))))\n",
    "#train_ds, val_ds, test_ds = dataloader.iters(batch_size=conf['minibatch_size'], \n",
    "#                                             bptt_len=2 * conf['training_bptt_len'],\n",
    "#                                             device=device)\n",
    "vocab = train_ds.dataset.fields['text'].vocab\n",
    "vocab_len = len(vocab)\n",
    "pad_token = '_'\n",
    "pad_index = vocab.stoi[pad_token]\n",
    "print('There are', int(train_ds_len), 'training batches')\n",
    "print('vocab_len =', vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Transformer(vocab_len, pad_index, encoder_decoder_attention=False)\n",
    "model = model.to(device)\n",
    "num_params = sum([np.prod(p.shape) for p in model.parameters()])\n",
    "print(f\"Model has {num_params:,d} parameters\")\n",
    "#model = nn.DataParallel(model, device_ids=conf['cuda_device_ids'])\n",
    "\n",
    "# Define the Loss\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "\n",
    "def compute_jacobian(inputs, output):\n",
    "    \"\"\"\n",
    "    :param inputs: Batch X Size (e.g. Depth X Width X Height)\n",
    "    :param output: Batch X Classes\n",
    "    :return: jacobian: Batch X Classes X Size\n",
    "    \"\"\"\n",
    "    assert inputs.requires_grad\n",
    "\n",
    "    num_classes = output.size()[1]\n",
    "\n",
    "    jacobian = torch.zeros(num_classes, *inputs.size())\n",
    "    grad_output = torch.zeros(*output.size())\n",
    "    if inputs.is_cuda:\n",
    "        grad_output = grad_output.to(inputs.device)\n",
    "        jacobian = jacobian.to(inputs.device)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        zero_gradients(inputs)\n",
    "        grad_output.zero_()\n",
    "        grad_output[:, i, :] = 1\n",
    "        output.backward(grad_output, retain_graph=True)\n",
    "        jacobian[i] = inputs.grad.data\n",
    "\n",
    "    jacobian = jacobian.squeeze()\n",
    "    jacobian = torch.transpose(jacobian, dim0=0, dim1=1)\n",
    "\n",
    "    return jacobian\n",
    "\n",
    "training_bptt_len = conf['training_bptt_len']\n",
    "#model = model.train()\n",
    "#model = model.train()\n",
    "\n",
    "\n",
    "size = (conf['minibatch_size'], conf['training_bptt_len'], conf['d_model'])\n",
    "#print('size=', size)\n",
    "eo = torch.ones(size, requires_grad=True)\n",
    "di = torch.tensor([[[1],[2.]]], requires_grad=True)\n",
    "assert di.shape == size\n",
    "\n",
    "#output = model.decoder.blocks[0].self_attn(di, di, di)\n",
    "#output = model.decoder.blocks[0].self_attn.attn_heads[0](di, di, di)\n",
    "#queries = model.decoder.blocks[0].self_attn.attn_heads[0].Wq(di)\n",
    "#keys = model.decoder.blocks[0].self_attn.attn_heads[0].Wk(di)\n",
    "#values = model.decoder.blocks[0].self_attn.attn_heads[0].Wv(di)\n",
    "\n",
    "# calculate compatibility function\n",
    "#scores = torch.matmul(queries, torch.transpose(keys, -2, -1)) \n",
    "#scores = scores / sqrt(conf['d_key'])\n",
    "#scores = scores.tril()\n",
    "#attn = torch.matmul(scores, values)\n",
    "#output = attn\n",
    "\n",
    "#output = model.decoder.blocks[0].enc_attn(di, di, di)\n",
    "#decoded = model.decoder(eo_emb, di_emb)\n",
    "#y_pred = model.linear(decoded)\n",
    "#test = torch.tensor([[[1, 2.],[2., 3.]]], requires_grad=True)\n",
    "#model = model.eval()\n",
    "#output = model(eo, di)\n",
    "output = model(eo, di, pre_embedded=True)\n",
    "print('output:', output)\n",
    "\n",
    "#print('eo:', eo)\n",
    "#print('di:', di)\n",
    "#print('y_pred:', y_pred)\n",
    "#print('y_pred.shape:', y_pred.shape)\n",
    "\n",
    "j = compute_jacobian(di, output)\n",
    "print()\n",
    "print('squeezed jacobian:')\n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scheduler_lr(step, d_model=conf['d_model'], warmup_steps=conf['warmup_steps']):\n",
    "    \"\"\"return the learning rate multiplier for this step. This number gets multiplied by \n",
    "    conf['initial_learnign_rate'] inside the scheduler.\"\"\"\n",
    "    step = step + 1  # handle step 0\n",
    "    lr = (d_model**-.5) * min(step**-.5, step * (warmup_steps**-1.5))\n",
    "    return lr\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             betas=(0.9, 0.98), \n",
    "                             eps=1e-9, \n",
    "                             lr=conf['initial_learning_rate'])\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=_scheduler_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, expected_indices):\n",
    "    indices = torch.max(output, dim=-1)[1]\n",
    "    indices = indices.squeeze()\n",
    "    acc = (indices == expected_indices) / float(indices.numel())\n",
    "    acc = float(acc.sum()) * 100\n",
    "    return acc\n",
    "\n",
    "def run_minibatch(di, y, optimizer, scheduler, model):\n",
    "    \"\"\"Runs one minibatch training and returns the loss and accuracy for that minibatch\"\"\"\n",
    "    model = model.train()\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(decoder_in=di)\n",
    "    acc = accuracy(y_pred, y)\n",
    "    y_pred = y_pred.transpose(-2, -1)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()  # Not sure why, but this step logs a UserWarning\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    duration = time.time() - start_time\n",
    "    sys.stdout.write('.')\n",
    "    return loss.item(), acc, duration\n",
    "\n",
    "def validate(model):\n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        minibatches = 0\n",
    "        for batch in test_ds:\n",
    "            target = batch.target.T\n",
    "            y_pred = model(decoder_in=batch.text.T)\n",
    "            accuracies.append(accuracy(y_pred, target))\n",
    "            y_pred = y_pred.transpose(-2, -1)\n",
    "            losses.append(criterion(y_pred, target).item())\n",
    "            minibatches += 1\n",
    "            break\n",
    "    acc = tensor(accuracies, device=device).float().mean().item()\n",
    "    loss = tensor(losses, device=device).float().mean()\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return acc, perplexity\n",
    "\n",
    "def _log_status(learning_rate, epoch, epoch_step, epoch_secs, step_secs, step_loss, \n",
    "                step_train_accuracy, test_accuracy, test_perplexity,\n",
    "                steps_per_epoch=train_ds_len):\n",
    "\n",
    "    ts_format = '%I:%M%p'\n",
    "    now = time.time()\n",
    "    epoch_pct_done = 100.0 * epoch_step / steps_per_epoch\n",
    "    est_total_secs = epoch_secs / (epoch_pct_done/100)\n",
    "    est_end_time = now - epoch_secs + est_total_secs\n",
    "    now_ts = time.strftime(ts_format, time.localtime())\n",
    "    est_end_ts = time.strftime(ts_format, time.localtime(est_end_time))\n",
    "    cumulative_step = (epoch * steps_per_epoch) + epoch_step\n",
    "    print()\n",
    "    print(now_ts,\n",
    "          '(%.1f%%)' % epoch_pct_done,\n",
    "          '(eta:%s)' % est_end_ts,\n",
    "          'epoch:%s' % epoch,\n",
    "          'step:%s' % epoch_step,\n",
    "          #'(%.1fs)' % epoch_secs, \n",
    "          'lr:%.6f' % learning_rate,\n",
    "          'test_perplexity=%.1f' % test_perplexity,\n",
    "          'test_accuracy=%.1f%%' % test_accuracy,\n",
    "          'train_step_loss=%.4f' % step_loss, \n",
    "          'train_accuracy=%.1f%%' % step_train_accuracy, \n",
    "         )   \n",
    "\n",
    "    if experiment:\n",
    "        experiment.log({'learning_rate': learning_rate,\n",
    "                        'epoch': epoch,\n",
    "                        'epoch_step': epoch_step,\n",
    "                        'cumulative_step': cumulative_step,\n",
    "                        'epoch_secs': epoch_secs,\n",
    "                        'step_secs': step_secs,\n",
    "                        'step_loss': step_loss,\n",
    "                        'step_train_accuracy': step_train_accuracy,\n",
    "                        'test_accuracy': test_accuracy,\n",
    "                        'test_perplexity': test_perplexity,\n",
    "                        'epoch_pct_done': epoch_pct_done})\n",
    "    \n",
    "            \n",
    "def do_epoch(epoch, model, optimizer, scheduler, checkpoint_freq=500, steps_per_epoch=train_ds_len):\n",
    "    \"\"\"Runs one full training batch and returns the average loss,\n",
    "    accuracy, and duration time in seconds\"\"\"\n",
    "    last_epoch_step = steps_per_epoch - 1\n",
    "\n",
    "    epoch_start_time = None\n",
    "    for epoch_step, batch in enumerate(train_ds):\n",
    "        epoch_step += 1\n",
    "        if epoch_start_time is None:\n",
    "            epoch_start_time = time.time()\n",
    "        #print('eo.shape:', encoder_out.shape, 'di.shape', decoder_in.shape, 'y.shape:', y.shape)\n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        step_loss, step_train_accuracy, step_secs = run_minibatch(batch.text.T, \n",
    "                                                                  batch.target.T, \n",
    "                                                                  optimizer, \n",
    "                                                                  scheduler,\n",
    "                                                                  model) \n",
    "        if (epoch_step % checkpoint_freq == 1) or (epoch_step == last_epoch_step):\n",
    "            test_accuracy, test_perplexity = validate(model)\n",
    "            epoch_secs = time.time() - epoch_start_time\n",
    "            _log_status(learning_rate=learning_rate, \n",
    "                        epoch=epoch, \n",
    "                        epoch_step=epoch_step,\n",
    "                        epoch_secs=epoch_secs, \n",
    "                        step_secs=step_secs, \n",
    "                        step_loss=step_loss, \n",
    "                        step_train_accuracy=step_train_accuracy, \n",
    "                        test_accuracy=test_accuracy,\n",
    "                        test_perplexity=test_perplexity)\n",
    "            save_file_name = './my-transformer_%s_%s-layer_%s-vocab_%s-epoch_%s_step.pt' % \\\n",
    "                             (conf['dataset'], conf['num_blocks_decoder'], len(vocab), epoch-1, epoch_step)\n",
    "            if hasattr(model, 'module'):\n",
    "                torch.save(model.module.state_dict(), save_file_name)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), save_file_name)\n",
    "    return\n",
    "\n",
    "def train(num_epochs=conf['total_epochs'], model=model, checkpoint_frequency=500,\n",
    "          vocab=vocab, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "         start_epoch=0):\n",
    "    \"\"\"Runs num_epochs training batches and prints out results\"\"\"\n",
    "        \n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        do_epoch(epoch, model, optimizer, scheduler, checkpoint_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH=\"\"\"./my-transformer_WikiText103_6-layer_60002-vocab_-1-epoch_17921_step.pt\"\"\"\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_epochs=conf['total_epochs'], start_epoch=0, checkpoint_frequency=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "def numericalize(string, pad=True):\n",
    "    \"\"\"Takse a string and returns a tensor of indices for the tokens\"\"\"\n",
    "    bptt_len = conf['training_bptt_len']\n",
    "    TEXT_FIELD = train_ds.dataset.fields['text']\n",
    "    tokens = TEXT_FIELD.tokenize(string)\n",
    "    tokens = tokens[-bptt_len:]\n",
    "    t = TEXT_FIELD.numericalize([tokens]).T\n",
    "    num_tokens = t.shape[1]\n",
    "    if pad:\n",
    "        pad_len = max(0, bptt_len - num_tokens)\n",
    "        pad_t = torch.zeros([1, pad_len]).long()\n",
    "        t = torch.cat((t, pad_t), dim=1)\n",
    "    return t, num_tokens\n",
    "\n",
    "def tokenize(indices):\n",
    "    \"Takes a tensor of token indices and returns a string\"\n",
    "    tokens = [vocab.itos[i] for i in indices.squeeze()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_next_token(decoder_in, pos, model=model, deterministic=False):\n",
    "    \"\"\"Runs one step of auto-regression, returning the output token for\n",
    "    position `pos`.\"\"\"\n",
    "    \n",
    "    #print('decoder_in=', decoder_in)\n",
    "    #print('pos=', pos)\n",
    "    #print('decoder_in[0,pos]=', decoder_in[0,pos])\n",
    "    #if pos + 1 < decoder_in.shape[1]:\n",
    "        #print('decoder_in[0,pos+1]=', decoder_in[0,pos+1])\n",
    "    decoder_out = model(decoder_in=decoder_in)\n",
    "    #print('decoder_out=', decoder_out)\n",
    "    #print('decoder_out[o,pos,:]=', decoder_out[0,pos,:])\n",
    "    \n",
    "    if deterministic:\n",
    "        _, indices = torch.max(decoder_out, dim=-1)\n",
    "    else:\n",
    "        probs = nn.functional.softmax(decoder_out.float(), dim=-1)\n",
    "        m = torch.distributions.multinomial.Multinomial(probs=probs)\n",
    "        _, indices = torch.max(m.sample(), dim=-1)\n",
    "\n",
    "    next_index = int(indices[0,pos])\n",
    "    return next_index, vocab.itos[next_index]\n",
    "\n",
    "def sample(prompt, deterministic=False, vocab=vocab, prnt=True):\n",
    "    \"\"\"Auto-regresses using prompt to create the encoder_out tensor\"\"\"\n",
    "    bptt_len = conf['training_bptt_len']\n",
    "    \n",
    "    di, num_tokens = numericalize(prompt)\n",
    "    di = di.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eval_model = model.eval()\n",
    "        out = []\n",
    "        next_token = None\n",
    "        next_index = None\n",
    "        pos = num_tokens - 1\n",
    "        #print(pos)\n",
    "        for _ in range(2 * bptt_len):\n",
    "            #print('di =', di)\n",
    "            next_index, next_token = get_next_token(di, pos=pos, model=eval_model, deterministic=deterministic)\n",
    "            #print('next_index =', next_index)\n",
    "            #print('next_token =', next_token)\n",
    "            if next_token in (eos_token, pad_token):\n",
    "                break\n",
    "            if next_token is not None:\n",
    "                out.append(next_token)\n",
    "                pos += 1\n",
    "                if pos >= di.shape[1]:\n",
    "                    di = torch.roll(di, -1, -1)\n",
    "                    pos -= 1\n",
    "                di[0, pos] = next_index\n",
    "        \n",
    "    out = ' '.join(out)\n",
    "    if prnt:\n",
    "        print(prompt + '\\n --> \\n' + out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Born in Omaha , Nebraska , Malcolm X spent his teenage years living in a series of foster homes after his father 's death and his mother 's hospitalization . He engaged in several illicit activities there , eventually being sentenced\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Albert Einstein ( 14 March 1879 @–@ 18 April 1955 ) was a German @-@ born theoretical physicist who developed the theory of relativity , one of the two pillars of modern physics ( alongside quantum mechanics ) . His work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(range(5)):\n",
    "    print()\n",
    "    print('Completion #%s:' % i)\n",
    "    #print('-' * 20)\n",
    "    out = sample(prompt, deterministic=False, prnt=False)\n",
    "    print(prompt, '-->', out)\n",
    "    #print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['The',\n",
    "           'Of',\n",
    "           'To',\n",
    "           'In',\n",
    "           'A', \n",
    "           'Was',\n",
    "           'The',\n",
    "           'On',\n",
    "           'That',\n",
    "           'For',\n",
    "           'As',\n",
    "           'With']\n",
    "for prompt in prompts:\n",
    "    #print('Prompt:')\n",
    "    #print(\"=\" * 40)\n",
    "    #print(prompt, '...')\n",
    "    #print(\"=\" * 40)\n",
    "    for i, _ in enumerate(range(1)):\n",
    "        print()\n",
    "        #print('Completion #%s:' % i)\n",
    "        #print('-' * 20)\n",
    "        out = sample(prompt, deterministic=False, prnt=False)\n",
    "        print(prompt, '-->', out)\n",
    "        #print('-' * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
